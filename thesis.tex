%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% To avoid any implication, this template is released into the
% public domain / CC0, whatever is most convenient for the author
% using this template.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[T1]{fontenc}

\title{Self-supervised learning\\for calcium segmentation on coronary OCT images}
\author{Naravich Chutisilp}
\supervisor{Karim K. Kaldry, Ph.D. candidate}
\adviser{Professor Elazer R. Edelman, M.D., Ph.D.}
\coadviser{Professor Maria Brbi\'c, Ph.D.}
\expert{Farhad Rikhtegar Nezami, Ph.D.}

% \newcommand{\sysname}{FooSystem\xspace}

\dedication{
    \begin{raggedleft}
        It’s no use going back to yesterday, because I was a different person then.\\
        --- Lewis Carroll, Alice’s Adventures in Wonderland\\
    \end{raggedleft}
    \vspace{4cm}
    \begin{center}
        Dedicated to my lovely family, my dear friends, and kind people who have given opportunity and support to this lucky life I have.
    \end{center}
}
\acknowledgments{
% This is where you thank those who supported you on this journey. Good examples
% are your significant other, family, advisers, and other parties that inspired
% you during this project. Generally, this section is about 1/2 page to a page.

% Consider acknowledging the use and location of this thesis package.

% Define your acknowledgments in \texttt{\textbackslash{}acknowledgments\{...\}}
% and show them with \texttt{\textbackslash{}makeacks}.

    I would like to show my deep gratitude to my mom and dad for their unwavering supports and firm trust in letting me choose my path. I would love to thank my two lovely sisters, Namking and Namkow, for their continuous love and understanding. 

    I am also grateful to all the friends I have made during my time at EPFL, Set, James, Kwang, Ice, Sundae, Nai, Cindy, Jenestin, Thomas, Paulina, Fah, Ting-Wei, Pin-Yen, Yi-Kai, Hong-Bin, Leo, Edvin, Anthon, Aamir, Jirka, and many more people I have not mentioned, who have made my life abroad memorable and amicable. Going to EPFL is my first time living abroad for more than a month and I could not have asked for a better experience than what I have had. Simple dinners in the evening every day after school, meaningful conversations about everything, and trips to alpine mountains and European cities have made this little boy feel accompanied in the middle of the unknown future.

    Furthermore, I could not forget the friends have made during my time at MIT, Mee, Pooh, Cue, and Champ who have made my final Master's semester fun and unforgettable. Coming to MIT is a dream come true, yet it was not easy to leave my friends in Switzerland and start anew in the US. Unexpectedly, since the first day I arrived, I was welcomed with open arms and was shown to many places in Boston. Worries and fears were replaced with excitement and joy. Hiking in the White Mountains, visiting California and Silicon Valley, walking along the Freedom Trail, enjoying delicacies of Lobster rolls and clam chowder, and board game night every Friday are just a few of the many memories I have made in the US that I will never forget.

    Additionally, it would not be possible if I did not have support from my friends in Thailand, Meak, Korn, Marie, Jump, Copter, Nat, Bank, Pinn, V, Poom, Pewt, Choomp, Krist, Minnie, May, Kuan and uncountable more that I could not fit into this section. Calls and messages from these lovely lives have always been a source of strength and comfort. Through tough times and good times, they have always been there for me. Even though we are 9,188 km apart, or even 13,707 km apart, it feels like they are always by my side. It is a blessing to have friends like them and I have always been grateful for that.

    I would like to thank Prof. Elazer Edelman and Karim Kaldry for giving me this unparalleled opportunity to join the IMES lab at MIT and let me complete my Master's thesis here, as well as providing me advice and support on my research. Opportunity 
}


\begin{document}
\maketitle
\makededication
\makeacks

\begin{abstract}
The sysname tool enables lateral decomposition of a multi-dimensional
flux compensator along the timing and space axes.

The abstract serves as an executive summary of your project.
Your abstract should cover at least the following topics, 1-2 sentences for
each: what area you are in, the problem you focus on, why existing work is
insufficient, what the high-level intuition of your work is, maybe a neat
design or implementation decision, and key results of your evaluation.
\end{abstract}

\begin{frenchabstract}
For a doctoral thesis, you have to provide a French translation of the
English abstract. For other projects this is optional.
\end{frenchabstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

The introduction is a longer writeup that gently eases the reader into your
% thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
In the second paragraph you can introduce the main challenge that you see.
The third paragraph lists why related work is insufficient.
The fourth and fifth paragraphs discuss your approach and why it is needed.
The sixth paragraph will introduce your thesis statement. Think how you can
distill the essence of your thesis into a single sentence.
The seventh paragraph will highlight some of your results
The eights paragraph discusses your core contribution.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your
% work. This is not necessarily related work but technologies and dependencies
% that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.
Coronary artery disease (CAD) is a leading cause of heart diseases and fatalities worldwide~\cite{Virani2021Heart, Wu2015}. It is primarily caused by atherosclerosis, a process of plaque building up in the coronary arteries, partially or totally obstructing the blood flow~\cite{Shahjehan2023}. Commonly, doctors use percutaneous coronary intervention (PCI) as a treatment for CAD. This procedure is done by enlarging the narrowing artery~\cite{Ahmad2023}. However, the outcome of PCI can be affected by the presence of coronary artery calcification (CAC) which hinders stent expansion, leading to stent underexpansion, distortion, dislodgement, or loss~\cite{Hennessey2023}. Thus, a process to remove these plaques is a prerequisite to increase the success of PCI. Intravascular lithotripsy (IVL) is a novel technology that uses sonic pressure waves to break the calcification lesions, modifying them into fractures, allowing stent deployment to be effective~\cite{Butt2023}. This technique requires an accurate definition of the calcified lesions in order to select the optimal strategy for IVL~\cite{Butt2023}. Generally, the calcified lesions are identified using intravascular imaging techniques such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT)~\cite{Butt2023}. Specifically, OCT can deliver sufficient accuracy higher than IVUS~\cite{Fujimoto2003, Costopoulos2016}. However, manual segmentation of calcified lesions in OCT images is time-consuming and subjective~\cite{Segars2013, Oktay2020, Carpenter2022}. Therefore, an automatic segmentation method is needed to improve the efficiency and accuracy of the process~\cite{Carpenter2022}.

2D segmentation is a common approach for image segmentation. Convolutional layers are used to extract features from the input images. The layer uses 2D kernels that slide over the input image to extract visual features. U-Net introduces an encoder-decoder architecture with skip connections to explicitly utilize features at different resolutions for segmentation. An encoder is a series of convolutional layers with downsampling, while a decoder is a series of convolutional layers with upsampling. Skip connections are added between the encoder and decoder to combine features at different resolutions. This enables the model to learn and utilize representations at different levels~\cite{Ronneberger2015}. Additionally, 3D segmentation modifies 2D kernels to 3D kernels. This allows the model to work on 3D volumes directly, preserving the spatial information in the depth dimension. Consequently, the model can learn the features in 3D space, instead of in 2D space. Despite constraints imposed for data to always be in 3D volumes and augmenting the cost of annotation and computation, it leverages more information from the data to be used and potentially improve the performance of the model.

%TODO: add a photo of U-Net architecture

Following the success of transformer architecture in NLP~\cite{Vaswani2017}, Vision Transformer (ViT) is introduced as an attempt to apply transformer to 2D image classification~\cite{Dosovitskiy2020vit}. It has been empirically proven to be successful in a wide range of tasks, including image segmentation. Transformer-based 2D segmentation is an alternative approach to its CNN-based counterpart. Instead of using convolutional layers, it divides the input image into patches and tokenizes them using CNN. These tokens are added with positional information and fed into a transformer. There are several transformer-based architectures for image segmentation proposed in the literature. However, it is common in visual SSL that a linear projection layer is added to the output of the ViT to predict the segmentation masks~\cite{Oquab2024dinov, Ranzinger2024RADIO}. To enable a transformer model to do 3D segmentation, patchification must be modified to 3D patches, and the CNN-based tokenizer should operate with 3D kernels instead. In addition, the positional encoder should be modified to include the 3rd dimension. The rest of the architecture can be kept the same as 2D segmentation.

%TODO: add a photo of ViT architecture with linear projection

Medical image segmentation is a challenging task that typically requires specialized annotators to provide pixel-level ground truth labels for training. This leads to several types of research seeking to find a cost-effective way for medical image annotation~\cite{Fu2012, Gal2017, Beluch2018, Rahimi2021}. Alternatively, self-supervised learning (SSL) has been proposed as a promising approach to learning representations from the data itself without human annotations. Although SSL is now a common approach in natural language processing (NLP), it is still under active research in computer vision (CV) with a collection of practices being made~\cite{Balestriero2023}. 

Originally, SSL in computer vision has been done using traditional pre-text tasks such as autoencoder~\cite{Hinton2006} and denoising autoencoder~\cite{Vincent2008}. Such pre-text tasks are empirically useful in some use cases, but their benefits are not yet on par with SSL in NLP. Hence, pre-text tasks that explicitly embrace visual semantics have been proposed. Such tasks include solving jigsaw puzzle~\cite{Noroozi2016}, rotation prediction~\cite{Gidaris2018}, and colorization~\cite{Larsson2017}. Recently, visual SSLs have been moving towards more efficient and simpler pre-text tasks which can be categorized into two groups, discriminative and generative tasks. Discriminative tasks learn representations by distinguishing between the same and different views of images. Generative tasks learn representations by generating values for the masked-out parts of images. These tasks are more effective than traditional pre-text tasks in learning representations~\cite{Chen2020Simple, He2020, He2022, Bao2022beit}.

Even though modern discriminative tasks in visual SSLs have been studied in medical imaging, their generative counterparts have not been explored. In medical imaging, researchers in SSL have been focusing on amalgamating multiple pre-text tasks together. Several papers study a combination of traditional pre-text tasks to improve classification and segmentation tasks in medical imaging~\cite{Noroozi2016, Zhuang2019}. More recent researches explore blending discriminative tasks with traditional pre-text tasks~\cite{Zhou2021, Zhang2021, Dufumier2021, Taleb2020, Zhang2021, He2022Intra, Ren2022}. To the best of our knowledge, the only generative task being studied in medical imaging is restorative~\cite{Pathak2016, Chen2019, Zhou2021, Tang2022, Haghighi2021, Haghighi2024}.

In recent years, generative tasks have now gained more attention in medical imaging following their precedent success in visual SSLs. Masked autoencoder (MAE) which learns representations by filling the masked-out parts of images is effective~\cite{He2022}. Despite its simplicity, working at a pixel level is computationally expensive. A bidirectional encoder for image transformer (BEiT) proposes to work on a tokenized level instead of pixel level~\cite{Bao2022beit}. This makes BEiT more efficient than MAE as no computation is needed to translate from token to pixel values. DINOv2~\cite{Oquab2024dinov} combines discriminative and generative tasks, working on tokenized level and feature level. This approach is more efficient and effective than other image SSLs. I-JEPA takes this to a level higher by directly predicting the encoded vectors of masked-out parts~\cite{Assran2023}. This approach is simpler and much more scalable than other generative SSLs. These methods are effective in learning representations in visual SSLs. However, they have not been explored in medical imaging. Particularly, if working on embedding level proves to be beneficial in medical imaging, it can be a game changer as this can be the most efficient and greatly scalable approach to learning representations in unannotated medical imaging.

% TODO: add about alternatives

%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

% Introduce and discuss the design decisions that you made during this project.
% Highlight why individual decisions are important and/or necessary. Discuss
% how the design fits together.

% This section is usually 5-10 pages.

\section{Datasets}\label{sec:design:datasets}
The main goal of this project is to study the effectiveness of self-supervised learning algorithms for calcium segmentation on coronary OCT images. Therefore, we categorized the datasets into two groups, annotated and unannotated datasets. Each category has its characteristics and relevance to the problem. Calcium OCT (Dataset \ref{enum:calcium-dataset}) contains 8 volumes with calcium plaque annotation. Lumen and Wall OCT (Dataset \ref{enum:lumen-and-wall-dataset}) is a dataset of 20 volumes annotated with lumen and wall. These two datasets are mutually exclusive, meaning that the OCT images are not duplicated between them. Furthermore, 500 3D volumes are unannotated. These volumes are divided into 3 groups, 167 volumes before IVL, 169 volumes after IVL, and 164 volumes after stent deployment. Even though these volumes are unannotated, a tabular meta-data is available for each volume. This meta-data describes the morphological properties of the calcium plaques, lumen, and wall. It can be summarized as follows:

\begin{enumerate}
    \item Annotated datasets:
    \begin{enumerate}
        \item \label{enum:calcium-dataset} \textbf{Calcium OCT} is a dataset of coronary OCT images with calcium annotations before IVL.
        \item \label{enum:lumen-and-wall-dataset} \textbf{Lumen and Wall OCT (LaW OCT)} is a dataset of coronary OCT images with lumen and wall annotations before IVL.
    \end{enumerate}
    \item \label{enum:unannotated-dataset} Unannotated datasets:
    \begin{enumerate}
        \item \textbf{Pre-IVL} is a dataset of coronary OCT images before IVL.
        \item \textbf{Post-IVL} is a dataset of coronary OCT images after IVL.
        \item \textbf{Post-Stent} is a dataset of coronary OCT images after stent deployment.
    \end{enumerate}
\end{enumerate}

Each dataset has its purpose in this study. Calcium OCT is used for evaluating the effectiveness of each algorithm on calcium segmentation. On the other hand, Lumen and Wall OCT can be used for supervised pre-training of the algorithms, while unannotated datasets are used for self-supervised pre-training. Additionally, the tabular meta-data can be used to evaluate the effectiveness of the algorithms on multi-modalities, but this is not the main focus of this study.

\begin{figure}[hbt]
    % \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/fig_datasets_calcium_oct_sample.pdf}
        \caption{An example of an annotated image in Calcium OCT dataset.}
        \label{fig:calcium-oct}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/fig_datasets_law_oct_sample.pdf}
        \caption{An example of an annotated image from the Lumen and Wall OCT dataset.}
        \label{fig:lumen-and-wall-oct}
    \end{subfigure}
    \caption{For each image, the first row displays the original image in three axes, while the bottom row presents the segmentation mask for the corresponding views.}
    \label{fig:enter-label}
\end{figure}

In Calcium OCT, only 1 class, calcium, is annotated. The size of each image is $500\times 500$ while the depth varies from $375$ to $539$. An example of the annotated image is shown in Figure~\ref{fig:calcium-oct}. The dataset is split into 6 training volumes and 2 testing volumes. The training volumes are divided into 3 folds of training and validation sets. In each fold, the model can be trained using the training set, while the best model can be selected using the validation set. Subsequently, all models trained on each fold can be evaluated on the testing set that has not been seen during training. Finally, the performance on the testing set can be averaged to get the final performance. This is done to ensure the robust evaluation of the models.

Lumen and Wall OCT has 2 classes, lumen, and wall, and is divided into 16 training volumes and 4 testing volumes. The size of each image is $500\times 500$ while the depth varies from $270$ to $540$. An example of the annotated image is shown in Figure~\ref{fig:lumen-and-wall-oct}.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/fig_datasets_calcium_oct_sample.pdf}
%     \caption{An example of an annotated image in Calcium OCT dataset. The first row displays the original image in three axes, while the bottom row presents the segmentation mask for the corresponding views.}
%     \label{fig:calcium-oct}
% \end{figure}

% \begin{figure}[hbt]
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/fig_datasets_law_oct_sample.pdf}
%     \caption{An example of an annotated image from the Lumen and Wall OCT dataset. The first row displays the original image in three axes, while the bottom row presents the segmentation mask for the corresponding views}
%     \label{fig:lumen-and-wall-oct}
% \end{figure}

The unannotated datasets are of size $500\times 500$. They are not split into training and testing sets since they are only used for self-supervised pre-training. However, each self-supervised learning algorithm may split them into train and validation sets for hyperparameter tuning. Samples of each modality are shown in Figure~\ref{fig:unannotated-oct}. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/fig_datasets_unlabeled_oct_sample.pdf}
    \caption{Samples of unannotated OCT images. The left column is Pre-IVL, the middle column is Post-IVL, and the right column is Post-Stent OCT modality.}
    \label{fig:unannotated-oct}
\end{figure}

\newpage
\section{Algorithms}
\subsection{CNN-based Supervised Learning - nnUNet}\label{sec:design:nnunet}
nnUNet is a self-configuring framework for medical image segmentation~\cite{Isensee2020}. First, the data fingerprint must be introduced, specifying the number of image modalities, how the algorithm should normalize each modality, and the number of segmentation classes and regions. Additional information about data, including intensity ranges, spacing, shape, and resolution, is derived from the its metadata. With this fingerprint, nnUNet automatically decides the network topology, sampling strategy, batch size, and patch size. It is also worth noting that nnUNet provides a supervised pre-training pipeline for the model to be trained on other datasets before fine-tuning it on the target dataset.

To apply the algorithm to our datasets, we design two approaches. The first approach is to directly train the model on Calcium OCT. In the second approach, we pre-train the model on Lumen and Wall OCT before fine-tuning on Calcium OCT. 

% Discuss 2D and 3D nnUNet for segmentation
nnUNet provides both 2D and 3D segmentation architectures as well as a cascade 3D architecture of full resolution and low-resolution networks. In our initial experiments, we find that 2D and 3D architectures are performing better than the cascade architecture. This leads us to subsequently only use 2D and 3D architectures for the rest of the experiments.

% TODO: add a photo of the nnUNet pipeline

\subsection{Transformer-based Supervised Learning - SegFormer}
SegFormer is a transformer-based model with multi-resolution feature fusion using simple multi-layer perceptrons (MLP~\cite{Rumelhart1986})~\cite{Xie2021SegFormer}. It has been pre-trained on the ADE20k dataset~\cite{Zhou2018}. Resembling U-Net's encoder, SegFormer explicitly downsamples the input image using overlap patch merging, which unifies adjacent patches into a single patch. This enables the model to learn the features at different resolutions. Furthermore, SegFormer employs these multi-resolution features with an All-MLP decoder that projects lower-resolution features to match the channels of their higher resolution counterparts. The decoder then upsamples the features up to the original resolution and concatenates them with the higher-resolution features. Finally, the decoder predicts the segmentation masks using only MLPs and upsampling layers. 

SegFormer architectures are available in multiple sizes, from B1 to B5, with the largest model, MiT-B5, containing 84.7M parameters. We fine-tune the largest pre-trained model on Calcium OCT for 80,000 iterations, which we find to be sufficient for the model to converge.
% TODO: add a photo of SegFormer architecture

\subsection{Transformer-based Supervised Learning - RADIO}
RADIO is a distillation framework that allows more than one vision foundation model to be its teacher~\cite{Ranzinger2024RADIO}. The author evaluates the model on 2D segmentation tasks, using batch normalization and linear projection as a decoder head. We follow the same evaluation setting to utilize their pre-trained ViT model. We use the RADIO pre-trained ViT-H/16 model and fine-tune it on Calcium OCT. The model is trained for 80,000 iterations which we find to be sufficient for the model to converge.

\subsection{Transformer-based Medical Supervised Learning - SuPreM}~\label{sec:design:suprem}
SuPreM is a supervised pre-training method for medical image segmentation~\cite{Li2024}. We fine-tune publicly available SuPreM pre-trained models on Calcium OCT. We choose their best performing model, SwinUNTER~\cite{Tang2022}. SwinUNTER is a transformer-based model designed to segment brain tumors in 3D MRI images. The architecture is based on the Swin Transformer as its encoder, which is a multi-scale transformer using shifted windows~\cite{Liu2021Swin}. SwinUNTER utilizes a CNN-based decoder called UNTER, hence the name SwinUNTER. This decoder consists of a series of convolutional layers with upsampling and merges features at different levels via skip connections~\cite{Hatamizadeh2022}. It is worth noting that the publicly available models have been pre-trained on $2,100$ CT volumes, rather than $9,262$, as the authors have not yet released the fully pre-trained models.

In addition to SuPreM pre-training, original authors of SwinUNTER have proposed a self-supervised learning pre-training algorithm for the model~\cite{Tang2022}. Therefore, we conduct three experiments to evaluate the algorithms. The first setting directly utilizes the pre-trained model on the SuPreM dataset and fine-tunes it on Calcium OCT. The second setting uses the self-supervised SwinUNTER model and fine-tunes it on Calcium OCT. The last setting uses the SwinUNTER model without any pre-training, and trains it on Calcium OCT. This approach allows us to evaluate the effectiveness of SuPreM pre-training and the self-supervised learning proposed in SwinUNTER on Calcium OCT.

\subsection{Unimodal Self-Supervised Learning - V-JEPA}
V-JEPA is a self-supervised learning method that acquires representations by predicting the encoded vectors of masked segments of a video~\cite{Bardes2024Vjepa}. We design two experiments to study the effectiveness of V-JEPA on OCT images. First, we directly fine-tune the pre-trained V-JEPA model on Calcium OCT. V-JEPA pre-trains the ViT model on the VideoMix2M dataset, which contains 2M videos. We then fine-tune this model on Calcium OCT. In the second experiment, we apply V-JEPA pre-training to unannotated OCT images and fine-tune the resulting model on Calcium OCT. Since the original paper does not evaluate the model on segmentation tasks, we will discuss the implementation of the decoder head in Section~\ref{sec:implementation:vjepa}.

% TODO: add a photo of V-JEPA framework

\subsection{Unimodal Self-Supervised Learning - Genesis}
Genesis is a self-supervised learning method that learns representations by restoring the original images from their corrupted versions~\cite{Zhou2021}. In their paper, Genesis is applied to a U-Net architecture, which contains 15M parameters. We adapt their method to the same 3D nnUNet architecture in Section~\ref{sec:design:nnunet}, which has 30M parameters. Thereafter, we apply Genesis pre-training to unannotated OCT images and fine-tune the resulting model on Calcium OCT.

% TODO: add a photo of Genesis framework

\subsection{Multi-modal Self-Supervised Learning - CLIP}
Contrastive Language-Image Pre-training (CLIP) is a self-supervised learning method that learns representations by matching encoded vectors of images and their text captions~\cite{Radford2021CLIP}. We adapt CLIP to OCT images by using co-registered Pre-IVL and Post-IVL images. We use the same nnUNet architecture as described in Section~\ref{sec:design:nnunet}. CLIP pre-training is applied to the 3D nnUNet. The resulting model is then fine-tuned on Calcium OCT. We modify the code from a publicly available project~\cite{Shariatnia2021}. The original paper proposes a pre-training framework for language and image. However, we apply the framework to multi-modalities of OCT images. We will discuss the adaptation of the framework to our specific use case in Section~\ref{sec:implementation:clip}.
% TODO: add a photo of the CLIP framework

%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

% The implementation covers some of the implementation details of your project.
% This is not intended to be a low-level description of every line of code that
% you wrote but covers the implementation aspects of the projects.

% This section is usually 3-5 pages.
\section{CNN-based Supervised Learning - nnUNet}
We apply publicly available code from nnUNet~\cite{Isensee2020} on our datasets. Additionally, we add an early stopping to the optimizer and train the model until stopping criterion is met. As we find that the 3D segmentation of nnUNet is performing significantly worse than its 2D counterpart, we experiment with different patch sizes on 3D segmentation instead of its default patch size. We find that reducing the depth of the original patch size from $112$ to $32$ improves the performance significantly. This could be because smaller depth allows the model to have more examples to learn from. In addition, we pre-train the model on Lumen and Wall OCT before fine-tuning it on Calcium OCT. nnUNet provides a supervised pre-training script as well as a pipeline to fine-tune on new datasets.

\section{Transformer-based Supervised Learning - SegFormer}\label{sec:implementation:segformer}
We use the pre-trained SegFormer model and a training script from MMSegmentation~\cite{mmseg2020}. As SegFormer only supports 2D segmentation, we pre-process our Calcium OCT datasets into 2D images with their segmentation masks and train the model on the newly formatted dataset.

\section{Transformer-based Supervised Learning - RADIO}
We use the pre-trained RADIO ViT model and their training script available in MMSegmentation~\cite{mmseg2020}. As RADIO also only supports 2D segmentation, we share the same pre-processing datasets with SegFormer in Section~\ref{sec:implementation:segformer}.

\section{Transformer-based Medical Supervised Learning - SuPreM}
SuPreM provides publicly available pre-trained models and codes~\cite{Li2024}. Thus, we formated our Calcium OCT dataset to match the format of their datasets. We then used their fine-tuning script on our datasets. We experimented with three settings, as described in Section~\ref{sec:design:suprem}, which are all available in their codes.

\section{Unimodal Self-Supervised Learning - V-JEPA}\label{sec:implementation:vjepa}
Because of V-JEPA's lack of evaluation on segmentation tasks, it is necessary to design a decoder head for V-JEPA. The paper proposes an attentive decoder head for classification tasks. 

Therefore, we developed an attentive decoder head for segmentation, drawing inspiration from the MAE framework~\cite{He2022}, which directly predicts pixel values. Specifically, the decoder head is an encoder-only transformer of 1 depth and 12 heads followed by a linear projection layer that yields embedding vectors into a vector of size \(\text{patch size} \times \text{patch size} \times \text{number of classes}\). Besides, we also experimented with other variations of decoding architecture, such as a simple batch normalization layer followed by a linear projection, similar to how RADIO~\cite{Ranzinger2024RADIO} and DINOv2~\cite{Oquab2024dinov} fine-tuned their models on segmentation tasks. This is shown in Figure~\ref{fig:vjepa-attentive-and-batchnorm-decoder-head}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/fig_implementation_vjepa_attentive_and_batchnorm_decoder.pdf}
    \caption{Attentive and linear batch normalization decoder head for ViT (V-JEPA).}
    \label{fig:vjepa-attentive-and-batchnorm-decoder-head}
\end{figure}%

Additionally, inspired by SegFormer~\cite{Xie2021SegFormer}, we experimented with simple multi-layer perceptrons (MLP~\cite{Rumelhart1986}) on each feature vector at different depths followed by a linear projection to combine those features into a segmentation mask. This enables the decoder to use multiple features at different levels. Unlike SegFormer, each feature is not downsampled, so it is not explicitly forced to be a lower resolution. This method allowed us to preserve the original architecture of V-JEPA's pre-training. Thus, we could utilize their pre-trained weight on our segmentation task. Multi-feature decoder head is shown in Figure~\ref{fig:vjepa-multi-feature-decoder-head}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig_implementation_vjepa_multifeat.pdf}
    \caption{A multi-feature decoder head. Features at each transformer layer are projected to the same dimension and concatenated before being passed to a convolutional layer that produces the segmentation mask.}
    \label{fig:vjepa-multi-feature-decoder-head}
\end{figure}

\section{Unimodal Self-Supervised Learning - Genesis}
Genesis has made their code available~\cite{Zhou2021}. Their implementation requires an initial pre-processing step on unannotated OCT images. It samples small cubes of size $64\times 64\times 64\times 32$ from the unannotated volumes. These cubes are then corrupted on the fly during the Genesis pre-training process. The author recommends $96$ cubes per volume, as increasing the number of cubes per volume might not improve the performance while increasing computation cost. We followed this recommendation and applied an additional sampling strategy. The strategy randomly samples $96$ cubes from each volume. This was done to ensure that the sampled cubes contain enough information since OCT images are filled with background unlike CT images used in the original paper. The sampling strategy involves selecting the cubes with average intensity above a certain threshold. The resulting cubes from the threshold strategy can be seen in Figure~\ref{fig:genesis-cubes-with-threshold}. Without this strategy, the sampled cubes are mainly background noise, which is shown in Figure~\ref{fig:genesis-cubes-without-threshold}. We then trained the model on these cubes until convergence.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube.png}
        \caption{Samples of cubes from unannotated OCT images with threshold sampling strategy.}
        \label{fig:genesis-cubes-with-threshold}

    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube_no_threshold.png}
        \caption{Samples of cubes from unannotated OCT images without threshold sampling strategy.}
        \label{fig:genesis-cubes-without-threshold}
    \end{subfigure}
    \caption{Samples of cubes from unannotated OCT images. Each row is a sampled cube with slices unrolled along the depth dimension. With threshold sampling in \ref{fig:genesis-cubes-with-threshold}, the cubes are the wall area of the OCT images. Without threshold sampling in \ref{fig:genesis-cubes-without-threshold}, the cubes are mainly background noise.}
\end{figure}

For fine-tuning, we loaded the pre-trained weights of the model's encoder and decoder into the 3D nnUNet, while leaving the segmentation head to be randomly initialized. Subsequently, we fine-tuned the pre-trained model on the Calcium OCT dataset. Following the findings of the 3D nnUNet, we fine-tuned the model with the best patch size found in Section~\ref{sec:design:nnunet}, which is $160\times 128\times 32$. To achieve this, we used the nnUNet's training script with the same hyperparameters as the supervised pre-training and provided the pre-trained model as the initial weights.

\section{Multi-modal Self-Supervised Learning - CLIP}\label{sec:implementation:clip}
Originally, the algorithm is designed for language-image pairs. However, we adapted the algorithm to multi-modalities of OCT images. To achieve this, we swap the text tokenizer of the original CLIP for an image encoder. 

From our experiments, we found that using separate encoders and projectors for each modality is not as effective as using a shared encoder and projector. We did a sanity check on different configurations by passing the same images to them and seeing whether they could learn to match a much simpler problem. It should be able to encode the same image to the same vector. We sanity-checked 3 configurations, separate encoders and projectors, separate encoders and shared projectors, and shared encoders and projectors. We found that separate encoders and shared projectors, and shared encoders and projectors can learn to match the same image.

Consequently, we applied these two configurations to the main task of matching co-registered images. This time, only the shared encoders and projectors could converge. This is reasonable, as the image encoder and its projector are forced to learn a representation that can be used for both modalities. This approach is illustrated in Figure~\ref{fig:clip-oct}. We refer the reader to Section~\cite{sec:results:discussion:clip} for details of the experiment.

\begin{figure}[hb]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/fig_implementation_clip_oct.pdf}
    \caption{A modified CLIP for co-registered multi-modal OCT images. Note that the projector is omitted for simplicity. The image encoder and projector are shared between all modalities}
    \label{fig:clip-oct}
\end{figure}

%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Discussion}
%%%%%%%%%%%%%%%%%%%%

% In the evaluation you convince the reader that your design works as intended.
% Describe the evaluation setup, the designed experiments, and how the
% experiments showcase the individual points you want to prove.

% This section is usually 5-10 pages.
We evaluated the effectiveness of self-supervised learning algorithms on calcium segmentation on coronary OCT images. All the models, pre-trained or not, were fine-tuned on the Calcium OCT dataset using the validation set to select the best model. We did this for 3 folds. The models were then evaluated on the testing set. The performance was measured using the Dice score, a common metric for segmentation tasks. It is defined as follows:

\begin{equation}
    \text{Dice} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}
\end{equation}

where TP, FP, and FN are true positive, false positive, and false negative, respectively. The Dice score ranges from 0 to 1, where 1 indicates perfect segmentation. The Dice score was calculated for each patient in the testing set, and the average Dice score was reported as the final performance of the model for each fold. The final performance is the average of the Dice scores of all folds.

\section{nnUNet}\label{sec:result:nnunet}
The Dice score of the default configuration of 3D nnUNet is significantly lower than that of the 2D nnUNet. By reducing the depth of the patch size of the 3D nnUNet from $112$ to $32$, the performance increases significantly. This could result from the smaller depth allowing the model to have more examples to learn from. In addition, expanding the width and height of the patch size from $160\times 128$ to $512\times 512$ does not improve performance. This may be due to the nature of calcium plaques in OCT images, which are localized. Thus, increasing the context in the width and height dimensions does not provide additional information to the model. The performance of the nnUNet on the Calcium OCT dataset is summarized in Figure~\ref{fig:nnunet-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/result_nnunet_results.png}
    \caption{Dice score of variations of nnUNet. Results are grouped according to the patch size used in training. Results in hashes indicate supervised pre-training on the lumen and wall dataset (LaW OCT). 
    % The one-sided paired t-test is done between with and without supervised pre-training. It is also done between the best 2D nnUNet model and the supervised pre-trained 3D nnUNet model. Lastly, the test is done between supervised pre-trained 3D nnUNet models.
    }
    \label{fig:nnunet-results}
\end{figure}

Additionally, pre-training the models with the lumen and wall dataset (LaW OCT) before fine-tuning them on the Calcium OCT significantly improves the 3D performance of the models when their patch sizes are reduced. Calcium plaques are typically located around the periphery of artery walls. Hence, this pre-training allows the model to learn the features of the walls, which may be useful for calcium segmentation. These pre-trained models are also statistically comparable to the best 2D nnUNet model.

\section{SuPreM}
We evaluated three configurations of the SwinUNTER model, as described in Section~\ref{sec:design:suprem}. Our findings indicate that SuPreM pre-training on SwinUNTER yields the best results with an average Dice score of $0.631\pm0.030$. This score is statistically higher than those of the other two configurations. SuPreM pre-training provides superior features for calcium segmentation compared to self-supervised learning and training from scratch. The results are summarized in Figure~\ref{fig:suprem-results}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/result_SwinUNETR_results.png}
    \caption{Dice score of SuPreM's SwinUNTER model. In the parenthesis indicate pre-training algorithms. SuPreM pre-training on SwinUNTER yields the best results and is significantly better than both self-supervised pre-training and training from scratch.}
    \label{fig:suprem-results}
\end{figure}

However, when comparing the result of the SuPreM pre-trained SwinUNTER model with that of the best 2D nnUNet model, the 2D nnUNet model outperforms the SuPreM model (see Figure~\ref{fig:transformer-results}). This will be discussed in the following section.

\section{Transformers-based Models}
We find that the best 3D nnUNet model outperforms all transformer-based models. This may be attributed to fundamental differences between CNN and transformer architectures. CNNs are designed to learn local features, while transformers are designed to learn global features. Calcium plaques in OCT images are localized, making CNNs more suitable for this task. This is also supported by the fact that, in CNNs, increasing the patch size in the width and height dimensions of the 3D nnUNet does not improve performance, while reducing the patch size in the depth dimension significantly improves performance (see Section~\ref{sec:result:nnunet}). The effectiveness of the SegFormer, ViT (RADIO), and SwinUNTER (SuPreM) compared to the nnUNet is summarized in Figure~\ref{fig:transformer-results}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/result_nnUNet_vs_Transformer_results.png}
    \caption{Dice scores of transformer-based models compared to those of the nnUNet. Compared to the best 3D nnUNet model pre-trained on the LaW OCT dataset, Dice scores of transformer-based models are statistically significantly lower.}
    \label{fig:transformer-results}
\end{figure}

\section{V-JEPA}
Each experiment is denoted by its architecture and its decoder head. The dataset V-JEPA was pre-trained on is indicated in the parenthesis. The control experiment, \textbf{ViT + Attentive*}, is the ViT model followed by an attentive decoder head, with intensive data augmentation and positive data sampling, but without V-JEPA pre-training. The results of different decoder heads are summarized in Figure~\ref{fig:vjepa-decoder-results}.

There is no significant difference between the various decoder heads. However, the representation learned by V-JEPA on VideoMix2M, namely \textbf{ViT + Attentive* (V-JEPA VideoMix2M)}, significantly improves the performance of the ViT model on calcium segmentation (\textbf{ViT + Attentive*}). This highlights the usefulness of V-JEPA pre-training.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/result_VJEPA_results.png}
    \caption{Dice score of V-JEPA's decoder head. BNLinear denotes batch normalization followed by linear projection. Attentive signifies the attentive decoder head. Attentive* indicates an attentive decoder head with more intensive data augmentation and positive data sampling. MultiFeat denotes the multi-features decoder head. The dataset V-JEPA was pre-trained on is indicated in the parenthesis.}
    \label{fig:vjepa-decoder-results}
\end{figure}

In addition to V-JEPA pre-trained on VideoMix2M, we also evaluated V-JEPA pre-trained on our unannotated OCT images, \textbf{ViT + Attentive* (V-JEPA Unannotated OCT)}. Pre-training on the unannotated OCT images did not improve the performance of the ViT model on calcium segmentation. This may be due to the size of the dataset. The unannotated OCT dataset consists of only 500 volumes, whereas VideoMix2M contains 2M videos. This demonstrates the importance of dataset scale for self-supervised learning on transformer-based models.

\section{CLIP}
We find that CLIP pre-training on co-registered OCT images significantly improves the performance of the 3D nnUNet model. In addition, CLIP is statistically comparable to the 3D nnUNet model pre-trained on the LaW OCT dataset. This shows that CLIP is as effective as supervised pre-training on the LaW OCT dataset, while the cost for co-registration is lower than the cost for annotating lumen and wall. The performance of CLIP pre-training on co-registered multi-modal OCT images is summarized in Figure~\ref{fig:clip-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_CLIP_results.png}
    \caption{Dice score of CLIP on Calcium OCT dataset compared to nnUNet. In the parenthesis indicates the algorithms used for pre-training. LaW OCT is the supervised pre-training on the Lumen and Wall OCT dataset. CLIP is the pre-training on co-registered multi-modal OCT images.
    %  Both supervised and CLIP pre-training significantly improves the performance of the 3D nnUNet model. They are statistically comparable to each other, and to the best 2D nnUNet model.
    }
    \label{fig:clip-results}
\end{figure}

The success of CLIP pre-training may result from the fact that aligning Pre-IVL and Post-IVL requires the model to learn the features of the lumen and wall, which has been proven to be useful for calcium segmentation in nnUNet (LaW OCT). To prove this, we visualize the PCA of the features learned by CLIP compared to the features output from the initial weight of the model. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_default_feature_map_batch0_feature1.png}
        \caption{PCA of the output of the features from initial weight of the model.}
        \label{fig:pca-initial}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_clip_feature_map_batch0_feature1.png}
        \caption{PCA of the features learned by CLIP.}
        \label{fig:pca-clip}
        
    \end{subfigure}
    \caption{PCA of the output of the features from the initial weight of the model and the features learned by CLIP. CLIP pre-training gives more distinguishable features on the walls than those from the initial weight of the model. 
    % This can be the reason why CLIP pre-training improves the performance of the model.
    }
\end{figure}

The dataset used in CLIP is co-registered Pre-IVL and Post-IVL. Therefore, the author suggests similar experiments be further studied on the co-registered Pre-IVL and Post-Stent dataset.

\section{Genesis}
Pre-training Genesis on unanotated OCT images results in a Dice score of $0.744\pm0.000$ on the Calcium OCT dataset. Furthermore, Genesis significantly improves the performance of the 3D nnUNet model. Additionally, Genesis performs comparably to the best 2D nnUNet and the pre-trained 3D nnUNet on the LaW OCT dataset, while not requiring additional data annotations. The results and statistical tests are summarized in Figure~\ref{fig:genesis-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_Genesis_results.png}
    \caption{a Dice score of Genesis on the Calcium OCT dataset compared to those of nnUNets. The dataset used for pre-training is indicated in the parenthesis.
    }
    \label{fig:genesis-results}    
\end{figure}


\section{Final Results}
The models are grouped into three categories: SSL for self-supervised learning models, Transformer for transformer-based models, and nnUNet for 2D and 3D CNN-based nnUNet models. V-JEPA on unannotated OCT is categorized under SSL, whereas V-JEPA on VideoMix2M is under Transformer, as it was only fine-tuned to the Calcium OCT dataset. Each result is statistically compared to the 2D nnUNet model, which has the highest average Dice score in our study. The results of the best models in each category are summarized in Figure~\ref{fig:results}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=1\textwidth]{figures/result_best_in_class_results.png}
    \caption{Summary of the results. The best model in each category is illustrated. SSL denotes self-supervised learning models, Transformer denotes transformer-based models, and nnUNet denotes 2D and 3D CNN-based nnUNet models.}
    \label{fig:results}
\end{figure}

In the SSL category, V-JEPA pre-trained on unannotated OCT images performs significantly worse than 2D nnUNet. As discussed in the previous section, the scale of the dataset may play an important role in transformer-based self-supervised learning. Genesis pre-training on unannotated OCT images also performs significantly worse than the 2D nnUNet. However, it performs comparably to the best 3D nnUNet pre-trained on the LaW OCT dataset. Although it is not comparable to the best-performing model, it is as effective as supervised pre-training while not requiring additional annotations. CLIP pre-training on co-registered multi-modal OCT images performs comparably to the 2D nnUNet model. Additionally, it performs comparably to the 3D nnUNet model pre-trained on the LaW OCT dataset. This demonstrates that CLIP can be as effective as supervised pre-training while being less costly in terms of annotation.

From our experiments, transformer-based models' Dice scores are significantly less than 2D nnUNet's. Transformer provides a self-attention mechanism that allows the model to combine features from different parts of the image. In contrast, CNNs' kernels are designed to learn local features. Since calcium plaques are typically located around the artery walls, CNNs are more suitable for this task. In addition, the scale of the dataset may play an important role. nnUNet is designed for small- to medium-sized medical datasets.
Scaling up the size of the dataset and model parameters will not be advantageous for CNNs when these two magnitudes reach a certain point. On the contrary, transformers benefit from scaling~\cite{Zhai2021}.
This study does not serve to prove that nnUNet is better than transformers in general. It only shows that nnUNet is more suitable for calcium segmentation on OCT images with the current scale of the dataset and model parameters.

Finally, the best model in our study is 2D nnUNet with an average Dice score of $0.753\pm0.006$. It is non trivial to note that 2D nnUNet does not benefit from pre-training on other datasets, as treating the dataset into 2D images is enough for the model to learn the features of the calcium plaques. However, the best 3D nnUNet is achieved by pre-training on LaW OCT dataset. It is equally vital to note that patch size of 3D nnUNet is crucial for the performance of the model. Reducing the patch size in the depth dimension augments the performance significantly, while providing more context in the width and height dimension does not improve the performance. This mirrors the nature of annotating the calcium plaques in OCT images. Calcium plaques are identified by their change in intensity around the artery walls. Therefore, x-y plane context does not provide additional information to the model. Similarly, calcium, more often than not, occurs between adjacent slices. There is no need for the model to have more context in the depth dimension.

\section{Discussion}
\subsection{Scale Analysis}
We do an initial study on the scale of the dataset used to train the model and their effect on the performance of the model. We subsample the training set of the Calcium OCT dataset to 33\% (1 training image and 1 validation image) and 66\% (3 training and 1 validation) of the original size. As shown in Figure~\ref{fig:scale-analysis}, the supervised pre-training on LaW OCT dataset is shown to be more reliable than self-supervised learning on a smaller dataset as they yields a lower standard deviation of dice score while the average dice score is higher. CLIP, on the other hand, is shown to be more affected in the lowest scale of the dataset. It has a lower average dice score and a higher standard deviation of dice score. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/discussion_scale_analysis.png}
    \caption{Dice score of 2D and 3D nnUNet on Calcium OCT dataset subsampled to 33\% and 66\% of the original size.}
    \label{fig:scale-analysis}
\end{figure}

We hypothesize 2 possible scenarios. One is that 8 volumes with 4 training, 2 validation and 2 testing volumes are already in a low data regime, making them lower as we study then does not reflect what other self-supervised learning research has shown, which is that SSL is effective in low data regime. The other is that CLIP features are not as effective as supervised pre-training features as CLIP implicitly learns the features of the lumen and wall, while supervised pre-training explicitly learns the features of the lumen and wall. This leads to more data needed for CLIP to translate the implicit features to the explicit features as we see CLIP thrive as the scale of the dataset increases. We recommend more studies to be done on the scale of the dataset as this study is only preliminary and may not be conclusive.

\subsection{Visually Good Features}
Visually good features do not always lead to a better performance as the distinction of the most important weighs more than the aesthetics of the features. We visualize the features from V-JEPA (VideoMix2M) and from its initial weight. Shown in Figure~\ref{fig:visually-good-features}, both retain the structure of the artery walls. However, the features from V-JEPA (VideoMix2M) put the background into the same cluster and give more distinction to the artery walls. This can be the reason why V-JEPA (VideoMix2M) improves the performance of the model. While the features from the initial weight of the model keep the original structure and the structure of the artery walls is visible to human eyes, a close look into the color of the features shows that the artery walls are not as distinguishable from the background or any other structures as they are blended into a gradient of colors. What might be deceivingly good features to human eyes may not be the best features for the model to learn from.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_input.jpg}
        \caption{Input image}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_initial.jpg}
        \caption{PCA feature from ViT with initial weight}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_pretrained.jpg}
        \caption{PCA feature from ViT with V-JEPA pre-trained weight}
    \end{subfigure}
    \caption{PCA of the output of the features from the initial weight of the model and the features learned by V-JEPA. A volume is unrolled along the depth dimension.}
    \label{fig:visually-good-features}
\end{figure}

Visualizing ViT (RADIO) also supports this hypothesis. Shown in Figure~\ref{fig:radio-features}, features from ViT (RADIO) are humanly interpretable as they can successfully distinguish OCT image semantics. The border between the background and where the waves could travel is visible. However, the features on the artery walls are not as distinct. This can be the reason why ViT (RADIO) does not perform well in terms of Dice score.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/discussion_radio_feature.png}
    \caption{PCA of the features output from ViT (RADIO). The middle column is a PCA of the features with the first principal component kept. The right column is with the first principal component removed.}
    \label{fig:radio-features}
\end{figure}

\subsection{V-JEPA Pre-training}
Unlike Genesis, where loss and semantic checks of how the model is learning on the pre-text task can be determined easily, V-JEPA does not. We experiment with different configurations of V-JEPA pre-training on our unannotated OCT images. We face difficulty in converging the loss. As shown in Figure~\ref{fig:v-jepa-training}, the loss curve of ViT-L (V-JEPA Unannotated OCT) is converging, but it is sensitive to the configurations used.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_1.png}
        \caption{ViT-L (V-JEPA Unannotated OCT)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_2.png}
        \caption{ViT-S (V-JEPA Unannotated OCT)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_3.png}
        \caption{ViT-L (V-JEPA Unannotated OCT) with more weight decay}
    \end{subfigure}
    \caption{Different loss curves of V-JEPA pre-training on unannotated OCT images with different configurations.}
    \label{fig:v-jepa-training}
\end{figure}

One way to determine V-JEPA convergence is to plot the hidden features from the target encoder and compare that to the context encoder. Illustrated in Figure~\ref{fig:v-jepa-prediction}, the pretraining can succeed in the V-JEPA pre-text task of predicting the hidden features of the masked-out parts of the video. In contrast to Genesis, or other pixel-level prediction pre-text tasks, it can not be easily determined if the prediction is correct. This is the trade-off of using V-JEPA which is more computationally efficient than MAE, but harder to determine if the model is learning the correct features.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/discussion_vjepa_prediction.png}
    \caption{Hidden features from target encoder and context encoder of ViT-L (V-JEPA Unannotated OCT) over training epochs. The bottom row shows different hidden features from the target encoder and context encoder in the last epoch.}
    \label{fig:v-jepa-prediction}
\end{figure}

\subsection{CLIP Pre-training}\label{sec:results:discussion:clip}
As mentioned in Section~\ref{sec:implementation:clip}, CLIP needs a shared encoder and projector for the pre-training to converge. In this section, we show the sanity check taken and the results for future works that may want to adapt CLIP to multi-modalities of OCT images.

We first do a sanity check of CLIP by training the model to match the same image with the same crop as shown in Figure~\ref{fig:clip-sanity-check-same-image}. This is a much simpler problem and its configuration that does not learn to match the same image will not be able to match co-registered multi-modal OCT images. We then train the model to match the Pre-IVL and Post-IVL images as shown in Figure~\ref{fig:clip-sanity-check-different-image}. This is the main task of CLIP. We find that only a shared encoder and projector can converge the losses down.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image.png}
        \caption{Same image}
        \label{fig:clip-sanity-check-same-image}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_pre-ivl_post-ivl_image.png}
        \caption{Pre-IVL and Post-IVL}
        \label{fig:clip-sanity-check-different-image}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_diff_enc_diff_proj_logits.png}
        \caption{Co-similarity matrix of different encoders and projectors on the same images.}
        \label{fig:clip-cosimilarity-diff-enc-proj}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_diff_enc_shared_proj_logits.png}
        \caption{Co-similarity matrix of different encoders and shared projectors on the same images.}
        \label{fig:clip-cosimilarity-same-enc-proj}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_shared_enc_shared_proj_logits.png}
        \caption{Co-similarity matrix of shared encoders and projectors on the different images.}
        \label{fig:clip-cosimilarity-shared-enc-proj}
    \end{subfigure}
    \caption{Sanity check of CLIP. The model is trained to match the same image and different crops of the same image.}
    \label{fig:clip-sanity-check}
\end{figure}

To determine whether the model is learning, we can plot a co-similarity matrix of the features output from the model. As shown in Figure~\ref{fig:clip-cosimilarity-diff-enc-proj}, using different encoders and projectors does not converge the losses down. In contrast, with a shared projector, the losses are converged down as shown in Figure~\ref{fig:clip-cosimilarity-same-enc-proj} using the same images. However, it does not converge the losses down when using different images. Only shared encoders and projectors can converge the losses down as shown in Figure~\ref{fig:clip-cosimilarity-shared-enc-proj} using different images.

Unlike language and image, different modalities of OCT images are not as distinct. Thus, having to learn to align hidden features with different encoders and projectors may add noises during the initial training phase leading to the model not being able to converge the losses down. 
%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

% The related work section covers closely related work. Here you can highlight
% the related work, how it solved the problem, and why it solved a different
% problem. Do not play down the importance of related work, all of these
% systems have been published and evaluated! Say what is different and how
% you overcome some of the weaknesses of related work by discussing the 
% trade-offs. Stay positive!

% This section is usually 3-5 pages.

% Self-supervised learning
\section{Self-supervised learning}
Self-supervised learning (SSL) has been proven to be a promising approach for natural language processing tasks (NLP). SSL can be split into two parts which are pre-text tasks and downstream tasks. Pre-text tasks are tasks that are designed to learn representations from the data itself without human annotations. Downstream tasks are finetuning tasks that use the learned representations from pre-text tasks to solve the tasks that require human annotations. It allows models to learn representations while avoiding the costs of human annotations~\cite{Jaiswal2020}. Unsupervised pre-text tasks are not uncommon in the computer vision (CV) domain. For example, autoencoder~\cite{Hinton2006}, denoising autoencoder~\cite{Vincent2008}, and colorization~\cite{Larsson2017} are some of the pre-text tasks that have been used in CV. However, suitable pre-text tasks for SSL in computer vision are still under research.

% Self-supervised learning in CVs
In recent years, discriminative and generative tasks have been proposed as pre-text tasks for CV field. SimCLR~\cite{Chen2020Simple} is a discriminative method that learns representations by maximizing the similarity between differently augmented views of the same image and minimizing the similarity between views of different images. This is also known as contrastive learning (CL). Representations learned from SimCLR have shown to be effective for classification tasks, outperforming supervised learning on ImageNet~\cite{Russakovsky2015} using the same architecture ResNet50~\cite{He2016}. SimCLRv2~\cite{Chen2020} is an improvement of SimCLR using a larger encoder, projection head and momentum contrast (MoCo)~\cite{He2020} that uses a queue and a moving average encoder to stabilize the training. Nevertheless, contrastive learning methods require a large batch size and a large number of negative samples to work effectively~\cite{Chen2020Simple}. This makes it difficult to scale to large datasets and models. 

In contrast to contrastive learning, masked autoencoding (MAE) learns representations by generating pixel values of the masked out parts of images, honing similar spirit to masked language modeling (MLM)~\cite{Devlin2019} in NLP. It has been preliminarily explored on vision transformer (ViT) model at the same time the architecture has been proposed~\cite{Dosovitskiy2020vit}. However, the performace of masked autoencoding in the original paper is not as good as supervised learning. To make this works, He et al.~\cite{He2022} propose masking out 75\% of an image rather than 50\% experimented previously. Furthermore, an asymmetric encoder-decoder architecture of MAE is introduced. An encoder operates only on the visible pixels and a light weight decoder operates on the encoded tokens as well as the mask tokens presented afterwards. Compared to CL, batch size, data augmentation and number of negative samples are not as critical for MAE, rendering it more scalable. MAE outperforms supervised learning and supervised pre-training on ImageNet using the same architecture ViT. Additionally, MAE has been evaluated on segmentic segmentation on ADE20k~\cite{Zhou2018}, highlighting that it can be used for dense prediction downstream tasks as well. Nonetheless, working at a pixel level, MAE is computationally expensive compared to CL.

Like MAE, bidirectional encoder for image transformer (BEiT) first learns image patch tokenizer and predicts the tokenized values of the mask tokens instead of the pixel values as in MAE~\cite{Bao2022beit}. BEiT has been shown to outperform supervised learning and supervised pre-training on ImageNet~\cite{Russakovsky2015}. It has also been evaluated on segmentic segmentation on ADE20k achieving state-of-the-art performace at that time. As BEiT requires tokenizer to be learned, it adds more complexity to the pre-text task compared to MAE. Combining restorative and discriminative pre-text tasks, DINOv2~\cite{Oquab2024dinov} proposes to learn representations by predicting the tokenized values of the mask tokens as well as matching the class tokens of teacher and student networks on different crops of the same image. This approach has been shown to outperform other image SSLs at the time. Performing representation learning at the token level allows DINOv2 to be more efficient than pixel-level SSLs. In addition, I-JEPA~\cite{Assran2023} adapts this idea one step further. Attempting to find the most efficient approach to learn feature representations, I-JEPA proposes to predict directly the encoded vectors of mask tokens instead of the tokenized values. Evaluated on image classification, I-JEPA has shown to be comparable to other SSLs while being simpler and more efficient. 

While reading texts provides a natural way to learn natural languages, visual representations are not limited to static 2D images. Striving to incorporate a temporal dimension, extensions of existing visual SSLs have been made. VideoMAE introduces tube masking to MAE~\cite{He2022} along with an extremely high masking ratio of 90\% to 90\%~\cite{Tong2022VideoMAE}. Without external data, VideoMAE demonstrates its understanding of the video achieving state-of-the-art results to supervised learning on Kinetics-400~\cite{Kay2017Kinetics} and Something-Something V2~\cite{Goyal2017Something-SomethingV2}. Generating pixel values on video imposes even more computational limitation, as one cannot efficiently fit a clip of a video with a large number of frames. V-JEPA extends from I-JEPA~\cite{Assran2023}, predicting the encoded vectors of mask tokens on video~\cite{Bardes2024Vjepa}. As JEPA is more efficient than MAE, V-JEPA is a more efficient approach to learn representations on video. Additionally, V-JEPA has been shown to outperform VideoMAE. These ideas have potential to be applied to 3D medical images, replacing temporal axis with depth axis.

% Self-supervised learning in medical imaging
\section{Self-supervised learning in medical imaging}
In medical imaging, self-supervised learning would be highly beneficial as human annotations for medical images are expensive and time-consuming. However, as it is a specialized domain, recent SSL methods in CV are not yet fully explored. There are challenges in applying SSL in medical imaging. First, medical images can be in 3D, which is not common explored in SSL for natural which is mostly in 2D. Second, medical images can be in various modalities such as X-ray, CT, MRI, and ultrasound. Specifically, in our study, we focus on optical coherence tomography (OCT) images of arteries which are 3D and does not have any public datasets for SSL. This poses a challenge for us as there is no direct research directly addressing our problem.

% Basic rotation, jigsaw, rubik
 Following the same idea, Song et al.~\cite{Song2022} use rotation prediction, instance discrimination and VAE~\cite{Kingma2013}. Using VAE as a pre-text task, the model is semgnetaion-ready for COVID infection segmentation on lung CT images. This method outperforms supervised learning on similar architecture U-Net~\cite{Ronneberger2015} and U-Net++~\cite{Zhou2020}. Approaching 3D medical images differently, Rubik's cube solving is used as a 3D pre-text task where the model learns to classifies the orientation and ordering of a shuffled volumetric cube. Even though discriminative, Rubik's cube shows to improvement 3D segmentation in medical images ~\cite{Zhuang2019}. Jigsaw learns feature representations by predicting the correct order of shuffled patches~\cite{Noroozi2016}. Deep clustering clusters features vectors of the image in an unsupervised manner and uses thes clusters as pseudo-labels for classification~\cite{Caron2018}. Applying one or multiple pre-text tasks has been the main theme in several medical imaging SSL research papers ~\cite{Zhou2021, Zhang2021, Dufumier2021}. Taleb et al. ~\cite{Taleb2020} study 5 separated 3D self-supervised learning tasks, namely predicting latent vectors of adjacent patches, predicting the location of a given patch, solving jigsaw, predicting rotation angle and contrastive learning. In their study, predicting latent vectors of adjacent patches yields the best results in downstream tasks.

% Constrastive learning
TS-SSL~\cite{Zhang2021} proposes an SSL on 2D spectral domain optical coherence tomography (SD-OCT) images of retina to better classify retinal anomaly. It learns representation from classification labels, discriminative and generative tasks simultaneously. Using contrastive loss, it maximizes the similarity between rotated views or shuffled patches of the same image and minimizes the similarity between views of different images. At the same time, it uses a same representation to predict the rotation angle of the rotated views and the order of the shuffled patches. Along these tasks, TS-SSL also uses classification labels to predict the class of the image. However, this method only works better than supervised learning when 10\% of the labels are used for training. TS-SSL is not the only method that adopts modern SSL methods to medical imaging. Dufumier et al.~\cite{Dufumier2021} adds anoter layer to contrastive learning by using meta-data available in medical images. Specifically, they use age of the patient to indicate the degree of similarity between different images. This method improves bipolar disorder, Alzheimer and schizophrenia classification on 3D brain MRI images. Similarly, CLIP loss~\cite{Radford2021CLIP} can be used to leverage multi-modalities for representation learning ~\cite{Hager2023}. Encouraging images and their tabular meta-data, ubiquitous in medical imaging, to have similar representations. This method has been shown to improve classification tasks on brain MR images. 

Exploring boon of contrastive learning in segmentation, He et al.~\cite{He2022Intra} propose an intra- and inter-slice contrastive learning for point supervised OCT fluid segmentation of retina. The nature of this retinal OCT is 3D. While training segmentation model with U-Net~\cite{Ronneberger2015}, the authors leverage the fact that adjacent slices of the same volume are highly correlated, inter-slice CL is proposed to maximize the similarity between encoded vectors of adjacent slices as well as their segmentation masks. Segmentation masks are also compared with their ground truth masks and cross-entropy loss. Segmented masks are subsequently used to select the locations of small patches to be used for intra-slice CL. Intra-slice CL maximizes the similarity between encoded fluid patches and the backgroun fluid patches. Its result outperforms other point-based segmentation methods but is not as good as fully supervised learning. Besides spatio consistency as previous work, temporal consistency can also be imposed~\cite{Ren2022}. Given that images are taken from the same patient at different time points, temporal consistency can be used to improve the segmentation of the images. This method has been shown to improve the segmentation of brain MRI images.

% Restorative tasks
Alternatively, generative tasks have been proposed as pre-text tasks for medical imaging. Patch shuffling~\cite{Chen2019} propose a restorative task for 2D medical images of MRI, CT and ultrasound. First patches are randomly cut from the original image and placed to random locations. The model learns to restore the images back to their original versions. This method improves the downstream segmentation tasks in a data-scarce setting. In-painting~\cite{Pathak2016} removes a part of the image and the model learns to predict the missing part. Advancing further, Genesis~\cite{Zhou2021} explore image distortion algorithms for 3D restorative pre-text tasks on CT images. Thorough experiments show that Genesis outperforms specialized 3D state-of-the-art segmentation models and other pre-text tasks including de-noising~\cite{Vincent2010}, in-painting~\cite{Pathak2016}, jigsaw~\cite{Noroozi2016}, deep clustering~\cite{Caron2018}, rubik's~\cite{Zhuang2019} and patch shuffling~\cite{Chen2019}. This establishes a new base line for 3D SSL in medical imaging showing that mixing pre-text tasks is beneficial. Persuing a similar concept, SwinUNTER~\cite{Tang2022} proposes training ViT-based model with a combination of in-paining, constrastive and rotation.

Introducing deep clustering~\cite{Caron2018} idea to Genesis~\cite{Zhou2021}, TransVW adds a classification task to the restorative task of Genesis. First, visual words are mined automatically and grouped into clusters with deep latent features. These visual words (VW) are used to be classified and restored. Visual words improve segmentation and classification task in some datasets compared to Genesis~\cite{Haghighi2021}. DiRA~\cite{Haghighi2024} further improves TransVW by adding adversarial model to tell apart the restored images and the original image. Additionally, constrastive learning is used as an additional pre-text task. Combining, discriminative, restorative and adversarial training, DiRA outperforms TransVW and train-from-scratch models. Eventhough, TransVW and DiRA have shown to be better than Genesis, their evaluations are not as thorough. TransVW only outperforms Genesis in some datasets, while DiRA only evaluates its performace against TransVW.

A gap between pre-text tasks studied in medical imaging and those introduced in recent visual SSL research is observed. While visual SSL research has been moving towards more efficient and simpler pre-text tasks such as DINOv2~\cite{Oquab2024dinov}, MAE~\cite{He2022}, and I-JEPA~\cite{Assran2023}, medical imaging SSL research has been focusing on blending multiple pre-text tasks together. An attempt to fill this gap is evident in Genesis~\cite{Zhou2021} which works on 3D medical images at a pixel level and has shown to be better than other pre-text tasks. More studies are needed to explore simpler and more efficient pre-text tasks for medical imaging, closing the gap between visual SSL and medical imaging SSL. Concurrently, a robust evaluation in medical imaging research must be done as many reported results may not be reproducible~\cite{Isensee2024}.
% Alternatives
\section{Self-supervised learning alternatives}
Self-supervised learning has a great potential in medical imaging. If simple and robust pre-text tasks can be found, it can be used to learn representations from medical images without human annotations. However, such tasks are being explored and much work on the evaulation have to be done. Alternatives are introduced to efficiently train the models with existing small medical datasets.

SuPreM suggests to pre-train models with supervised learning on a large annotated datasets. SuPreM collates 9,262 CT volumes, pre-trains a models to segment subset of organs and fine-tune them on mutually exclusive subset of organs. The experiments show that it requires less data to learn meaning representations than SSL while also provides better transferability~\cite{Li2024}.

Rather than pre-training, nnUNet proposes a self-configuring framework for medical image segmentation. The framework automatically build UNet-like model for 2D and 3D medical images for a given dataset. It has been shown to robustly outperform state-of-the-art models, including popular Transformer-based and Mamba-base architectures, on several medical image segmentation tasks and works well on small datasets due to it robust positive data sampling and augmentation~\cite{Isensee2020}. Follow up work on nnUNet has been done showing that many reported results may not be reproducible due to many reasons. The study highlights that nnUNet is the consistently best performing model on the tasks it has been evaluated on without additional data~\cite{Isensee2024}.

Previous work at IMES to segment OCT images have been done with SegFormer pre-trained with ADE20k. SegFormer is a ViT-based model that enforces multiple resolution at different depths of the transformer. Simple MLP~\cite{Rumelhart1986} decoder heads are introduced to combine the features from different depths. The model has been shown to be state-of-the-art on segmentic segmentation at the time~\cite{Xie2021SegFormer}.

Taking a different approach, RADIO proposes to learn representations from already learned models. RADIO is a student model that learns to predict a same feature from multiple teacher models, including DINOv2~\cite{Oquab2024dinov}, SAM~\cite{Kirillov2023SAM} and CLIP~\cite{Radford2021CLIP}. In addition, RADIO is trained to be resolution-agnostic, allowing it to understand images at different resolutions. RADIO has been shown to provide a high resolution feature map that can be used for downstream tasks~\cite{Ranzinger2024RADIO}. While SSLs has a great potential on medical imaging, their approaches should be compared to all other existing ones in order to ensure a comprehensive comparison.

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion, you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\end{document}