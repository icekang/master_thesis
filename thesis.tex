%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% To avoid any implication, this template is released into the
% public domain / CC0, whatever is most convenient for the author
% using this template.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[T1]{fontenc}

\title{Self-supervised learning\\for calcium segmentation on coronary OCT images}
\author{Naravich Chutisilp}
\supervisor{Karim K. Kaldry, Ph.D. candidate}
\adviser{Professor Elazer R. Edelman, M.D., Ph.D.}
\coadviser{Professor Maria Brbi\'c, Ph.D.}
\expert{Farhad Rikhtegar Nezami, Ph.D.}

% \newcommand{\sysname}{FooSystem\xspace}

\dedication{
    \begin{raggedleft}
        It’s no use going back to yesterday, because I was a different person then.\\
        --- Lewis Carroll, Alice’s Adventures in Wonderland\\
    \end{raggedleft}
    \vspace{4cm}
    \begin{center}
        Dedicated to my lovely family, my dear friends, and kind people who have given opportunity and support to this lucky live I have.
    \end{center}
}
\acknowledgments{
% This is where you thank those who supported you on this journey. Good examples
% are your significant other, family, advisers, and other parties that inspired
% you during this project. Generally this section is about 1/2 page to a page.

% Consider acknowledging the use and location of this thesis package.

% Define your acknowledgments in \texttt{\textbackslash{}acknowledgments\{...\}}
% and show them with \texttt{\textbackslash{}makeacks}.

    I would like to show my deep gratitude to my mom and dad for their unwavering supports and firm trust to let me choose my own path. I would love to thank my two lovely sisters, Namking and Namkow, for their continuous love and understanding. 

    I am also grateful to all the friends I have made during my time at EPFL, Set, James, Kwang, Ice, Sundae, Nai, Cindy, Jenestin, Thomas, Paulina, Fah, Ting-Wei, Pin-Yen, Yi-Kai, Hong-Bin, Leo, Edvin, Anthon, Aamir, Jirka, and many more people I have not mentioned, who have made my life abroad memorable and amicable. Going to EPFL is the first time living abroad for more than a month and I could not have asked for a better experience than what I have had. Simple dinner in the evening every day after school, meaningful conversations about everything, and trips to alpine mountains and european cities have made this little boy feel accompanied in the middle the unknown future.

    Furthermore, I could not forget the friends have made during my time at MIT, Mee, Pooh, Cue, and Champ who have made my final Master's semester fun and unforgettable. Coming to MIT is a dream-comes-true, yet it was not easy to leave my friends in Switzerland and start anew in the US. Unexpectedly, since the first day I arrived, I was welcomed with open arms and got shown to many places in Boston. Worries and fears were replaced with excitement and joy. Hiking in the White Mountains, visiting California and silicon valley, walking along the freedom trail, enjoying delicacies of Lobster rolls and clam chowder, board game night every Friday are just a few of the many memories I have made in the US that I will never forget.

    Additionally, it would not be possible if I did not have support from my friends in Thailand, Meak, Korn, Marie, Jump, Copter, Nat, Bank, Pinn, V, Poom, Pewt, Choomp, Krist, Minnie, May, Kuan and uncountable more that I could not fit into this section. Calls and messages from these lovely lives have always been a source of strength and comfort. Through though times and good times, they have always been there for me. Even though we are 9,188 km apart, or even 13,707 km apart, it feels like they are always by my side. It is a blessing to have friends like them and I have always been grateful for that.

    I would like to thank Prof. Elazer Edelman and Karim Kaldry for giving me this unparalleled opportunity to join the IMES lab at MIT and let me complete my Master's thesis here, as well as providing me advises and support on my research. Opportunity 
}


\begin{document}
\maketitle
\makededication
\makeacks

\begin{abstract}
The sysname tool enables lateral decomposition of a multi-dimensional
flux compensator along the timing and space axes.

The abstract serves as an executive summary of your project.
Your abstract should cover at least the following topics, 1-2 sentences for
each: what area you are in, the problem you focus on, why existing work is
insufficient, what the high-level intuition of your work is, maybe a neat
design or implementation decision, and key results of your evaluation.
\end{abstract}

\begin{frenchabstract}
For a doctoral thesis, you have to provide a French translation of the
English abstract. For other projects this is optional.
\end{frenchabstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

The introduction is a longer writeup that gently eases the reader into your
% thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
In the second paragraph you can introduce the main challenge that you see.
The third paragraph lists why related work is insufficient.
The fourth and fifth paragraphs discuss your approach and why it is needed.
The sixth paragraph will introduce your thesis statement. Think how you can
distill the essence of your thesis into a single sentence.
The seventh paragraph will highlight some of your results
The eights paragraph discusses your core contribution.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your
% work. This is not necessarily related work but technologies and dependencies
% that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.
Coronary artery diseases (CAD) is a leading cause of heart diseases and fatalities worldwide~\cite{Virani2021Heart, Wu2015}. It is primarily caused by atherosclerosis, a process of plaque building up in the coronary arteries, partially or totally obstructing the blood flow~\cite{Shahjehan2023}. Commonly, doctors use percutaneous coronary intervention (PCI) as a treatment for CAD. This procudure is done by enlarging the narrowing artery~\cite{Ahmad2023}. However, the outcome of PCI can be affected by the presence of coronary artery calcification (CAC) which hinders stent expansion, leading to stent underexpansion, distortion, dislodgement or loss~\cite{Hennessey2023}. Thus, a process to remove these plaques is a prerequisite to increase the success of PCI. Intravascular lithotripsy (IVL) is a novel technology that uses sonic pressure waves to break the calcification leisons, modifying them into fractures, allowing stent deployment to be effective~\cite{Butt2023}. This technique requires accurate definition of the calcified lesions in order to select the optimal strategy for IVL~\cite{Butt2023}. Generally, the calcified lesions are identified using intravascular imaging techniques such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT)~\cite{Butt2023}. Specifically, OCT can deliver sufficient accuracy higher than IVUS~\cite{Fujimoto2003, Costopoulos2016}. However, manual segmentation of calcified lesions in OCT images is time-consuming and subjective~\cite{Segars2013, Oktay2020, Carpenter2022}. Therefore, an automatic segmentation method is needed to improve the efficiency and accuracy of the process~\cite{Carpenter2022}.

2D segmentation is a common approach for image segmentation. Convolutional layers are used to extract features from the input images. The layer uses 2D kernels that slide over the input image to extract visual features. U-Net introduces an encoder-decoder architecture with skip connections to explicitly utilizes features at different resolutions for segmentation. Encoder is a series of convolutional layers with downsampling, while decoder is a series of convolutional layers with upsampling. Skip connections are added between encoder and decoder to combine features at different resolutions. This enables the model to learn and utilize representations at different levels~\cite{Ronneberger2015}. Additionally, 3D segmentation modifies 2D kernels to 3D kernels. This allows the model to work on 3D volumes directly, preserving the spatial information in the depth dimension. This allows the model to learn the features in 3D space, instead of in 2D space. It constrains the data to always be in 3D volumes, augmenting the cost of annotation and computation, at the same time, leveraging more information from the data to be used and potentially improve the performance of the model.

%TODO: add a photo of U-Net architecture

Following the success of transformer architecture in NLP~\cite{Vaswani2017}, Vision Transformer (ViT) is introduced as an attempt to apply transformer to 2D image classification~\cite{Dosovitskiy2020vit}. It has been empirically proven to be successful in wide range of tasks. Thus, image segmentation is also explored using ViT. Transformer-based 2D segmentation is an alternative approach to its CNN-based counterpart. Instead of using convolutional layers, it pachifies the input image into patches and tokenized them using CNN. These tokens are added with positional information and fed into a transformer. There are several transformer-based architectures for image segmentation proposed in the literature. However, it is common in visual SSL that a linear projection layer is added to the output of the ViT to predict the segmentation masks. To enable a transformer model to do 3D segmentation, patchification must be modified to 3D patches, and the CNN-based tokenizer should operate with 3D kernels instead. In addition, the positional encoder should be modified to include the 3rd dimension. The rest of the architecture can be kept the same as 2D segmentation.

%TODO: add a photo of ViT architecture with linear projection

Medical image segmentation is a challenging task that typically requires specialized annotators to provide pixel-level groud truth labels for training. This leads to several researches seeking to find a cost-effective way for medical image annotation~\cite{Fu2012, Gal2017, Beluch2018, Rahimi2021}. Alternatively, self-supervised learning (SSL) has been proposed as a promising approach to learn representations from the data itself without human annotations. Although SSL is now a common approach in natural language processing (NLP), it is still under active research in computer vision (CV) with collection of practices being made~\cite{Balestriero2023}. 

Originally, SSL in CV has been done using traditional pre-text tasks such as  autoencoder~\cite{Hinton2006} and denoising autoencoder~\cite{Vincent2008}. Such pre-text tasks are empirically useful in some use cases, but their benefits are not yet on par with SSL in NLP. Hence, pre-text tasks that explicitly enbrace visual semantics have been proposed. Such tasks include solving jigsaw puzzle~\cite{Noroozi2016}, rotation prediction~\cite{Gidaris2018}, and colorization~\cite{Larsson2017}. Recently, visual SSLs have been moving towards more efficient and simpler pre-text tasks which can be categorized into two groups, discriminative and generative tasks. Discriminative tasks learn representations by learning to discriminate between same and different views of images. Generative tasks learn representations by generating values of the masked out parts of images. These tasks have been shown to be more effective than traditional pre-text tasks in learning representations~\cite{Chen2020Simple, He2020, He2022, Bao2022beit}.

Even though, modern discriminative tasks in visual SSLs have been studied in medical imaging, their generative counterparts have not been explored. In medical imaging, researches in SSL have been focusing on amalgamating multiple pre-text tasks together. Several papers study combination of traditional pre-text tasks to improve classification and segmentation tasks in medical imaging~\cite{Noroozi2016, Zhuang2019}. More recent researches explore blending discriminative tasks with traditional pre-text tasks~\cite{Zhou2021, Zhang2021, Dufumier2021, Taleb2020, Zhang2021, He2022Intra, Ren2022}. To our best of knowledge, the only generative task being studied in medical imaging is restorative~\cite{Pathak2016,Chen2019,Zhou2021,Tang2022,Haghighi2021,Haghighi2024}.

In recent years, generative tasks are now gaining attention in medical imaging following their precedent success in visual SSLs. Masked autoencoder (MAE) which learns representations by filling the masked out parts of images has been shown to be effective~\cite{He2022}. Despite its simplicity, working at a pixel-level is computationally expensive. Bidirectional encoder for image transformer (BEiT) proposes to work on tokenized level instead of pixel level~\cite{Bao2022beit}. This makes BEiT more efficient than MAE. DINOv2~\cite{Oquab2024dinov} combines discriminative and generative tasks, working on tokenized level and feature level. This approach has been shown to be more efficient and effective than other image SSLs. I-JEPA takes this to a level higher by directly predicting the encoded vectors of masked out parts~\cite{Assran2023}. This approach is simpler and much more scalable than other generative SSLs. These methods have been shown to be effective in learning representations in visual SSLs. However, they have not been explored in medical imaging. Particularly, if working on embedding level proves to be beneficial in medical imaging, it can be a game changer as this can be the most efficient and greatly scalable approach to learn representations in unannotated medical imaging.

% TODO: add about alternatives

%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

% Introduce and discuss the design decisions that you made during this project.
% Highlight why individual decisions are important and/or necessary. Discuss
% how the design fits together.

% This section is usually 5-10 pages.

\section{Datasets}\label{sec:design:datasets}
The main goal of this project is to study the effectiveness of self-supervised learning algorithms for calcium segmentation on coronary OCT images. Therefore, we categorized the datasets into two groups, annotated and unannotated datasets. Each category has its own characteristics and relevance to the problem. Calcium OCT (Dataset \ref{enum:calcium-dataset}) contains 8 3D OCT volumes in total annotated with calcium plaques. Lumen and Wall OCT (Dataset \ref{enum:lumen-and-wall-dataset}) is a dataset of 20 3D volumes annotated with lumen and wall. These two datasets are mutually exclusive. Furthermore, 500 3D volumes are unannotated. These volumes are divided into 3 groups, 167 volumes before IVL, 169 volumes after IVL, and 164 volumes after stent deployment. Even though these volumes are unannotated, a tabular meta-data is available for each volume. This meta-data describes morphological properties of the calcium plaques, lumen and wall. This can be summarized as follows:

\begin{enumerate}
    \item Annotated datasets:
    \begin{enumerate}
        \item \label{enum:calcium-dataset} \textbf{Calcium OCT} is a dataset of coronary OCT images with calcium annotations before IVL.
        \item \label{enum:lumen-and-wall-dataset} \textbf{Lumen and Wall OCT} is a dataset of coronary OCT images with lumen and wall annotations before IVL.
    \end{enumerate}
    \item \label{enum:unannotated-dataset} Unannotated datasets:
    \begin{enumerate}
        \item \textbf{Pre-IVL} is a dataset of coronary OCT images before IVL.
        \item \textbf{Post-IVL} is a dataset of coronary OCT images after IVL.
        \item \textbf{Post-Stent} is a dataset of coronary OCT images after stent deployment.
    \end{enumerate}
\end{enumerate}

Each dataset has its own purpose in this study. Calcium OCT is used for evaluating effectiveness of each algorithm on calcium segmentation. On the other hand, Lumen and Wall OCT can be used for supervised pre-training of the algorithms, while unannotated datasets are used for self-supervised pre-training. Additionally, the tabular meta-data can be used to evaluate the effectiveness of the algorithms on multi-modalities, but this is not the main focus of this study.

In Calcium OCT, only 1 class is annotated, calcium. The size of each image is $500\times 500$ while the depth varies from $375$ to $539$. An example of the annotated image is shown in Figure~\ref{fig:calcium-oct}. The dataset is split into 6 training volumes and 2 testing volumes. In the training volumes, they are split into 3 folds of training and validation sets. In each fold, the model can be train using the training set, while the best model can be selected using the validation set. Subsequently, all models trained on each fold can be evaluated on the testing set that has not been seen during training. Eventually, the performace on the testing set can be averaged to get the final performace. This is done to ensure the robust evaluation of the models.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_calcium_oct_sample.pdf}
    \caption{An example of annotated image in Calcium OCT dataset.}
    \label{fig:calcium-oct}
\end{figure}

Lumen and Wall OCT has 2 classes, lumen and wall, and is split into 16 training volumes and 4 testing volumes. The size of each image is $500\times 500$ while the depth varies from $270$ to $540$. An example of the annotated image is shown in Figure~\ref{fig:lumen-and-wall-oct}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_law_oct_sample.pdf}
    \caption{An example of annotated image in Lumen and Wall OCT dataset.}
    \label{fig:lumen-and-wall-oct}
\end{figure}

The unannotated datasets are of size $500\times 500$. They are not splitted into training and testing sets since they are only used for self-supervised pre-training. However, each self-supervised learning algorithm may split them into train and validation set for hyperparameter tuning. Samples of each modality are shown in Figure~\ref{fig:unannotated-oct}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_unlabeled_oct_sample.pdf}
    \caption{Samples of unannotated OCT images.}
    \label{fig:unannotated-oct}
\end{figure}
\section{Algorithms}
\subsection{CNN-based Supervised Learning - nnUNet}\label{sec:design:nnunet}
nnUNet is a self-configuring framework for medical image segmentation~\cite{Isensee2020}. First, the data fingerprint must be introduced, specifying number of image modalities, how should the algorithm normalize each modality and the number of segmentation classes and regions. Additional information about data including intensity ranges, spacing, shape, and resolution are derived from the data metadata themselves. With this fingerprint, nnUNet automatically decides the network topology, sampling strategy, batch size and patch size. It is also worth noting that nnUNet provides a supervised pre-training pipeline for the model to be trained on other datasets before fine-tuning on the target dataset.

To apply the algorithm to our datasets, we designed two approaches. The first approach is to directly train the model on Calcium OCT. In addition to the former approach, on the second approach, we also pre-train the model on Lumen and Wall OCT before fine-tuning on Calcium OCT. 

% Discuss 2D and 3D nnUNet for segmentation
nnUNet provides both 2D- and 3D-segmentation architectures as well as a cascade 3D architecture of full resolution and low resolution networks. In our initial experiments, we find that 2D and 3D architecture are performing better than the cascade architecture. This leads us to subsequently only use 2D and 3D architectures for the rest of the experiments.

% TODO: add a photo of nnUNet pipeline

\subsection{Transformer-based Supervised Learning - SegFormer}
SegFormer is a transformer-based model with multi-resolution feature fusion with simple multi-layer perceptrons (MLP~\cite{Rumelhart1986})~\cite{Xie2021SegFormer}, and has been pre-trained on ADE20k~\cite{Zhou2018} datasets. Resembling encoder of U-Net that downsamples the input image to lower resolution at each layer, SegFormer explicitly downsamples the input image with overlap patch merging which unifies adjacent patches into a single patch. This allows the model to learn the features at different resolutions. Furthermore, SegFormer employs these multi-resolution features with All-MLP decoder that projects lower resolution features to match their one-level higher resolution features' channels. The decoder then upsamples the features up to origianl resolution and concatenates them with the higher resolution features. Finally, the decoder predicts the segmentation masks all using only MLPs and upsampling layers. 

SegFormer architectures are available in multiple sizes, from small to large, with the largest model, MiT-B5, containing 84.7 parameters. We use the largest pre-trained model to fine-tune on Calcium OCT for 80,000 iteration which we find is long enough to see the model converges.
% TODO: add a photo of SegFormer architecture

\subsection{Transformer-based Supervised Learning - RADIO}
RADIO is a distillation framework that allows one than one vision foundation models to be its teacher~\cite{Ranzinger2024RADIO}. The author evaluates the model on 2D segmentation task using batch normalization and linear projection as the decoder head. We follow the same evaluation setting in order to be able to utilize their pre-trained model which is a ViT model. We use RADIO pre-trained ViT-H/16 model and fine-tune it on Calcium OCT. The model is trained for 80,000 iterations which we find is long enough to see the model converges.

\subsection{Transformer-based Medical Supervised Learning - SuPreM}~\label{sec:design:suprem}
SuPreM is a supervised pre-training method for medical image segmentation~\cite{Li2024}. As we do not have a large annotated dataset, we use publicly available pre-trained models of the paper to fine-tune on Calcium OCT. Following their paper, we use their best performing model which is SwinUNTER~\cite{Tang2022} with their training scripts. SwinUNTER is a transformer-based model designed to segment brain tumors in 3D MRI images. The architecture is based on Swin transformer as its encoder which is a multi-scale transformer with shifted windows~\cite{Liu2021Swin}. SwinUNTER utilizes a CNN-based decoder called similar to UNTER, hence the name SwinUNTER. The decoder is a series of convolutional layers with upsampling merging features at different levels via skip connections~\cite{Hatamizadeh2022}.

Because SwinUNTER is also, on its own, another transformer-based model with self-supervised learning, we conduct three experimentals to evaluate the algorithms. The first setting directly utilizes the pre-trained model on SuPreM large datasets, and fine-tunes it on Calcium OCT. The second setting uses self-supervised SwinUNTER model, and fine-tunes it on Calcium OCT. The last setting uses SwinUNTER model without any pre-training, and trains it on Calcium OCT. Consequently, we can evaluate the effectiveness of SuPreM pre-training and self-supervised learning proposed in SwinUNTER on Calcium OCT.

\subsection{Unimodal Self-Supervised Learning - V-JEPA}
V-JEPA is a self-supervised learning method that acquires representations by predicting the encoded vectors of the masked segments of a video~\cite{Bardes2024Vjepa}. We desgin two experiments to study the effectiveness of V-JEPA on OCT images. Firstly, we directly fine-tunes the pre-trained V-JEPA model on Calcium OCT. This means that the model is a simple ViT~\cite{Dosovitskiy2020vit} and is trained on natural videos and then fine-tuned on medical images . Secondly, we apply V-JEPA pre-training on unannotated OCT images, and fine-tune the resulting model on Calcium OCT. Since the original paper does not evaluate the model on segmentation tasks, we will discuss the implementation of the decoder head in Section~\ref{sec:implementation:vjepa}.

% TODO: add a photo of V-JEPA framework

\subsection{Unimodal Self-Supervised Learning - Genesis}
Genesis is a self-supervised learning method that learns representations by restoring the original images from their corrupted versions~\cite{Zhou2021}. In their paper, Genesis is applied on a U-Net architecture, containing 15M parameters. We adapt their method to the same 3D nnUNet architecture in Section~\ref{sec:design:nnunet} which is of 30M parameters. Thereafter, we apply Genesis pre-training on unannotated OCT images, and fine-tune the resulting model on Calcium OCT.

% TODO: add a photo of Genesis framework

\subsection{Multi-modal Self-Supervised Learning - CLIP}
Contrastive Language-Image Pre-trainig (CLIP~\cite{Radford2021CLIP}) is a self-supervised learning method that learns representations by matching encoded vectors of images and their text captions. We adapt CLIP to OCT images by using co-registered Pre-IVL, Post-IVL, and Post-Stent images. We use the same nnUNet architecture as in Section~\ref{sec:design:nnunet} to pre-traine CLIP on and fine-tune the final model on Calcium OCT. Note that the original paper does not publish the code for CLIP, we modify the code from a publicly available project~\cite{Shariatnia2021}. Original paper proposes a pre-training framework on language and image. However, we apply the framework on multi-modalities of OCT images. We will discuss the adaptation of the framework to our specific use case in Section~\ref{sec:implementation:clip}.
% TODO: add a photo of CLIP framework

%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

% The implementation covers some of the implementation details of your project.
% This is not intended to be a low level description of every line of code that
% you wrote but covers the implementation aspects of the projects.

% This section is usually 3-5 pages.
\section{CNN-based Supervised Learning - nnUNet}
We apply publicly available code from nnUNet~\cite{Isensee2020} on our datasets. Additionally, we add an early stopping to the optimizer and train for a longer period. As we find that 3D segmentation of nnUNet is performing significantly worse than its 2D counterpart, we experiment different patch sizes on 3D segmentation instead of its automatically configured patch size. We find that reducing the depth of the original patch size from $112$ to $32$ improves the performance significantly. This could be due to the fact that smaller depth allows the model to have more examples to learn from. In addition, inspired by SuPreM, we also experiment with supervised pre-training on Lumen and Wall OCT before fine-tuning on Calcium OCT. nnUNet provides a supervised pre-training script as well as a pipeline to fine-tune on new datasets.

\section{Transformer-based Supervised Learning - SegFormer}\label{sec:implementation:segformer}
We use the pre-trained SegFormer model and their training script from MMSegmentation~\cite{mmseg2020}. As SegFormer only supports 2D segmentation, we preprocess our Calcium OCT datasets into 2D images with their segmentation masks, and train the model on the newly formatted dataset.

\section{Transformer-based Supervised Learning - RADIO}
We use the pre-trained RADIO model and their training script from available as MMSegmentation~\cite{mmseg2020}. As RADIO also only supports 2D segmentation, we share the same pre-processing datasets with SegFormer in Section~\ref{sec:implementation:segformer}.

\section{Transformer-based Medical Supervised Learning - SuPreM}
SuPreM has publicly available pre-trained model and codes~\cite{Li2024}. Thus, we format our Calcium OCT dataset into the same format as their datasets. Preserving their training algorithm, we use their fine-tuning script on our datasets. We experiment with three settings as described in Section~\ref{sec:design:suprem} as they are all available in their original codes.

\section{Unimodal Self-Supervised Learning - V-JEPA}\label{sec:implementation:vjepa}
Because of V-JEPA's lack of evaluation on segmentation tasks, it is necessary to design a decoder head for V-JEPA. The paper proposes an attentive decoder head for classification tasks. 

Therefore, we also develop an attentive decoder head for segmentation, drawing inspiration from the MAE framework~\cite{He2022}, which directly predicts pixel values. Specifically, the decoder head is an encoder-only transformer of 1 depth and 12 heads followed by a linear projection layer that yields embedding vectors into a vector of size \(\text{patch size} \times \text{patch size} \times \text{number of classes}\). Besides, we also experiment with other variations of decoding architecture, such as a simple batch normalization layer followed by a linear projection, similar how RADIO~\cite{Ranzinger2024RADIO} and DINOv2~\cite{Oquab2024dinov} fine-tune their models on segmentation tasks. This is shown in Figure~\ref{fig:vjepa-attentive-and-batchnorm-decoder-head}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/fig_implementation_vjepa_attentive_and_batchnorm_decoder.pdf}
    \caption{Attentive and linear batch normalization decoder head.}
    \label{fig:vjepa-attentive-and-batchnorm-decoder-head}
\end{figure}%

Additionally, inspired by SegFormer~\cite{Xie2021SegFormer}, we also experiment with simple multi-layer perceptrons (MLP~\cite{Rumelhart1986}) on each feature vectors at different depths followed by a linear projection to combine those features into a segmentation mask. This enables the decoder to multiple features at different levels. Unlike SegFormer, each features are not downsampled, so they are not explicitly forced to be different resolutions. However, this methods allows us to preserve the original architecture of V-JEPA's pre-training. Thus, we can utilize their pre-trained weight on our segmentation task. Multi-feature decoder head is shown in Figure~\ref{fig:vjepa-multi-feature-decoder-head}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig_implementation_vjepa_multifeat.pdf}
    \caption{A multi-feature decoder head. Features at each transformer layer are projected to the same dimension and concatenated before being passed to a convolutional layer that produces the segmentation mask.}
    \label{fig:vjepa-multi-feature-decoder-head}
\end{figure}

\section{Unimodal Self-Supervised Learning - Genesis}
Genesis has made their code available~\cite{Zhou2021}. Their implementation requires first to pre-process unannotated OCT images into small cubes of size $64\times 64\times 64\times 32$. These cubes will then be loaded and corrupted on the fly while training. The author recommends $96$ cubes per volume as increasing the number of cubes per volume may not improve the performance while increasing the computation cost. We follow this recommendation and apply a sampling strategy to randomly sample $96$ cubes from each volume. This is done to ensure that the cubes sampled contain enough information to learn the representations as OCT images are filled with background unlike CT images used in the original paper. The sampling strategy is done by calculating the average intensity of each cube and resample the cube if the average intensity is below a certain threshold. The resulting cubes can be seen in Figure~\ref{fig:genesis-cubes-with-threshold}. Without this strategy, the sampled cubes are mainly background noises which is shown in Figure~\ref{fig:genesis-cubes-without-threshold}. We then train the model on these cubes until convergence.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube.png}
        \caption{Samples of cubes from unannotated OCT images with threshold sampling strategy.}
        \label{fig:genesis-cubes-with-threshold}

    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube_no_threshold.png}
        \caption{Samples of cubes from unannotated OCT images without threshold sampling strategy.}
        \label{fig:genesis-cubes-without-threshold}
    \end{subfigure}
    \caption{Samples of cubes from unannotated OCT images. Each row is a sampled cube with slices unrolled along the depth dimension. With threshold sampling in \ref{fig:genesis-cubes-with-threshold}, the cubes are the wall area of the OCT images. Without threshold sampling in \ref{fig:genesis-cubes-without-threshold}, the cubes are mainly background noises.}
\end{figure}

For fine-tuning, we load the pre-trained model encoder and decoder weight back to the 3D nnUNet while leaving the segmentation head to be randomly initialized. Subsequently, we fine-tune the pre-trained model on Calcium OCT dataset. This is similar to how nnUNet supervised pre-training is implemented. Following the finding of the 3D nnUNet, we fine-tune the model with the best patch size found in Section~\ref{sec:design:nnunet}, which is $160\times 128\times 32$. For nnUNet training to be standardized, we use the nnUNet's training script with the same hyperparameters as the supervised pre-training and provide the pre-trained model as the initial weight.

\section{Multi-modal Self-Supervised Learning - CLIP}\label{sec:implementation:clip}
Originally, the algorithm is designed for language-image pairs. However, we adapt the algorithm to multi-modalities of OCT images. To achieve this, we swap the text tokenizer of the original CLIP with an image encoder. 

From our experiments, we find that using separate encoders and projectors for each modality is not as effective as using a shared encoder and projector. Our experiments do sanity check on different configurations by letting all pairs of image be the same image and see whether configurations can learn to match the same image. Since this is an easy case it should be able to encode the same image to the same vector. We test this on separate encoders and projectors, separate encoders and shared projectors, and shared encoders and projectors. We find that only separate encoders and shared projectors, and shared encoders and projectors can learn to match the same image. 

Consequently, we test these two configurations on the main task of matching different images. We find only shared encoders and projectors can converge the losses down. This configuration is reasonable as the image encoder and its projector is forced to learn a representation that can be used for both modalities. This approach is illustrated in Figure~\ref{fig:clip-oct}.

\begin{figure}[hb]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/fig_implementation_clip_oct.pdf}
    \caption{A modified CLIP for co-registered multi-modal OCT images. Note that the projector is omitted for simplicity. The image encoder and projector are shared between all modalities}
    \label{fig:clip-oct}
\end{figure}

%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Discussion}
%%%%%%%%%%%%%%%%%%%%

% In the evaluation you convince the reader that your design works as intended.
% Describe the evaluation setup, the designed experiments, and how the
% experiments showcase the individual points you want to prove.

% This section is usually 5-10 pages.
We evaluate the effectiveness of self-supervised learning algorithms on calcium segmentation on coronary OCT images. All the models, pre-trained or not, are fine-tuned on Calcium OCT dataset for 3 folds with validation set to select the best model. The models are then evaluated on the testing set. The performance is measured using Dice score, a common metric for segmentation tasks, which is defined as follows:

\begin{equation}
    \text{Dice} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}
\end{equation}

where TP, FP, and FN are true positive, false positive, and false negative, respectively. The Dice score ranges from 0 to 1, where 1 indicates perfect segmentation. Dice score is calculated for each patient in the testing set, and the average Dice score is reported as the final performance of the model for each fold. The final performance is the average of the Dice scores of all folds.

\section{nnUNet}
Performance of nnUNet experiments on Calcium OCT dataset is summarized in Figure~\ref{fig:nnunet-results}. We find that default configuration of 3D nnUNet is significant lower than 2D nnUNet. To investigate further, we find that reducing the patch size of 3D nnUNet from $112$ to $32$ improves the performance significantly. This could be due to the fact that smaller depth allows the model to have more examples to learn from. In addition, comparing the patch size of $512\times 512\times 32$ and $160\times 128\times 32$ on 3D nnUNet, we can conclude that increase the patch size in the width and height dimension does not improve the performance as much. This may be due to the nature of calcium plaques in OCT images which are localized. Thus, increasing the context in the width and height dimension does not provide additional information to the model. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/result_nnunet_results.png}
    \caption{Dice score of variations of nnUNet. Results are groups according to the patch size used in training. Results in hashes indicate supervised pre-training on lumen and wall dataset (LaW OCT). The one-sided paired t-test is done between with and without supervised pre-training. It is also done between the best 2D nnUNet model and supervised pre-trained 3D nnUNet model. Lastly, the test is done between supervised pre-trained 3D nnUNet models.}
    \label{fig:nnunet-results}
\end{figure}

Additionally, pre-training the models with LaW OCT data set before fine-tuning on Calcium OCT significantly improves the 3D performance of models with their patch sizes reduced. Generally, calcium plaques around artery walls, so this pre-training allows the model to learn the features of the walls which can be useful for calcium segmentation. These pre-trained models are also statistically comparable to the best 2D nnUNet model.

\section{SuPreM}
We evaluated 3 settings of SuPreM's SwinUNTER model, as described in Section~\ref{sec:design:suprem}. The results are summarized in Figure~\ref{fig:suprem-results}. We find that SuPreM pre-training on SwinUNTER yields the best results with an average Dice score of $0.631\pm0.030$ which is statistically greater than the other two settings. This shows the effectiveness of SuPreM pre-training provides better features for image segmentation than self-supervised learning and no pre-training. The result is summarized in Figure~\ref{fig:suprem-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/result_SwinUNETR_results.png}
    \caption{Dice score of SuPreM's SwinUNTER model. In the parenthesis indicate pre-training algorithms. SuPreM pre-training on SwinUNTER yields the best results and is significantly better than both self-supervised pre-training and training it from scratch.}
    \label{fig:suprem-results}

\end{figure}

However, comparing the results of SuPreM's SwinUNTER model with the best 2D nnUNet model, we find that the 2D nnUNet model outperforms the SuPreM's SwinUNTER model (Figure~\ref{fig:transformer-results}). We will discuss this the next section.

\section{Transformers-based Models}
Effectiveness of the SegFormer, RADIO, and SwinUNTER pre-trained with SuPreM compared to nnUNet is summarized in Figure~\ref{fig:transformer-results}. We find that the best 3D nnUNet model all outperforms the transformer-based models. This effect could be fundamental differences between CNN and transformer architectures. CNNs are designed to learn local features while transformers are designed to learn global features. Calcium plaques in OCT images are localized, so CNNs are more suitable for this task. This is also supported by the fact that increasing the patch size in width and height dimension of 3D nnUNet does not improve the performance, while reducing the patch size in the depth dimension significantly improves the performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_vs_Transformer_results.png}
    \caption{Dice score of transformer-based models compared to nnUNet. Compared to the best 3D nnUNet model pre-trained on LaW OCT dataset, Dice scores of transformer-based models are statistically significantly lower.}
    \label{fig:transformer-results}
\end{figure}

\section{V-JEPA}
Performace of different decoder heads is summarized in Figure~\ref{fig:vjepa-decoder-results}. Each experiment is denoted by its architecture and its decoder head. In parenthesis indicates V-JEPA pre-trained on which dataset. The control experiment is ViT + Attentive* which is only ViT model with attentive decoder head and intensive data augmentation and positive data sampling rate without V-JEPA pre-training.

There is no statistically significant difference between different decoder heads. However, it can be concluded that the representation learned by V-JEPA on 2M natural videos, named \textbf{ViT + Attentive* (V-JEPA VideoMix2M)}, statistically significantly improves the performance of the ViT model on calcium segmentation, named \textbf{ViT + Attentive*}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/result_VJEPA_results.png}
    \caption{Dice score of V-JEPA's decoder head. BNLinear is batch normalization followed by linear projection. Attentive signifies the attentive decoder head. Attentive* means attentive decoder head with more intensed data augmentation and positive data sampling rate. MultiFeat connotes the multi-features decoder head. In the parenthesis indicates the algorithm and the data used for pre-training.}
    \label{fig:vjepa-decoder-results}
\end{figure}

Besides V-JEPA pre-trained on VideoMix2M, we also evaluate V-JEPA pre-trained on unannotated OCT images, \textbf{ViT + Attentive* (V-JEPA Unannotated OCT)}. Pre-training on unannotated OCT images does not improve the performance of the ViT model on calcium segmentation. This is due to the magnitude of the dataset. Unannotated OCT images are only 300 volumes, while VideoMix2M contains 2M videos. This shows that the scale of the dataset is important for self-supervised learning on a transformer-based model.

\section{CLIP}
Performance of CLIP pre-training on co-registered multi-modal OCT images is summarized in Figure~\ref{fig:clip-results}. We find that CLIP pre-training on OCT images significantly improves the performance of the 3D nnUNet model. In addition, we fail to reject the null hypothesis that CLIP is statistically comparable to 3D nnUNet model pre-trained on LaW OCT dataset. This shows that CLIP is as effective as supervised pre-training on LaW OCT dataset while the cost for co-registration is lower than the cost for annotating lumen and wall.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_CLIP_results.png}
    \caption{Dice score of CLIP on Calcium OCT dataset compared to nnUNet. In the parenthesis indicates the algorithms used for pre-training. LaW OCT is the supervised pre-training on Lumen and Wall OCT dataset. CLIP is the pre-training on co-registered multi-modal OCT images. Both supervised and CLIP pre-training significantly improves the performance of the 3D nnUNet model. They are statistically comparable to each other, and to the best 2D nnUNet model.}
    \label{fig:clip-results}
\end{figure}

The success of CLIP pre-training may come from the fact that aligning Pre-IVL and Post-IVL requires the model to learn the features of the lumen and wall which has been proven to be useful for calcium segmentation in nnUNet (LaW OCT). To prove this, we visualize the PCA of the features learned by CLIP compared to the features output from initial weight of the model. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_default_feature_map_batch0_feature1.png}
        \caption{PCA of the features output from initial weight of the model.}
        \label{fig:pca-initial}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_clip_feature_map_batch0_feature1.png}
        \caption{PCA of the features learned by CLIP.}
        \label{fig:pca-clip}
        
    \end{subfigure}
    \caption{PCA of the features output from initial weight of the model and the features learned by CLIP. CLIP pre-training gives more distinguishable features on the walls than those from the initial weight of the model. This can be the reason why CLIP pre-training improves the performance of the model.}
\end{figure}

The co-registered dataset used in CLIP is Pre-IVL and Post-IVL. Therefore, similar experiment should be further studied on Pre-IVL and Post-Stent dataset. This is left for future work.

\section{Genesis}
Genesis pre-training on unannotated OCT images yields $0.744\pm0.000$ Dice score on Calcium OCT dataset. However, Genesis does not statistically significantly improves the performance of the 3D nnUNet model based on the one-sided paired t-test. Nevertheless, according to the two-sided paired t-test, we can conclude that Genesis has a comparable performance with pre-trained 3D nnUNet on LaW OCT as well as the best 2D nnUNet. The result and statistical tests are summarized in Figure~\ref{fig:genesis-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_Genesis_results.png}
    \caption{Dice score of Genesis on Calcium OCT dataset. In the parenthesis indicates the algorithms used for pre-training. Genesis is the pre-training on unannotated OCT images. Genesis significantly improves the performance of the 3D nnUNet model.}
    \label{fig:genesis-results}    
\end{figure}


\section{Final Results}
The results of the best model in each category are summarized in Figure~\ref{fig:results}. They are grouped into 3 categories, SSL for self-supervised learning models, Transformer for transformer-based models, and nnUNet for 2D and 3D CNN-based nnUNet models. Each result is statistically tested again 2D nnUNet model who has the highest average Dice score in our study.

In SSL,
V-JEPA pre-trained on unannotated OCT images is significantly lower than 2D nnUNet. As discussed in the previous section, the scale of the dataset may play an important role in transformer-based self-supervised learning.
%TODO: wait for Genesis
CLIP pre-training on co-registered multi-modal OCT images is statisti.cally comparable to 2D nnUNet model. Additionally, it is comparable to 3D nnUNet model pre-trained on LaW OCT dataset. This shows that CLIP can be equally effective as supervised pre-training while costing less for annotation.

From our experiments, transformer-based models' Dice scores are significantly less than 2D nnUNet's. Transformer provides attention mechanism that allows the model to combine features from different parts of the image. In contrast, CNNs' kernels are designed to learn local features. Since calcium plaques generally lie around the artery walls, CNNs are more suitable for this task. In addition, the scale of the dataset may play an important role. nnUNet is designed for small- to medium-sized medical datasets.
Scaling up the size of the dataset and model parameters will not be advantageous for CNNs when these two magnitudes reach a certain point. On the contrary, transformers benefit from scaling. % TODO: paper to support this
This study does not serve to prove that nnUNet is better than transformers in general. It only shows that nnUNet is more suitable for calcium segmentation on OCT images with the current scale of the dataset and model parameters.

Finally, the best model in our study is 2D nnUNet with an average Dice score of $0.753\pm0.006$. It is non trivial to note that 2D nnUNet does not benefit from pre-training on other datasets, as treating the dataset into 2D images is enough for the model to learn the features of the calcium plaques. However, the best 3D nnUNet is achieved by pre-training on LaW OCT dataset. It is equally vital to note that patch size of 3D nnUNet is crucial for the performance of the model. Reducing the patch size in the depth dimension augments the performance significantly, while providing more context in the width and height dimension does not improve the performance. This mirrors the nature of annotating the calcium plaques in OCT images. Calcium plaques are identified by their change in intensity around the artery walls. Therefore, x-y plane context does not provide additional information to the model. Similarly, calcium, more often than not, occurs between adjacent slices. There is no need for the model to have more context in the depth dimension.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/result_best_in_class_results.png}
    \caption{Summary of the results. The best model in each category is illustrated. They are grouped into 3 categories, SSL for self-supervised learning models, Transformer for transformer-based models, and nnUNet for 2D and 3D CNN-based nnUNet models. Each result is statistically tested again 2D nnUNet model who has the highest average Dice score in our study.}
    \label{fig:results}
\end{figure}

\section{Discussion}
\subsection{Scale Analysis}
We do an initial study on the scale of the dataset used to train the model and their effect on the performance of the model. We subsample the training set of the Calcium OCT dataset to 33\% (1 training image and 1 validation image) and 66\% (3 training and 1 validation) of the original size. As shown in Figure~\ref{fig:scale-analysis}, the supervised pre-training on LaW OCT dataset is shown to be more reliable than self-supervised learning on a smaller dataset as they yields a lower standard deviation of dice score while the average dice score is higher. CLIP, on the other hand, is shown to be more affected in the lowest scale of the dataset. It has a lower average dice score and a higher standard deviation of dice score. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/discussion_scale_analysis.png}
    \caption{Dice score of 2D and 3D nnUNet on Calcium OCT dataset subsampled to 33\% and 66\% of the original size.}
    \label{fig:scale-analysis}
\end{figure}

We hypothesize 2 possible scenarios. One is that 8 volumes with 4 training, 2 validation and 2 testing volumes are already in a low data regime, making them lower as we study then does not reflect what other self-supervised learning research has shown, which is that SSL is effective in low data regime. The other is that CLIP features are not as effective as supervised pre-training features as CLIP implicitly learns the features of the lumen and wall, while supervised pre-training explicitly learns the features of the lumen and wall. This leads to more data needed for CLIP to translate the implicit features to the explicit features as we see CLIP thrives as the scale of the dataset increases. We recommend more study to be done on the scale of the dataset as this study is only preliminary and may not be conclusive.

\subsection{Visually Good Features}
Visually good features do no always lead to a better performance as the distinction of the most important weighs more than the aesthetics of the features. We visualize the features from V-JEPA (VideoMix2M) and from its initial weight. Shown in Figure~\ref{fig:visually-good-features}, both retain the structure of the artery walls. However, the features from V-JEPA (VideoMix2M) puts background into the same cluster and give more distinction to the artery walls. This can be the reason why V-JEPA (VideoMix2M) improves the performance of the model. While the features from the initial weight of the model keep the original structure and the structure of the artery walls is visible to human eyes, a close look into the color of the features shows that the artery walls are not as distinguishable from the background or any other structures as they are blended into a gradient of colors. What might be deceivingly good features to human eyes may not be the best features for the model to learn from.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_input.jpg}
        \caption{Input image}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_initial.jpg}
        \caption{PCA feature from ViT with initial weight}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_pretrained.jpg}
        \caption{PCA feature from ViT with V-JEPA pre-trained weight}
    \end{subfigure}
    \caption{PCA of the features output from initial weight of the model and the features learned by V-JEPA. A volume is unrolled along the depth dimension.}
    \label{fig:visually-good-features}
\end{figure}

Visualizing ViT (RADIO) also supports this hypothesis. Shown in Figure~\ref{fig:radio-features}, features from ViT (RADIO) is humanly interpretable as it can successfully distinguish OCT image semantics. The border between the background and where the waves could travel is clearly visible. However, the features on the artery walls are not as distinct. This can be the reason why ViT (RADIO) does not perform well in terms of Dice score.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/discussion_radio_feature.png}
    \caption{PCA of the features output from ViT (RADIO). The middle column is a PCA of the features with first principal component kept. The right column is with first principal component removed.}
    \label{fig:radio-features}
\end{figure}

\subsection{V-JEPA Pre-training}
Unlike Genesis, where loss and semantic check of how the model is learning on the pre-text task can be determined easily, V-JEPA does not. We experiment with different configurations of V-JEPA pre-training on our unannotated OCT images. We face difficulty in converging the loss. As shown in Figure~\ref{fig:v-jepa-training}, the loss curve of ViT-L (V-JEPA Unannotated OCT) is converging, but it is sensitive to the configurations used.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_1.png}
        \caption{ViT-L (V-JEPA Unannotated OCT)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_2.png}
        \caption{ViT-S (V-JEPA Unannotated OCT)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_3.png}
        \caption{ViT-L (V-JEPA Unannotated OCT) with more weight decay}
    \end{subfigure}
    \caption{Different loss curves of V-JEPA pre-training on unannotated OCT images with different configuration.}
    \label{fig:v-jepa-training}
\end{figure}

One way to determine V-JEPA convergence is to plot the hidden features from target encoder and compare that to the context encoder. Illustrated in the Figure~\ref{fig:v-jepa-prediction}, the pretraining can succeed the V-JEPA pre-text task of predicting the hidden features of the masked out parts of the video. In contrast to Genesis, or other pixel-level prediction pre-text tasks, it can not be easily determined if the prediction is correct. This is the trade-off of using V-JEPA which is more computationaly efficient than MAE, but harder to determine if the model is learning the correct features.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/discussion_vjepa_prediction.png}
    \caption{Hidden features from target encoder and context encoder of ViT-L (V-JEPA Unannotated OCT) over training epochs. At the bottom row shows different hidden features from the target encoder and context encoder in the last epoch.}
    \label{fig:v-jepa-prediction}
\end{figure}

\subsection{CLIP Pre-training}



%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

% The related work section covers closely related work. Here you can highlight
% the related work, how it solved the problem, and why it solved a different
% problem. Do not play down the importance of related work, all of these
% systems have been published and evaluated! Say what is different and how
% you overcome some of the weaknesses of related work by discussing the 
% trade-offs. Stay positive!

% This section is usually 3-5 pages.

% Self-supervised learning
\section{Self-supervised learning}
Self-supervised learning (SSL) has been proven to be a promising approach for natural language processing tasks (NLP). SSL can be split into two parts which are pre-text tasks and downstream tasks. Pre-text tasks are tasks that are designed to learn representations from the data itself without human annotations. Downstream tasks are finetuning tasks that use the learned representations from pre-text tasks to solve the tasks that require human annotations. It allows models to learn representations while avoiding costs of human annotations~\cite{Jaiswal2020}. Unsupervised pre-text tasks is not uncommon in the computer vision (CV) domain. For example, autoencoder~\cite{Hinton2006}, denoising autoencoder~\cite{Vincent2008}, and colorization~\cite{Larsson2017} are some of the pre-text tasks that have been used in CV. However, suitable pre-text tasks for SSL in computer vision are still under research.

% Self-supervised learning in CVs
In the recent years, discriminative and generative tasks have been proposed as pre-text tasks for CV field. SimCLR~\cite{Chen2020Simple} is a discriminative method that learns representations by maximizing the similarity between differently augmented views of the same image and minimizing the similarity between views of different images. This is also known as contrastive learning (CL). Representations learned from SimCLR have shown to be effective for classification tasks, outperforming supervised learning on ImageNet~\cite{Russakovsky2015} using the same architecture ResNet50~\cite{He2016}. SimCLRv2~\cite{Chen2020} is an improvement of SimCLR using a larger encoder, projection head and momentum contrast (MoCo)~\cite{He2020} that uses a queue and a moving average encoder to stabilize the training. Nevertheless, contrastive learning methods require a large batch size and a large number of negative samples to work effectively~\cite{Chen2020Simple}. This makes it difficult to scale to large datasets and models. 

In contrast to contrastive learning, masked autoencoding (MAE) learns representations by generating pixel values of the masked out parts of images, honing similar spirit to masked language modeling (MLM)~\cite{Devlin2019} in NLP. It has been preliminarily explored on vision transformer (ViT) model at the same time the architecture has been proposed~\cite{Dosovitskiy2020vit}. However, the performace of masked autoencoding in the original paper is not as good as supervised learning. To make this works, He et al.~\cite{He2022} propose masking out 75\% of an image rather than 50\% experimented previously. Furthermore, an asymmetric encoder-decoder architecture of MAE is introduced. An encoder operates only on the visible pixels and a light weight decoder operates on the encoded tokens as well as the mask tokens presented afterwards. Compared to CL, batch size, data augmentation and number of negative samples are not as critical for MAE, rendering it more scalable. MAE outperforms supervised learning and supervised pre-training on ImageNet using the same architecture ViT. Additionally, MAE has been evaluated on segmentic segmentation on ADE20k~\cite{Zhou2018}, highlighting that it can be used for dense prediction downstream tasks as well. Nonetheless, working at a pixel level, MAE is computationally expensive compared to CL.

In a manner similar to MAE, bidirectional encoder for image transformer (BEiT) first learns image patch tokenizer and predicts the tokenized values of the mask tokens instead of the pixel values as in MAE~\cite{Bao2022beit}. BEiT has been shown to outperform supervised learning and supervised pre-training on ImageNet~\cite{Russakovsky2015}. It has also been evaluated on segmentic segmentation on ADE20k achieving state-of-the-art performace at that time. As BEiT requires tokenizer to be learned, it adds more complexity to the pre-text task compared to MAE. Combining restorative and discriminative pre-text tasks, DINOv2~\cite{Oquab2024dinov} proposes to learn representations by predicting the tokenized values of the mask tokens as well as matching the class tokens of teacher and student networks on different crops of the same image. This approach has been shown to outperform other image SSLs at the time. Performing representation learning at the token level allows DINOv2 to be more efficient than pixel-level SSLs. In addition, I-JEPA~\cite{Assran2023} adapts this idea one step further. Attempting to find the most efficient approach to learn feature representations, I-JEPA proposes to predict directly the encoded vectors of mask tokens instead of the tokenized values. Evaluated on image classification, I-JEPA has shown to be comparable to other SSLs while being simpler and more efficient. 

While reading texts provides a natural way to learn natural languages, visual representations are not limited to static 2D images. Striving to incorporate a temporal dimension, extensions of existing visual SSLs have been made. VideoMAE introduces tube masking to MAE~\cite{He2022} along with an extremely high masking ratio of 90\% to 90\%~\cite{Tong2022VideoMAE}. Without external data, VideoMAE demonstrates its understanding of the video achieving state-of-the-art results to supervised learning on Kinetics-400~\cite{Kay2017Kinetics} and Something-Something V2~\cite{Goyal2017Something-SomethingV2}. Generating pixel values on video imposes even more computational limitation, as one cannot efficiently fit a clip of a video with a large number of frames. V-JEPA extends from I-JEPA~\cite{Assran2023}, predicting the encoded vectors of mask tokens on video~\cite{Bardes2024Vjepa}. As JEPA is more efficient than MAE, V-JEPA is a more efficient approach to learn representations on video. Additionally, V-JEPA has been shown to outperform VideoMAE. These ideas have potential to be applied to 3D medical images, replacing temporal axis with depth axis.

% Self-supervised learning in medical imaging
\section{Self-supervised learning in medical imaging}
In medical imaging, self-supervised learning would be highly beneficial as human annotations for medical images are expensive and time-consuming. However, as it is a specialized domain, recent SSL methods in CV are not yet fully explored. There are challenges in applying SSL in medical imaging. First, medical images can be in 3D, which is not common explored in SSL for natural which is mostly in 2D. Second, medical images can be in various modalities such as X-ray, CT, MRI, and ultrasound. Specifically, in our study, we focus on optical coherence tomography (OCT) images of arteries which are 3D and does not have any public datasets for SSL. This poses a challenge for us as there is no direct research directly addressing our problem.

% Basic rotation, jigsaw, rubik
 Following the same idea, Song et al.~\cite{Song2022} use rotation prediction, instance discrimination and VAE~\cite{Kingma2013}. Using VAE as a pre-text task, the model is semgnetaion-ready for COVID infection segmentation on lung CT images. This method outperforms supervised learning on similar architecture U-Net~\cite{Ronneberger2015} and U-Net++~\cite{Zhou2020}. Approaching 3D medical images differently, Rubik's cube solving is used as a 3D pre-text task where the model learns to classifies the orientation and ordering of a shuffled volumetric cube. Even though discriminative, Rubik's cube shows to improvement 3D segmentation in medical images ~\cite{Zhuang2019}. Jigsaw learns feature representations by predicting the correct order of shuffled patches~\cite{Noroozi2016}. Deep clustering clusters features vectors of the image in an unsupervised manner and uses thes clusters as pseudo-labels for classification~\cite{Caron2018}. Applying one or multiple pre-text tasks has been the main theme in several medical imaging SSL research papers ~\cite{Zhou2021, Zhang2021, Dufumier2021}. Taleb et al. ~\cite{Taleb2020} study 5 separated 3D self-supervised learning tasks, namely predicting latent vectors of adjacent patches, predicting the location of a given patch, solving jigsaw, predicting rotation angle and contrastive learning. In their study, predicting latent vectors of adjacent patches yields the best results in downstream tasks.

% Constrastive learning
TS-SSL~\cite{Zhang2021} proposes an SSL on 2D spectral domain optical coherence tomography (SD-OCT) images of retina to better classify retinal anomaly. It learns representation from classification labels, discriminative and generative tasks simultaneously. Using contrastive loss, it maximizes the similarity between rotated views or shuffled patches of the same image and minimizes the similarity between views of different images. At the same time, it uses a same representation to predict the rotation angle of the rotated views and the order of the shuffled patches. Along these tasks, TS-SSL also uses classification labels to predict the class of the image. However, this method only works better than supervised learning when 10\% of the labels are used for training. TS-SSL is not the only method that adopts modern SSL methods to medical imaging. Dufumier et al.~\cite{Dufumier2021} adds anoter layer to contrastive learning by using meta-data available in medical images. Specifically, they use age of the patient to indicate the degree of similarity between different images. This method improves bipolar disorder, Alzheimer and schizophrenia classification on 3D brain MRI images. Similarly, CLIP loss~\cite{Radford2021CLIP} can be used to leverage multi-modalities for representation learning ~\cite{Hager2023}. Encouraging images and their tabular meta-data, ubiquitous in medical imaging, to have similar representations. This method has been shown to improve classification tasks on brain MR images. 

Exploring boon of contrastive learning in segmentation, He et al.~\cite{He2022Intra} propose an intra- and inter-slice contrastive learning for point supervised OCT fluid segmentation of retina. The nature of this retinal OCT is 3D. While training segmentation model with U-Net~\cite{Ronneberger2015}, the authors leverage the fact that adjacent slices of the same volume are highly correlated, inter-slice CL is proposed to maximize the similarity between encoded vectors of adjacent slices as well as their segmentation masks. Segmentation masks are also compared with their ground truth masks and cross-entropy loss. Segmented masks are subsequently used to select the locations of small patches to be used for intra-slice CL. Intra-slice CL maximizes the similarity between encoded fluid patches and the backgroun fluid patches. Its result outperforms other point-based segmentation methods but is not as good as fully supervised learning. Besides spatio consistency as previous work, temporal consistency can also be imposed~\cite{Ren2022}. Given that images are taken from the same patient at different time points, temporal consistency can be used to improve the segmentation of the images. This method has been shown to improve the segmentation of brain MRI images.

% Restorative tasks
Alternatively, generative tasks have been proposed as pre-text tasks for medical imaging. Patch shuffling~\cite{Chen2019} propose a restorative task for 2D medical images of MRI, CT and ultrasound. First patches are randomly cut from the original image and placed to random locations. The model learns to restore the images back to their original versions. This method improves the downstream segmentation tasks in a data-scarce setting. In-painting~\cite{Pathak2016} removes a part of the image and the model learns to predict the missing part. Advancing further, Genesis~\cite{Zhou2021} explore image distortion algorithms for 3D restorative pre-text tasks on CT images. Thorough experiments show that Genesis outperforms specialized 3D state-of-the-art segmentation models and other pre-text tasks including de-noising~\cite{Vincent2010}, in-painting~\cite{Pathak2016}, jigsaw~\cite{Noroozi2016}, deep clustering~\cite{Caron2018}, rubik's~\cite{Zhuang2019} and patch shuffling~\cite{Chen2019}. This establishes a new base line for 3D SSL in medical imaging showing that mixing pre-text tasks is beneficial. Persuing a similar concept, SwinUNTER~\cite{Tang2022} proposes training ViT-based model with a combination of in-paining, constrastive and rotation.

Introducing deep clustering~\cite{Caron2018} idea to Genesis~\cite{Zhou2021}, TransVW adds a classification task to the restorative task of Genesis. First, visual words are mined automatically and grouped into clusters with deep latent features. These visual words (VW) are used to be classified and restored. Visual words improve segmentation and classification task in some datasets compared to Genesis~\cite{Haghighi2021}. DiRA~\cite{Haghighi2024} further improves TransVW by adding adversarial model to tell apart the restored images and the original image. Additionally, constrastive learning is used as an additional pre-text task. Combining, discriminative, restorative and adversarial training, DiRA outperforms TransVW and train-from-scratch models. Eventhough, TransVW and DiRA have shown to be better than Genesis, their evaluations are not as thorough. TransVW only outperforms Genesis in some datasets, while DiRA only evaluates its performace against TransVW.

A gap between pre-text tasks studied in medical imaging and those introduced in recent visual SSL research is observed. While visual SSL research has been moving towards more efficient and simpler pre-text tasks such as DINOv2~\cite{Oquab2024dinov}, MAE~\cite{He2022}, and I-JEPA~\cite{Assran2023}, medical imaging SSL research has been focusing on blending multiple pre-text tasks together. An attempt to fill this gap is evident in Genesis~\cite{Zhou2021} which works on 3D medical images at a pixel level and has shown to be better than other pre-text tasks. More studies are needed to explore simpler and more efficient pre-text tasks for medical imaging, closing the gap between visual SSL and medical imaging SSL. Concurrently, a robust evaluation in medical imaging research must be done as many reported results may not be reproducible~\cite{Isensee2024}.
% Alternatives
\section{Self-supervised learning alternatives}
Self-supervised learning has a great potential in medical imaging. If simple and robust pre-text tasks can be found, it can be used to learn representations from medical images without human annotations. However, such tasks are being explored and much work on the evaulation have to be done. Alternatives are introduced to efficiently train the models with existing small medical datasets.

SuPreM suggests to pre-train models with supervised learning on a large annotated datasets. SuPreM collates 9,262 CT volumes, pre-trains a models to segment subset of organs and fine-tune them on mutually exclusive subset of organs. The experiments show that it requires less data to learn meaning representations than SSL while also provides better transferability~\cite{Li2024}.

Rather than pre-training, nnUNet proposes a self-configuring framework for medical image segmentation. The framework automatically build UNet-like model for 2D and 3D medical images for a given dataset. It has been shown to robustly outperform state-of-the-art models, including popular Transformer-based and Mamba-base architectures, on several medical image segmentation tasks and works well on small datasets due to it robust positive data sampling and augmentation~\cite{Isensee2020}. Follow up work on nnUNet has been done showing that many reported results may not be reproducible due to many reasons. The study highlights that nnUNet is the consistently best performing model on the tasks it has been evaluated on without additional data~\cite{Isensee2024}.

Previous work at IMES to segment OCT images have been done with SegFormer pre-trained with ADE20k. SegFormer is a ViT-based model that enforces multiple resolution at different depths of the transformer. Simple MLP~\cite{Rumelhart1986} decoder heads are introduced to combine the features from different depths. The model has been shown to be state-of-the-art on segmentic segmentation at the time~\cite{Xie2021SegFormer}.

Taking a different approach, RADIO proposes to learn representations from already learned models. RADIO is a student model that learns to predict a same feature from multiple teacher models, including DINOv2~\cite{Oquab2024dinov}, SAM~\cite{Kirillov2023SAM} and CLIP~\cite{Radford2021CLIP}. In addition, RADIO is trained to be resolution-agnostic, allowing it to understand images at different resolutions. RADIO has been shown to provide a high resolution feature map that can be used for downstream tasks~\cite{Ranzinger2024RADIO}. While SSLs has a great potential on medical imaging, their approaches should be compared to all other existing ones in order to ensure a comprehensive comparison.

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\end{document}