%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% To avoid any implication, this template is released into the
% public domain / CC0, whatever is most convenient for the author
% using this template.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{xspace}
\usepackage{amsmath}


\title{Self-supervised learning\\for calcium segmentation on coronary OCT images}
\author{Naravich Chutisilp}
\supervisor{The Doctoral Student}
\adviser{Prof. Dr. sc. ETH Mathias Payer}
%\coadviser{Second Adviser}
\expert{The External Reviewer}

% \newcommand{\sysname}{FooSystem\xspace}

\dedication{
    \begin{raggedleft}
        It’s no use going back to yesterday, because I was a different person then.\\
        --- Lewis Carroll, Alice’s Adventures in Wonderland\\
    \end{raggedleft}
    \vspace{4cm}
    \begin{center}
        Dedicated to my lovely family.
    \end{center}
}
\acknowledgments{
% This is where you thank those who supported you on this journey. Good examples
% are your significant other, family, advisers, and other parties that inspired
% you during this project. Generally this section is about 1/2 page to a page.

% Consider acknowledging the use and location of this thesis package.

% Define your acknowledgments in \texttt{\textbackslash{}acknowledgments\{...\}}
% and show them with \texttt{\textbackslash{}makeacks}.

    I would like to show my deep gratitude to my mom and dad for their unwavering supports and firm trust to let me choose my own path, my two lovely sisters, Namking and Namkow, for their continuous love and understanding. 

    I am also grateful to all the friends I have made during my time at EPFL, Set, James, Kwang, Ice, Sundae, Nai, Cindy, Jenestin, Thomas, Paulina, Fah, Ting-Wei, Pin-Yen, Kai, Hong-Bin, Leo, Edvin, Anthon, Aamir, Jirka, and many more people I have not mentioned, who have made my life abroad memorable and amicable. Going to EPFL was the first time I have been abroad and I could not have asked for a better experience. Simple dinner in the evening every day after school, meaningful conversations about everything, trips to alpine mountains, and european cities.

    Furthermore, I could not forget the friends have made during my time at MIT, Mee, Pooh, and Cue who have made my final Master's semester fun and unforgettable. Coming to MIT was a dream come true, yet it was not easy to leave my friends in Switzerland and start anew in the US. However, since the first day I arrived, I have been welcomed with open arms and got shown to many places in Boston. Worries and fears have been replaced with excitement and joy. Hiking in the White Mountains, visiting California and silicon valley, walking along the freedom trail, enjoying delicacies of Lobster rolls and clam chowder.

    Additionally, it would not be possible if I did not have support from my friends in Thailand, V, Meak, Korn, Marie and uncountable more that I could not fit into this section. Calls and messages from you have always been a source of strength and comfort. Through though times and good times, you have always been there for me. Even though we are 9,188 km apart, or even 13,707 km apart, it feels like you are always by my side.

    I would like to thank Prof. Elazer Edelman and Karim Kaldry for giving me this unparalleled opportunity to join the IMES lab at MIT and let me complete my Master's thesis here, as well as providing me advises and support on my research. Opportunity 
}


\begin{document}
\maketitle
\makededication
\makeacks

\begin{abstract}
The sysname tool enables lateral decomposition of a multi-dimensional
flux compensator along the timing and space axes.

The abstract serves as an executive summary of your project.
Your abstract should cover at least the following topics, 1-2 sentences for
each: what area you are in, the problem you focus on, why existing work is
insufficient, what the high-level intuition of your work is, maybe a neat
design or implementation decision, and key results of your evaluation.
\end{abstract}

\begin{frenchabstract}
For a doctoral thesis, you have to provide a French translation of the
English abstract. For other projects this is optional.
\end{frenchabstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

The introduction is a longer writeup that gently eases the reader into your
% thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
In the second paragraph you can introduce the main challenge that you see.
The third paragraph lists why related work is insufficient.
The fourth and fifth paragraphs discuss your approach and why it is needed.
The sixth paragraph will introduce your thesis statement. Think how you can
distill the essence of your thesis into a single sentence.
The seventh paragraph will highlight some of your results
The eights paragraph discusses your core contribution.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your
% work. This is not necessarily related work but technologies and dependencies
% that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.
Coronary artery diseases (CAD) is a leading cause of heart diseases and fatalities worldwide~\cite{Virani2021Heart, Wu2015}. It is primarily caused by atherosclerosis, a process of plaque building up in the coronary arteries, partially or totally obstructing the blood flow~\cite{Shahjehan2023}. Commonly, doctors use percutaneous coronary intervention (PCI) as a treatment for CAD. This procudure is done by enlarging the narrowing artery~\cite{Ahmad2023}. However, the outcome of PCI can be affected by the presence of coronary artery calcification (CAC) which hinders stent expansion, leading to stent underexpansion, distortion, dislodgement or loss~\cite{Hennessey2023}. Thus, a process to remove these plaques is a prerequisite to increase the success of PCI. Intravascular lithotripsy (IVL) is a novel technology that uses sonic pressure waves to break the calcification leisons, modifying them into fractures, allowing stent deployment to be effective~\cite{Butt2023}. This technique requires accurate definition of the calcified lesions in order to select the optimal strategy for IVL~\cite{Butt2023}. Generally, the calcified lesions are identified using intravascular imaging techniques such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT)~\cite{Butt2023}. Specifically, OCT can deliver sufficient accuracy higher than IVUS~\cite{Fujimoto2003, Costopoulos2016}. However, manual segmentation of calcified lesions in OCT images is time-consuming and subjective~\cite{Segars2013, Oktay2020, Carpenter2022}. Therefore, an automatic segmentation method is needed to improve the efficiency and accuracy of the process~\cite{Carpenter2022}.

Medical image segmentation is a challenging task that typically requires specialized annotators to provide pixel-level groud truth labels for training. This leads to several researches seeking to find a cost-effective way for medical image annotation~\cite{Fu2012, Gal2017, Beluch2018, Rahimi2021}. Alternatively, self-supervised learning (SSL) has been proposed as a promising approach to learn representations from the data itself without human annotations. Although SSL is now a common approach in natural language processing (NLP), it is still under active research in computer vision (CV) with collection of practices being made~\cite{Balestriero2023}. 

Originally, SSL in CV has been done using traditional pre-text tasks such as  autoencoder~\cite{Hinton2006} and denoising autoencoder~\cite{Vincent2008}. Such pre-text tasks are empirically useful in some use cases, but their benefits are not yet on par with SSL in NLP. Hence, pre-text tasks that explicitly enbrace visual semantics have been proposed. Such tasks include solving jigsaw puzzle~\cite{Noroozi2016}, rotation prediction~\cite{Gidaris2018}, and colorization~\cite{Larsson2017}. Recently, visual SSLs have been moving towards more efficient and simpler pre-text tasks which can be categorized into two groups, discriminative and generative tasks. Discriminative tasks learn representations by learning to discriminate between same and different views of images. Generative tasks learn representations by generating values of the masked out parts of images. These tasks have been shown to be more effective than traditional pre-text tasks in learning representations~\cite{Chen2020Simple, He2020, He2022, Bao2022beit}.

Even though, modern discriminative tasks in visual SSLs have been studied in medical imaging, their generative counterparts have not been explored. In medical imaging, researches in SSL have been focusing on amalgamating multiple pre-text tasks together. Several papers study combination of traditional pre-text tasks to improve classification and segmentation tasks in medical imaging~\cite{Noroozi2016, Zhuang2019}. More recent researches explore blending discriminative tasks with traditional pre-text tasks~\cite{Zhou2021, Zhang2021, Dufumier2021, Taleb2020, Zhang2021, He2022Intra, Ren2022}. To our best of knowledge, the only generative task being studied in medical imaging is restorative~\cite{Pathak2016,Chen2019,Zhou2021,Tang2022,Haghighi2021,Haghighi2024}.

In recent years, generative tasks are now gaining attention in medical imaging following their precedent success in visual SSLs. Masked autoencoder (MAE) which learns representations by filling the masked out parts of images has been shown to be effective~\cite{He2022}. Despite its simplicity, working at a pixel-level is computationally expensive. Bidirectional encoder for image transformer (BEiT) proposes to work on tokenized level instead of pixel level~\cite{Bao2022beit}. This makes BEiT more efficient than MAE. DINOv2~\cite{Oquab2024dinov} combines discriminative and generative tasks, working on tokenized level and feature level. This approach has been shown to be more efficient and effective than other image SSLs. I-JEPA takes this to a level higher by directly predicting the encoded vectors of masked out parts~\cite{Assran2023}. This approach is simpler and much more scalable than other generative SSLs. These methods have been shown to be effective in learning representations in visual SSLs. However, they have not been explored in medical imaging. Particularly, if working on embedding level proves to be beneficial in medical imaging, it can be a game changer as this can be the most efficient and greatly scalable approach to learn representations in unannotated medical imaging.

% TODO: add about alternatives

%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

% Introduce and discuss the design decisions that you made during this project.
% Highlight why individual decisions are important and/or necessary. Discuss
% how the design fits together.

% This section is usually 5-10 pages.

\section{Datasets}\label{sec:design:datasets}
The main goal of this project is to study the effectiveness of self-supervised learning algorithms for calcium segmentation on coronary OCT images. Therefore, we categorized the datasets into two groups, annotated and unannotated datasets. Each category has its own characteristics and relevance to the problem. Calcium OCT (Dataset \ref{enum:calcium-dataset}) contains 8 3D OCT volumes in total annotated with calcium plaques. Lumen and Wall OCT (Dataset \ref{enum:lumen-and-wall-dataset}) is a dataset of 20 3D volumes annotated with lumen and wall. These two datasets are mutually exclusive. Furthermore, 500 3D volumes are unannotated. These volumes are divided into 3 groups, 167 volumes before IVL, 169 volumes after IVL, and 164 volumes after stent deployment. Even though these volumes are unannotated, a tabular meta-data is available for each volume. This meta-data describes morphological properties of the calcium plaques, lumen and wall. This can be summarized as follows:

\begin{enumerate}
    \item Annotated datasets:
    \begin{enumerate}
        \item \label{enum:calcium-dataset} \textbf{Calcium OCT} is a dataset of coronary OCT images with calcium annotations before IVL.
        \item \label{enum:lumen-and-wall-dataset} \textbf{Lumen and Wall OCT} is a dataset of coronary OCT images with lumen and wall annotations before IVL.
    \end{enumerate}
    \item \label{enum:unannotated-dataset} Unannotated datasets:
    \begin{enumerate}
        \item \textbf{Pre-IVL} is a dataset of coronary OCT images before IVL.
        \item \textbf{Post-IVL} is a dataset of coronary OCT images after IVL.
        \item \textbf{Post-Stent} is a dataset of coronary OCT images after stent deployment.
    \end{enumerate}
\end{enumerate}

Each dataset has its own purpose in this study. Calcium OCT is used for evaluating effectiveness of each algorithm on calcium segmentation. On the other hand, Lumen and Wall OCT can be used for supervised pre-training of the algorithms, while unannotated datasets are used for self-supervised pre-training. Additionally, the tabular meta-data can be used to evaluate the effectiveness of the algorithms on multi-modalities, but this is not the main focus of this study.

In Calcium OCT, only 1 class is annotated, calcium. The size of each image is $500\times 500$ while the depth varies from $375$ to $539$. An example of the annotated image is shown in Figure~\ref{fig:calcium-oct}. The dataset is split into 6 training volumes and 2 testing volumes. In the training volumes, they are split into 3 folds of training and validation sets. In each fold, the model can be train using the training set, while the best model can be selected using the validation set. Subsequently, all models trained on each fold can be evaluated on the testing set that has not been seen during training. Eventually, the performace on the testing set can be averaged to get the final performace. This is done to ensure the robust evaluation of the models.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_calcium_oct_sample.pdf}
    \caption{An example of annotated image in Calcium OCT dataset.}
    \label{fig:calcium-oct}
\end{figure}

Lumen and Wall OCT has 2 classes, lumen and wall, and is split into 16 training volumes and 4 testing volumes. The size of each image is $500\times 500$ while the depth varies from $270$ to $540$. An example of the annotated image is shown in Figure~\ref{fig:lumen-and-wall-oct}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_law_oct_sample.pdf}
    \caption{An example of annotated image in Lumen and Wall OCT dataset.}
    \label{fig:lumen-and-wall-oct}
\end{figure}

The unannotated datasets are of size $500\times 500$. They are not splitted into training and testing sets since they are only used for self-supervised pre-training. However, each self-supervised learning algorithm may split them into train and validation set for hyperparameter tuning. Samples of each modality are shown in Figure~\ref{fig:unannotated-oct}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig_datasets_unlabeled_oct_sample.pdf}
    \caption{Samples of unannotated OCT images.}
    \label{fig:unannotated-oct}
\end{figure}

\section{Algorithms}
% TODO: Something like "we select generative tasks for self-supervised learning on our datasets" but more detailed and fomal

\subsection{CNN-based Supervised Learning - nnUNet}\label{sec:design:nnunet}
nnUNet is a self-configuring framework for medical image segmentation~\cite{Isensee2020}. To apply the algorithm to our datasets, we designed two approaches. The first approach is to directly train the model on Calcium OCT. In addition to the former approach, on the second approach, we also pre-train the model on Lumen and Wall OCT before fine-tuning on Calcium OCT. 

% Discuss 2D and 3D nnUNet for segmentation
nnUNet provides both 2D- and 3D-segmentation architectures as well as a cascade 3D architecture of full resolution and low resolution networks. In our initial experiments, we find that 2D and 3D architecture are performing better than the cascade architecture. This leads us to subsequently only use 2D and 3D architectures for the rest of the experiments.

\subsection{Transformer-based Supervised Learning - SegFormer}
SegFormer is a transformer-based model with multi-resolution feature fusion with simple multi-layer perceptrons (MLP~\cite{Rumelhart1986})~\cite{Xie2021SegFormer}, and has been pre-trained on ADE20k~\cite{Zhou2018} datasets. Following the previous work, we use the pre-trained model to fine-tune on Calcium OCT.

\subsection{Transformer-based Medical Supervised Learning - SuPReM}~\label{sec:design:suprem}
SuPReM is a supervised pre-training method for medical image segmentation~\cite{Li2024}. As we do not have a large annotated dataset, we use publicly available pre-trained models of the paper to fine-tune on Calcium OCT. Following their paper, we use their best performing model which is SwinUNTER~\cite{Tang2022} with their training scripts. Because SwinUNTER is also on its own another transformer-based model with self-supervised learning, we conduct three experimentals to evaluate the algorithms. The first setting directly utilizes the pre-trained model on SuPReM large datasets, and fine-tunes on Calcium OCT. The second setting uses self-supervised SwinUNTER model, and fine-tunes on Calcium OCT. The last setting uses SwinUNTER model without any pre-training, and fine-tunes on Calcium OCT. Consequently, we can evaluate the effectiveness of SuPReM pre-training and self-supervised learning proposed in SwinUNTER on Calcium OCT.

\subsection{Unimodal Self-Supervised Learning - Genesis}
Genesis is a self-supervised learning method that learns representations by restoring the original images from their corrupted versions~\cite{Zhou2021}. In their paper, Genesis is applied on a small U-Net architecture, containing 15M parameters. We adapt their method to the same 3D nnUNet architecture in Section~\ref{sec:design:nnunet}. Thereafter, we apply Genesis pre-training on unannotated OCT images, and fine-tune the resulting model on Calcium OCT.

\subsection{Unimodal Self-Supervised Learning - V-JEPA}
V-JEPA is a self-supervised learning method that acquires representations by predicting the encoded vectors of the masked segments of a video~\cite{Bardes2024Vjepa}. We desgin two experiments to study the effectiveness of V-JEPA on OCT images. Firstly, we directly fine-tunes the pre-trained V-JEPA model on Calcium OCT. This means that the model is a simple ViT~\cite{Dosovitskiy2020vit} and is trained on natural videos and then fine-tuned on medical images . Secondly, we apply V-JEPA pre-training on unannotated OCT images, and fine-tune the resulting model on Calcium OCT. Since the original paper does not evaluate the model on segmentation tasks, we will discuss the implementation of the decoder head in Section~\ref{sec:implementation:vjepa}.

\subsection{Multi-modal Self-Supervised Learning - CLIP}
Contrastive\cite{Radford2021CLIP} Language-Image Pre-trainig (CLIP~\cite{Radford2021CLIP}) is a self-supervised learning method that learns representations by matching encoded vectors of images and their text captions. We adapt CLIP to OCT images by using co-registered Pre-IVL, Post-IVL, and Post-Stent images. We use the same nnUNet architecture as in Section~\ref{sec:design:nnunet} to pre-traine CLIP on and fine-tune the final model on Calcium OCT. Note that the original paper does not publish the code for CLIP, we modify the code from a publicly available project~\cite{Shariatnia2021}. Original paper proposes a pre-training framework on language and image. However, we apply the framework on multi-modalities of OCT images. We will discuss the adaptation of the framework to our specific use case in Section~\ref{sec:implementation:clip}.

%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

% The implementation covers some of the implementation details of your project.
% This is not intended to be a low level description of every line of code that
% you wrote but covers the implementation aspects of the projects.

% This section is usually 3-5 pages.
\section{CNN-based Supervised Learning - nnUNet}
We apply publicly available code from nnUNet~\cite{Isensee2020} on our datasets. Additionally, we add an early stopping to the optimizer and train for a longer period. As we find that 3D segmentation of nnUNet is performing significantly worse than its 2D counterpart, we experiment different patch sizes on 3D segmentation instead of its automatically configured patch size. We find that reducing the depth of the original patch size from $112$ to $32$ improves the performance. This could be due to the fact that smaller depth allows the model to have more examples to learn from. In addition, inspired by SuPReM, we also experiment with supervised pre-training on Lumen and Wall OCT before fine-tuning on Calcium OCT. nnUNet provides a supervised pre-training script as well as a pipeline to fine-tune on new datasets.

\section{Transformer-based Supervised Learning - SegFormer}
We use the pre-trained SegFormer model and their training script from MMSegmentation~\cite{mmseg2020}. As SegFormer only supports 2D segmentation, we preprocess our Calcium OCT datasets into 2D images with their segmentation masks, and train the model on the newly formatted dataset.

\section{Transformer-based Medical Supervised Learning - SuPReM}
SuPReM has publicly available pre-trained model and codes~\cite{Li2024}. Thus, we format our Calcium OCT dataset into the same format as their datasets. Preserving their training algorithm, we use their fine-tuning script on our datasets. We experiment with three settings as described in Section~\ref{sec:design:suprem} as they are all available in their original codes.

\section{Unimodal Self-Supervised Learning - Genesis}
Genesis also has publicly available code~\cite{Zhou2021}. However, only pseudo code is available in fine-tuning. Therefore, we adapt the code from nnUNet~\cite{Isensee2020} to Genesis for fine-tuning. By using the same architecture as nnUNet instead of Genesis's U-Net, we can simply replace the encoder and decoder of this pre-trained model to nnUNet training script. We use the best patch size found in Section~\ref{sec:design:nnunet} on Genesis for a fair comparison.

\section{Unimodal Self-Supervised Learning - V-JEPA}\label{sec:implementation:vjepa}
Because of V-JEPA's lack of evaluation on segmentation tasks, it is necessary to design a decoder head for V-JEPA. The paper proposes an attentive decoder head for classification tasks. Therefore, we also develop an attentive decoder head for segmentation, drawing inspiration from the MAE framework~\cite{He2022}, which directly predicts pixel values.Specifically, the decoder head is an encoder-only transformer of 1 depth and 12 heads followed by a linear projection layer that yields embedding vectors into a vector of size \(\text{patch size} \times \text{patch size} \times \text{number of classes}\). Besides, we also experiment with other variations of decoding architecture, such as a simple batch normalization layer followed by a linear projection, similar how RADIO~\cite{Ranzinger2024RADIO} and DINOv2~\cite{Oquab2024dinov} fine-tune their models on segmentation tasks. Additionally, inspired by SegFormer~\cite{Xie2021SegFormer}, we also experiment with simple multi-layer perceptrons (MLP~\cite{Rumelhart1986}) on each feature vectors at different depths followed by a linear projection to combine those features into a segmentation mask. This enables the decoder to multiple features at different levels. Unlike SegFormer, each features are not downsampled, so they are not explicitly forced to be different resolutions. However, this methods allows us to preserve the original architecture of V-JEPA's pre-training. Thus, we can utilize their pre-trained weight on our segmentation task.

% TODO: add images of the decoder head (attentive, batch norm, mlp)

\section{Multi-modal Self-Supervised Learning - CLIP}\label{sec:implementation:clip}
Originally, the algorithm is designed for language-image pairs. However, we adapt the algorithm to multi-modalities of OCT images. To achieve this, we swap the text tokenizer of the original CLIP with an image encoder. From our experiments, we find that using separate encoders and projectors for each modality is not as effective as using a shared encoder and projector. Our experiments do sanity check on different configurations by letting all pairs of image be the same image and see whether configurations can learn to match the same image. Since this is an easy case it should be able to encode the same image to the same vector. We test this on separate encoders and projectors, separate encoders and shared projectors, and shared encoders and projectors. We find that only separate encoders and shared projectors, and shared encoders and projectors can learn to match the same image. Consequently, we test these two configurations on the main task of matching different images. We find only shared encoders and projectors can converge the losses down. This configuration is reasonable as the image encoder and its projector is forced to learn a representation that can be used for both modalities.

% TODO: of final implementation of CLIP on OCT images (shared encoder and projector)
%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%

In the evaluation you convince the reader that your design works as intended.
Describe the evaluation setup, the designed experiments, and how the
experiments showcase the individual points you want to prove.

This section is usually 5-10 pages.


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

% The related work section covers closely related work. Here you can highlight
% the related work, how it solved the problem, and why it solved a different
% problem. Do not play down the importance of related work, all of these
% systems have been published and evaluated! Say what is different and how
% you overcome some of the weaknesses of related work by discussing the 
% trade-offs. Stay positive!

% This section is usually 3-5 pages.

% Self-supervised learning
\section{Self-supervised learning}
Self-supervised learning (SSL) has been proven to be a promising approach for natural language processing tasks (NLP). SSL can be split into two parts which are pre-text tasks and downstream tasks. Pre-text tasks are tasks that are designed to learn representations from the data itself without human annotations. Downstream tasks are finetuning tasks that use the learned representations from pre-text tasks to solve the tasks that require human annotations. It allows models to learn representations while avoiding costs of human annotations~\cite{Jaiswal2020}. Unsupervised pre-text tasks is not uncommon in the computer vision (CV) domain. For example, autoencoder~\cite{Hinton2006}, denoising autoencoder~\cite{Vincent2008}, and colorization~\cite{Larsson2017} are some of the pre-text tasks that have been used in CV. However, suitable pre-text tasks for SSL in computer vision are still under research.

% Self-supervised learning in CVs
In the recent years, discriminative and generative tasks have been proposed as pre-text tasks for CV field. SimCLR~\cite{Chen2020Simple} is a discriminative method that learns representations by maximizing the similarity between differently augmented views of the same image and minimizing the similarity between views of different images. This is also known as contrastive learning (CL). Representations learned from SimCLR have shown to be effective for classification tasks, outperforming supervised learning on ImageNet~\cite{Russakovsky2015} using the same architecture ResNet50~\cite{He2016}. SimCLRv2~\cite{Chen2020} is an improvement of SimCLR using a larger encoder, projection head and momentum contrast (MoCo)~\cite{He2020} that uses a queue and a moving average encoder to stabilize the training. Nevertheless, contrastive learning methods require a large batch size and a large number of negative samples to work effectively~\cite{Chen2020Simple}. This makes it difficult to scale to large datasets and models. 

In contrast to contrastive learning, masked autoencoding (MAE) learns representations by generating pixel values of the masked out parts of images, honing similar spirit to masked language modeling (MLM)~\cite{Devlin2019} in NLP. It has been preliminarily explored on vision transformer (ViT) model at the same time the architecture has been proposed~\cite{Dosovitskiy2020vit}. However, the performace of masked autoencoding in the original paper is not as good as supervised learning. To make this works, He et al.~\cite{He2022} propose masking out 75\% of an image rather than 50\% experimented previously. Furthermore, an asymmetric encoder-decoder architecture of MAE is introduced. An encoder operates only on the visible pixels and a light weight decoder operates on the encoded tokens as well as the mask tokens presented afterwards. Compared to CL, batch size, data augmentation and number of negative samples are not as critical for MAE, rendering it more scalable. MAE outperforms supervised learning and supervised pre-training on ImageNet using the same architecture ViT. Additionally, MAE has been evaluated on segmentic segmentation on ADE20k~\cite{Zhou2018}, highlighting that it can be used for dense prediction downstream tasks as well. Nonetheless, working at a pixel level, MAE is computationally expensive compared to CL.

In a manner similar to MAE, bidirectional encoder for image transformer (BEiT) first learns image patch tokenizer and predicts the tokenized values of the mask tokens instead of the pixel values as in MAE~\cite{Bao2022beit}. BEiT has been shown to outperform supervised learning and supervised pre-training on ImageNet~\cite{Russakovsky2015}. It has also been evaluated on segmentic segmentation on ADE20k achieving state-of-the-art performace at that time. As BEiT requires tokenizer to be learned, it adds more complexity to the pre-text task compared to MAE. Combining restorative and discriminative pre-text tasks, DINOv2~\cite{Oquab2024dinov} proposes to learn representations by predicting the tokenized values of the mask tokens as well as matching the class tokens of teacher and student networks on different crops of the same image. This approach has been shown to outperform other image SSLs at the time. Performing representation learning at the token level allows DINOv2 to be more efficient than pixel-level SSLs. In addition, I-JEPA~\cite{Assran2023} adapts this idea one step further. Attempting to find the most efficient approach to learn feature representations, I-JEPA proposes to predict directly the encoded vectors of mask tokens instead of the tokenized values. Evaluated on image classification, I-JEPA has shown to be comparable to other SSLs while being simpler and more efficient. 

While reading texts provides a natural way to learn natural languages, visual representations are not limited to static 2D images. Striving to incorporate a temporal dimension, extensions of existing visual SSLs have been made. VideoMAE introduces tube masking to MAE~\cite{He2022} along with an extremely high masking ratio of 90\% to 90\%~\cite{Tong2022VideoMAE}. Without external data, VideoMAE demonstrates its understanding of the video achieving state-of-the-art results to supervised learning on Kinetics-400~\cite{Kay2017Kinetics} and Something-Something V2~\cite{Goyal2017Something-SomethingV2}. Generating pixel values on video imposes even more computational limitation, as one cannot efficiently fit a clip of a video with a large number of frames. V-JEPA extends from I-JEPA~\cite{Assran2023}, predicting the encoded vectors of mask tokens on video~\cite{Bardes2024Vjepa}. As JEPA is more efficient than MAE, V-JEPA is a more efficient approach to learn representations on video. Additionally, V-JEPA has been shown to outperform VideoMAE. These ideas have potential to be applied to 3D medical images, replacing temporal axis with depth axis.

% Self-supervised learning in medical imaging
\section{Self-supervised learning in medical imaging}
In medical imaging, self-supervised learning would be highly beneficial as human annotations for medical images are expensive and time-consuming. However, as it is a specialized domain, recent SSL methods in CV are not yet fully explored. There are challenges in applying SSL in medical imaging. First, medical images can be in 3D, which is not common explored in SSL for natural which is mostly in 2D. Second, medical images can be in various modalities such as X-ray, CT, MRI, and ultrasound. Specifically, in our study, we focus on optical coherence tomography (OCT) images of arteries which are 3D and does not have any public datasets for SSL. This poses a challenge for us as there is no direct research directly addressing our problem.

% Basic rotation, jigsaw, rubik
 Following the same idea, Song et al.~\cite{Song2022} use rotation prediction, instance discrimination and VAE~\cite{Kingma2013}. Using VAE as a pre-text task, the model is semgnetaion-ready for COVID infection segmentation on lung CT images. This method outperforms supervised learning on similar architecture U-Net~\cite{Ronneberger2015} and U-Net++~\cite{Zhou2020}. Approaching 3D medical images differently, Rubik's cube solving is used as a 3D pre-text task where the model learns to classifies the orientation and ordering of a shuffled volumetric cube. Even though discriminative, Rubik's cube shows to improvement 3D segmentation in medical images ~\cite{Zhuang2019}. Jigsaw learns feature representations by predicting the correct order of shuffled patches~\cite{Noroozi2016}. Deep clustering clusters features vectors of the image in an unsupervised manner and uses thes clusters as pseudo-labels for classification~\cite{Caron2018}. Applying one or multiple pre-text tasks has been the main theme in several medical imaging SSL research papers ~\cite{Zhou2021, Zhang2021, Dufumier2021}. Taleb et al. ~\cite{Taleb2020} study 5 separated 3D self-supervised learning tasks, namely predicting latent vectors of adjacent patches, predicting the location of a given patch, solving jigsaw, predicting rotation angle and contrastive learning. In their study, predicting latent vectors of adjacent patches yields the best results in downstream tasks.

% Constrastive learning
TS-SSL~\cite{Zhang2021} proposes an SSL on 2D spectral domain optical coherence tomography (SD-OCT) images of retina to better classify retinal anomaly. It learns representation from classification labels, discriminative and generative tasks simultaneously. Using contrastive loss, it maximizes the similarity between rotated views or shuffled patches of the same image and minimizes the similarity between views of different images. At the same time, it uses a same representation to predict the rotation angle of the rotated views and the order of the shuffled patches. Along these tasks, TS-SSL also uses classification labels to predict the class of the image. However, this method only works better than supervised learning when 10\% of the labels are used for training. TS-SSL is not the only method that adopts modern SSL methods to medical imaging. Dufumier et al.~\cite{Dufumier2021} adds anoter layer to contrastive learning by using meta-data available in medical images. Specifically, they use age of the patient to indicate the degree of similarity between different images. This method improves bipolar disorder, Alzheimer and schizophrenia classification on 3D brain MRI images. Similarly, CLIP loss~\cite{Radford2021CLIP} can be used to leverage multi-modalities for representation learning ~\cite{Hager2023}. Encouraging images and their tabular meta-data, ubiquitous in medical imaging, to have similar representations. This method has been shown to improve classification tasks on brain MR images. 

Exploring boon of contrastive learning in segmentation, He et al.~\cite{He2022Intra} propose an intra- and inter-slice contrastive learning for point supervised OCT fluid segmentation of retina. The nature of this retinal OCT is 3D. While training segmentation model with U-Net~\cite{Ronneberger2015}, the authors leverage the fact that adjacent slices of the same volume are highly correlated, inter-slice CL is proposed to maximize the similarity between encoded vectors of adjacent slices as well as their segmentation masks. Segmentation masks are also compared with their ground truth masks and cross-entropy loss. Segmented masks are subsequently used to select the locations of small patches to be used for intra-slice CL. Intra-slice CL maximizes the similarity between encoded fluid patches and the backgroun fluid patches. Its result outperforms other point-based segmentation methods but is not as good as fully supervised learning. Besides spatio consistency as previous work, temporal consistency can also be imposed~\cite{Ren2022}. Given that images are taken from the same patient at different time points, temporal consistency can be used to improve the segmentation of the images. This method has been shown to improve the segmentation of brain MRI images.

% Restorative tasks
Alternatively, generative tasks have been proposed as pre-text tasks for medical imaging. Patch shuffling~\cite{Chen2019} propose a restorative task for 2D medical images of MRI, CT and ultrasound. First patches are randomly cut from the original image and placed to random locations. The model learns to restore the images back to their original versions. This method improves the downstream segmentation tasks in a data-scarce setting. In-painting~\cite{Pathak2016} removes a part of the image and the model learns to predict the missing part. Advancing further, Genesis~\cite{Zhou2021} explore image distortion algorithms for 3D restorative pre-text tasks on CT images. Thorough experiments show that Genesis outperforms specialized 3D state-of-the-art segmentation models and other pre-text tasks including de-noising~\cite{Vincent2010}, in-painting~\cite{Pathak2016}, jigsaw~\cite{Noroozi2016}, deep clustering~\cite{Caron2018}, rubik's~\cite{Zhuang2019} and patch shuffling~\cite{Chen2019}. This establishes a new base line for 3D SSL in medical imaging showing that mixing pre-text tasks is beneficial. Persuing a similar concept, SwinUNTER~\cite{Tang2022} proposes training ViT-based model with a combination of in-paining, constrastive and rotation.

Introducing deep clustering~\cite{Caron2018} idea to Genesis~\cite{Zhou2021}, TransVW adds a classification task to the restorative task of Genesis. First, visual words are mined automatically and grouped into clusters with deep latent features. These visual words (VW) are used to be classified and restored. Visual words improve segmentation and classification task in some datasets compared to Genesis~\cite{Haghighi2021}. DiRA~\cite{Haghighi2024} further improves TransVW by adding adversarial model to tell apart the restored images and the original image. Additionally, constrastive learning is used as an additional pre-text task. Combining, discriminative, restorative and adversarial training, DiRA outperforms TransVW and train-from-scratch models. Eventhough, TransVW and DiRA have shown to be better than Genesis, their evaluations are not as thorough. TransVW only outperforms Genesis in some datasets, while DiRA only evaluates its performace against TransVW.

A gap between pre-text tasks studied in medical imaging and those introduced in recent visual SSL research is observed. While visual SSL research has been moving towards more efficient and simpler pre-text tasks such as DINOv2~\cite{Oquab2024dinov}, MAE~\cite{He2022}, and I-JEPA~\cite{Assran2023}, medical imaging SSL research has been focusing on blending multiple pre-text tasks together. An attempt to fill this gap is evident in Genesis~\cite{Zhou2021} which works on 3D medical images at a pixel level and has shown to be better than other pre-text tasks. More studies are needed to explore simpler and more efficient pre-text tasks for medical imaging, closing the gap between visual SSL and medical imaging SSL. Concurrently, a robust evaluation in medical imaging research must be done as many reported results may not be reproducible~\cite{Isensee2024}.
% Alternatives
\section{Self-supervised learning alternatives}
Self-supervised learning has a great potential in medical imaging. If simple and robust pre-text tasks can be found, it can be used to learn representations from medical images without human annotations. However, such tasks are being explored and much work on the evaulation have to be done. Alternatives are introduced to efficiently train the models with existing small medical datasets.

SuPReM suggests to pre-train models with supervised learning on a large annotated datasets. SuPReM collates 9,262 CT volumes, pre-trains a models to segment subset of organs and fine-tune them on mutually exclusive subset of organs. The experiments show that it requires less data to learn meaning representations than SSL while also provides better transferability~\cite{Li2024}.

Rather than pre-training, nnUNet proposes a self-configuring framework for medical image segmentation. The framework automatically build UNet-like model for 2D and 3D medical images for a given dataset. It has been shown to robustly outperform state-of-the-art models, including popular Transformer-based and Mamba-base architectures, on several medical image segmentation tasks and works well on small datasets due to it robust positive data sampling and augmentation~\cite{Isensee2020}. Follow up work on nnUNet has been done showing that many reported results may not be reproducible due to many reasons. The study highlights that nnUNet is the consistently best performing model on the tasks it has been evaluated on without additional data~\cite{Isensee2024}.

Previous work at IMES to segment OCT images have been done with SegFormer pre-trained with ADE20k. SegFormer is a ViT-based model that enforces multiple resulotion at different depths of the transformer. Simple MLP~\cite{Rumelhart1986} decoder heads are introduced to combine the features from different depths. The model has been shown to be state-of-the-art on segmentic segmentation at the time~\cite{Xie2021SegFormer}.

Taking a different approach, RADIO proposes to learn representations from already learned models. RADIO is a student model that learns to predict a same feature from multiple teacher models, including DINOv2~\cite{Oquab2024dinov}, SAM~\cite{Kirillov2023SAM} and CLIP~\cite{Radford2021CLIP}. In addition, RADIO is trained to be resolution-agnostic, allowing it to understand images at different resolutions. RADIO has been shown to provide a high resolution feature map that can be used for downstream tasks~\cite{Ranzinger2024RADIO}. While SSLs has a great potential on medical imaging, their approaches should be compared to all other existing ones in order to ensure a comprehensive comparison.

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\end{document}