%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% To avoid any implication, this template is released into the
% public domain / CC0, whatever is most convenient for the author
% using this template.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[T1]{fontenc}

\title{Self-supervised learning\\for calcium segmentation on coronary OCT images}
\author{Naravich Chutisilp}
\supervisor{Karim Kadry, Ph.D. candidate}
\adviser{Professor Elazer R. Edelman, M.D., Ph.D.}
\coadviser{Professor Maria Brbi\'c, Ph.D.}
\expert{Farhad Rikhtegar Nezami, Ph.D.}

% \newcommand{\sysname}{FooSystem\xspace}

\dedication{
    \begin{raggedleft}
        It’s no use going back to yesterday, because I was a different person then.\\
        --- Lewis Carroll, Alice’s Adventures in Wonderland\\
    \end{raggedleft}
    \vspace{4cm}
    \begin{center}
        Dedicated to my lovely family, my dear friends, and kind people who have given opportunity and support to this lucky life I have.
    \end{center}
}
\acknowledgments{
% This is where you thank those who supported you on this journey. Good examples
% are your significant other, family, advisers, and other parties that inspired
% you during this project. Generally, this section is about 1/2 page to a page.

% Consider acknowledging the use and location of this thesis package.

% Define your acknowledgments in \texttt{\textbackslash{}acknowledgments\{...\}}
% and show them with \texttt{\textbackslash{}makeacks}.

    I would like to express my deep gratitude to my mom and dad for their unwavering support and firm faith in letting me choose my path. I would love to thank my two lovely sisters, Namking and Namkow, for their continuous love and understanding. 

    I am also grateful to all the friends I have made during my time at EPFL, Set, James, Kwang, Ice, Sundae, Nai, Cindy, Jenestin, Thomas, Paulina, Fah, Ting-Wei, Pin-Yen, Yi-Kai, Hong-Bin, Leo, Edvin, Anthon, Aamir, Jirka, and many more people not mentioned here, who have made my life abroad memorable and enjoyable. Going to EPFL was my first time living abroad for more than a month, and I could not have asked for a better experience than what I have had. Simple dinners in the evening every day after school, meaningful conversations about everything, and trips to alpine mountains and European cities have made this little boy feel accompanied in a foreign land amid an uncertain future.

    I must also not forget the friends I have made during my time at MIT, Mee, Pooh, Cue, and Champ, who have made my final Master's semester fun and unforgettable. Coming to MIT was a dream come true, yet it was not easy to leave my friends in Switzerland and start anew in the US. Unexpectedly, from the first day I arrived, I was welcomed with open arms and was shown to many places in New England. Worries and fears were replaced with excitement and joy. Hiking in the White Mountains, visiting California and Silicon Valley, walking along the Freedom Trail, enjoying such delicacies as lobster rolls and clam chowder, and playing board games every Friday night are just a few of the many memories I have made in the US that I will never forget.

    None of this would have been possible without the support from my friends in Thailand, Meak, Korn, Marie, Jump, Copter, Nat, Bank, Pinn, V, Poom, Pewt, Choomp, Krist, Minnie, May, Kuan, and uncountable more that I cannot fit into this section. Calls and messages from these lovely individuals have always been a source of strength and comfort. Through tough times and good times, they have always been there for me. Even though we are 9,188 km apart, or even 13,707 km apart, it feels as if they are always by my side. It is a blessing to have friends like them, and I have always been grateful for that.

    I would like to thank Prof. Elazer Edelman and Karim Kadry for giving me this unparalleled opportunity to join the IMES lab at MIT and for letting me complete my Master's thesis here, as well as for providing me with advice and support on my research. 
}


\begin{document}
\maketitle
\makededication
\makeacks

\begin{abstract}
% The system tool enables the lateral decomposition of a multi-dimensional
% flux compensator along the timing and space axes.

% The abstract serves as an executive summary of your project.
% Your abstract should cover at least the following topics, 1-2 sentences for
% each: what area you are in, the problem you focus on, why existing work is
% insufficient, what the high-level intuition of your work is, maybe a neat
% design or implementation decision, and key results of your evaluation.

Coronary artery disease (CAD) is a leading cause of mortality and morbidity worldwide, often treated with percutaneous coronary intervention (PCI) to expand narrowed arteries. However, the success of PCI can be hindered by coronary artery calcification (CAC), which may lead to under-expansion or malpositioning of the stent. Intravascular lithotripsy (IVL) is employed to break down these calcified plaques, facilitating proper stent deployment. Accurate calcium segmentation is critical for the success of IVL and can also be used to create digital twins of calcified arteries for enhanced stent deployment planning. Given the challenges and costs associated with manual annotation of optical coherence tomography (OCT) images, this thesis investigates methods to improve calcium segmentation without requiring additional annotations. We evaluate three main approaches: training from scratch with specialized algorithms, fine-tuning pre-trained models, and employing self-supervised learning (SSL) techniques on unannotated data. Our findings indicate that pre-training models to segment lumen and wall features is most effective for calcium identification. Additionally, we demonstrate that restorative and multi-modal image alignment tasks, such as Genesis and CLIP, offer viable alternatives with comparable accuracy, and without annotations. This study provides a comprehensive analysis of SSL methods tailored to OCT imaging, highlighting their potential to enhance segmentation performance while minimizing the need for expert annotations.
\end{abstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

% The introduction is a longer writeup that gently eases the reader into your
% % thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
% In the second paragraph, you can introduce the main challenge that you see.
% The third paragraph lists why related work is insufficient.
% The fourth and fifth paragraphs discuss your approach and why it is needed.
% The sixth paragraph will introduce your thesis statement. Think about how you can
% distill the essence of your thesis into a single sentence.
% The seventh paragraph will highlight some of your results
% The eighth paragraph discusses your core contribution.

% This section is usually 3-5 pages.
Coronary artery disease (CAD) is a leading cause of heart disease, death, and disability worldwide. It is characterized by the narrowing of coronary arteries due to plaque deposition, which impedes blood flow. Percutaneous coronary intervention (PCI) is a common treatment of CAD that involves expanding the arteries using a stent. However, the effectiveness of this treatment can be compromised by coronary artery calcification (CAC), which can prevent the stent from expanding properly, leading to under-expansion or malpositioning. Intravascular lithotripsy (IVL) is a technique used to break down these calcified plaques by sending shockwaves to pulverize the calcium deposits, allowing for successful stent deployment. Consequently, automatic calcium segmentation is indispensable for IVL procedures. These segmentations can also be used to create a digital twin of the calcified artery, facilitating research and planning for stent deployment.

% OCT images and co-registered Pre- and Post-IVl

% Annotation is tedious so SSL is needed
Acquiring human annotations for medical images is particularly challenging because it requires experts who understand specific medical image modalities. In our case, experts must identify the calcium deposits in optical coherence tomography (OCT) images, so the model can be trained to automatically segment the calcium. However, the annotation process is both expensive and time-consuming, as the experts must manually annotate each pixel of the image. To overcome these challenges, we seek to improve calcium segmentation without the need for additional annotations.

% What can be done to improve calcium segmentation
There are three main approaches to addressing the challenge of limited annotations for calcium segmentation: 
\begin{enumerate}
    \item \textbf{Training from Scratch} - Utilize an algorithm specifically designed for small medical datasets.
    \item \textbf{Fine-Tuning Pre-Trained Models} - Fine-tune models that have been pre-trained on larger datasets.
    \item \textbf{Self-Supervised Learning (SSL)} - Pre-train models on unannotated data using self-supervised learning techniques.
\end{enumerate}
The first two methods fall under the category of supervised learning, as they require annotations and are relatively straightforward. However, self-supervised learning might be a novel concept for some.

% The fourth and fifth paragraphs discuss your approach and why it is needed.
Self-supervised learning (SSL) is a technique that extracts knowledge from unannotated by generating pseudo-labels. For example, an SSL model might be trained to restore the original color of a grayscale image. This enables us to leverage the data that would otherwise be left unused to enhance the main task. The optimal pre-text task for images in SSL, however, remains an active area of research. Visual SSL has evolved significantly, from tasks such as denoising, colorizing, and solving jigsaw puzzles to more advanced techniques such as aligning different views of the same image, restoring highly distorted images, or predicting the masked areas of an image based on their contexts. 

% The third paragraph lists why related work is insufficient.
Although there are numerous methods proposed for both supervised and self-supervised learning, no existing research is directly applicable to our setting. Most techniques are designed for modalities such as X-ray, CT, and MRI images, which are publicly available. However, these modalities differ significantly from OCT images, particularly in the context of calcium segmentation, where the area of interest is small and surrounded by a large background. Moreover, the boundaries of calcium plaques in OCT images are less distinct compared to the objects in X-ray, CT, and MRI images. Furthermore, techniques such as CLIP, which is designed for language-image pairs, are not directly applicable to our co-registered image pairs of arteries before and after IVL. As a result, these techniques must be evaluated, compared, and potentially modified to suit our data. 

% The sixth paragraph will introduce your thesis statement. Think about how you can
% distill the essence of your thesis into a single sentence.
In this thesis, we explore the optimal methods for improving calcium segmentation in OCT images by leveraging both annotated and unannotated data. By tailoring state-of-the-art self-supervised learning techniques to our unannotated data and comparing them with supervised learning methods, we aim to understand the key factors in identifying calcium. 
% This comprehensive approach is essential for assessing the suitability of various research methods in our specific setting. 
We hypothesize that utilizing unannotated data can enhance calcium segmentation, offering an economical solution for clinicians when resources are limited.

% The seventh paragraph will highlight some of your results
% Objective
% What we did and the result
Through our comprehensive experiments, we found that the most effective method for improving calcium segmentation involves pre-training the model to segment the lumen and wall, as they are the key features for identifying calcified plaques. However, restorative tasks (e.g., Genesis) and multi-modal image alignment tasks (e.g., CLIP) offer cost-effective alternatives that provide comparable results without requiring annotations. Our analysis of these techniques across different scales of training data demonstrated that both Genesis and CLIP can improve segmentation accuracy, with CLIP offering better confidence and correctness.

% The eighth paragraph discusses your core contribution.
Our contributions are as follows:
\begin{enumerate}
    \item Compared supervised and self-supervised learning techniques for calcium segmentation.
    \item Assessed the efficacy of pre-trained models from both self-supervised and supervised pre-training for calcium segmentation. 
    \item Analyzed 2D and 3D segmentation approaches in the context of 3D digital twin reconstruction, focusing on the continuity of the masks produced by each method.
    \item Evaluated self-supervised learning methods for medical imaging, including contrastive, restorative, and mask-image-modeling approaches, to bridge the gap between natural and medical visual self-supervised learning.
    \item Proposed and evaluated segmentation heads for V-JEPA, a transformer-based self-supervised learning method for natural videos, and developed an approach to track its convergence during pre-training.
    \item Suggested modifications to CLIP pre-training for multi-modal co-registered OCT images.
    \item Proposed modifications to Genesis pre-training for unannotated OCT images. 
    \item Analyzed the best methods across different scales of training data to provide insights into the suitability of each technique.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your
% work. This is not necessarily related work but technologies and dependencies
% that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.
Coronary artery disease (CAD) is a leading cause of heart diseases and fatalities worldwide~\cite{Ralapanawa2021, Virani2021Heart, Wu2015}. It is primarily caused by atherosclerosis, a condition characterized by the buildup of plaques in coronary arteries, which partially or totally obstruct blood flow~\cite{Shahjehan2023}. A common treatment for CAD is percutaneous coronary intervention (PCI), a procedure that enlarges the narrowed artery by expanding the stent in the lesion~\cite{Ahmad2023}. However, the success of PCI can be compromised by coronary artery calcification (CAC), which hinders stent expansion and may lead to under-expansion, distortion, or dislodgement of the stent~\cite{Hennessey2023}. Therefore, removing these calcium plaques is necessary to improve the effectiveness of PCI. 

Intravascular lithotripsy (IVL) is a novel technology that uses sonic pressure waves to break calcified lesions into fragments, facilitating stent deployment~\cite{Butt2023}. For IVL to be successful, accurate identification of the calcified lesions is crucial~\cite{Butt2023}. These lesions are generally detected using intravascular imaging techniques such as intravascular ultrasound (IVUS) and optical coherence tomography (OCT)~\cite{Butt2023}, with OCT delivering higher accuracy~\cite{Fujimoto2003, Costopoulos2016}. However, the manual segmentation of calcified lesions in OCT images is both time-consuming and subjective~\cite{Segars2013, Oktay2020, Carpenter2022}. Consequently, an automatic calcium segmentation method is needed to improve the efficiency and accuracy of the process~\cite{Carpenter2022}. Additionally, calcium segmentation can aid in creating a digital twin of the calcified plaque, enabling physics-based simulations~\cite{Karanasiou2020, Poletti2022}, and can also be used to train synthetic anatomy diffusion models, generating a broader variety of data for simulations~\cite{Kadry2024}.

U-Net is a commonly used model for image segmentation, employing convolutional layers to extract features from input images. Each layer uses kernels that slide over the input image to extract visual features. U-Net introduces an encoder-decoder architecture with skip connections that explicitly leverage features at different resolutions for segmentation~\cite{Ronneberger2015}. The encoder consists of convolutional layers with downsampling, while the decoder involves convolutional layers with upsampling. Skip connections between the encoder and decoder allow the model to combine features at various resolutions, enabling it to learn and utilize representations at different levels. To adapt U-Net for 3D segmentation, the 2D kernels in convolutional layers are replaced with 3D kernels, allowing the model to process 3D volumes directly and preserve spatial information in the depth dimension. This adaptation, while requiring 3D data and increasing the cost of annotation and computation, allows the model to leverage more information and potentially improve performance.

The transformer architecture is increasingly recognized for its effectiveness in segmentation tasks. Following its success in natural language processing (NLP)~\cite{Vaswani2017}, the Vision Transformer (ViT) was introduced for 2D image classification~\cite{Dosovitskiy2020vit} and has proven successful in various tasks, including image segmentation. ViT divides the input image into patches, tokenizes them using a convolutional encoder, and adds positional information before feeding them into a transformer. Several transformer-based architectures for image segmentation have been proposed in the literature. However, linear projection can be used to transform ViT outputs to segmentation masks without modifying the architecture~\cite{Oquab2024dinov, Ranzinger2024RADIO}. To enable a transformer model to perform 3D segmentation, the patchification process must be adapted to 3D patches, the tokenizer should use 3D kernels, and the positional encoder should incorporate the depth dimension, while the rest of the architecture remains the same as in 2D segmentation.

Medical image segmentation is a challenging task that typically requires specialized annotators to provide pixel-level ground truth labels for training, leading to research focused on finding cost-effective methods for medical image annotation~\cite{Fu2012, Gal2017, Beluch2018, Rahimi2021}. Self-supervised learning (SSL) has emerged as a promising approach, allowing models to learn representations from the data itself without human annotations. Although SSL is well-established in NLP, it is still an active area pf research in computer vision (CV)~\cite{Balestriero2023}. 

Initially, SSL in computer vision was based on traditional pre-text tasks such as autoencoder~\cite{Hinton2006} and denoising autoencoder~\cite{Vincent2008}. While useful in some cases, these tasks do not yet match the performance of SSL in NLP. To address this, pre-text tasks that explicitly embrace visual semantics have been proposed, including solving jigsaw puzzle~\cite{Noroozi2016}, rotation prediction~\cite{Gidaris2018}, and colorization~\cite{Larsson2017}. Recently, visual SSL has become more efficient and simpler, with approaches generally categorized into discriminative and generative tasks. Discriminative tasks learn representations by distinguishing between different views of the same image. For instance, SimCLR enforces that an augmented image be encoded into the same latent vector as the original image~\cite{Chen2020Simple}. Generative tasks learn representations by predicting the data directly. Mask image modeling (MIM), for example, predicts the masked-out parts of images. These modern tasks are more effective than traditional pre-text tasks in learning representations~\cite{Chen2020Simple, He2020, He2022, Bao2022beit}.

In medical imaging, visual SSL is evolving in tandem with advancements in SSL for natural images. Researchers have been exploring the combination of multiple pre-text tasks to enhance classification and segmentation tasks in medical imaging \cite{Noroozi2016, Zhuang2019}. More recent researches also investigate blending discriminative tasks with traditional pre-text tasks~\cite{Zhou2021, Zhang2021, Dufumier2021, Taleb2020, Zhang2021, He2022Intra, Ren2022}. Additionally, discriminative tasks allow different modalities of data to be learned together by operating on the embedding vectors of each modality~\cite{Radford2021CLIP, Hager2023}. Generative tasks in medical imaging are predominantly restorative \cite{Pathak2016, Chen2019, Zhou2021, Tang2022, Haghighi2021, Haghighi2024}. However, experiments applying modern generative tasks beyond restorative ones have shown great potential \cite{Baharoon2023general, Zhou2022}.

Generative tasks in SSL are also evolving.  The masked autoencoder (MAE) learns representations by filling in the masked patches of images \cite{He2022}. Despite its simplicity, working at the pixel level is computationally expensive. The bidirectional encoder for image transformers (BEiT) operates at the token level instead of the pixel level, making it more efficient as it eliminates the need for translating from tokens to pixel values \cite{Bao2022beit}. DINOv2 combines discriminative and generative tasks, working at the token level and in latent space, making it more efficient and effective than other visual SSL methods~\cite{Oquab2024dinov}. I-JEPA takes this a step further by directly predicting the encoded vectors of masked patches~\cite{Assran2023}, offering a simpler and more scalable approach to generative SSL. While these methods have proven effective in visual SSL, their application in medical imaging remains underexplored. If embedding-level operations prove beneficial in medical imaging, it could revolutionize the field by offering a highly efficient and scalable approach to learning representations.

Despite the significant potential of visual SSL in medical imaging, other viable options should also be considered. One approach is to use frameworks specifically designed for small medical datasets \cite{Isensee2020}. Alternatively, fine-tuning models pre-trained on larger datasets can be effective, as these models can leverage their learned knowledge across different tasks \cite{Ghosh2024, Khaled2023}.
%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

% Introduce and discuss the design decisions that you made during this project.
% Highlight why individual decisions are important and/or necessary. Discuss
% how the design fits together.

% This section is usually 5-10 pages.

\section{Datasets}\label{sec:design:datasets}
The primary goal of this thesis is to study the effectiveness of self-supervised and supervised pre-training methods for calcium segmentation of coronary optical coherence tomography (OCT) images. The datasets we utilized involve 528 volumes of coronary OCT images, which can be divided into the following three categories based on their annotation or the lack thereof. 

\subsubsection{Dataset Categories}

The first category, Calcium OCT (Dataset \ref{enum:calcium-dataset}), contains 8 volumes in which calcium plaques are annotated. The second category, Lumen and Wall OCT (LaW OCT) (Dataset \ref{enum:lumen-and-wall-dataset}), contains 20 volumes in which the lumen and the wall are annotated. These two datasets are mutually exclusive, meaning that the OCT images are not duplicated between them. The third and final category, Unannotated OCT (Dataset \ref{enum:unannotated-dataset}), contains 500 volumes with no annotations. 

The unannotated OCT images were roughly equally distributed across different treatment stages, including 167 volumes before intravascular lithotripsy (Pre-IVL), 169 volumes after IVL (Post-IVL), and 164 volumes after stent deployment (Post-Stent). Some of these volumes are co-registered pairs of Pre-IVL and Post-IVL images; i.e., the images were taken from the same site before and after the IVL procedure, ensuring alignment along the depth dimension to match their positions in the artery (see Figure~\ref{fig:co-registered-oct}). 

It is worth noting that the vast majority of our datasets are unannotated (500 out of 528 volumes), highlighting the scarcity of annotations and the cost of obtaining them. Although we have only 528 volumes of OCT images in total, each volume is a three-dimensional (3D) image that can be diced into smaller chunks, effectively increasing the amount of data available for this project. Algorithms that exploit this feature will be explained in later sections.

In summary, the datasets utilized in this study include:

% TODO: describe Pre/Post-IVL here
\begin{enumerate}
    \item Annotated datasets:
    \begin{enumerate}
        \item \label{enum:calcium-dataset} \textbf{Calcium OCT} is a dataset of coronary OCT images with calcium annotations before IVL.
        \item \label{enum:lumen-and-wall-dataset} \textbf{Lumen and Wall OCT (LaW OCT)} is a dataset of coronary OCT images with lumen and wall annotations before IVL.
    \end{enumerate}
    \item \label{enum:unannotated-dataset} Unannotated datasets:
    \begin{enumerate}
        \item \textbf{Pre-IVL} is a dataset of coronary OCT images before IVL.
        \item \textbf{Post-IVL} is a dataset of coronary OCT images after IVL.
        \item \textbf{Post-Stent} is a dataset of coronary OCT images after stent deployment.
    \end{enumerate}
\end{enumerate}

\subsubsection{Purpose of Each Dataset}

Each dataset serves a different purpose in this study. Calcium OCT has already been annotated for calcium plaques by medical professionals, and hence this dataset may be used to train a model or serve as the ground truth for evaluating the effectiveness of our computational algorithms for calcium segmentation. LaW OCT has already been annotated for the lumen and the wall, and hence this dataset can be used for supervised pre-training ("supervised" indicates prior annotation). Finally, the unannotated OCT datasets are used for self-supervised pre-training ("self-supervised" indicates no prior annotation). 
% Additionally, the tabular meta-data can be used to evaluate the effectiveness of the algorithms on multi-modalities, but this is not the main focus of this study.

\begin{figure}[hbt]
    % \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/fig_datasets_calcium_oct_sample.pdf}
        \caption{An example of an annotated image from the Calcium OCT dataset.}
        \label{fig:calcium-oct}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/fig_datasets_law_oct_sample.pdf}
        \caption{An example of an annotated image from the Lumen and Wall OCT dataset (LaW OCT).}
        \label{fig:lumen-and-wall-oct}
    \end{subfigure}
    \caption{Examples of annotated Calcium OCT (a) and LaW OCT (b) images. Top, the original OCT image from the middle slice across three axes, displayed as three panels from left to right. Bottom, the corresponding segmentation masks.}
    \label{fig:annotated-oct}
\end{figure}

\subsubsection{Usage of Each Dataset}

% Each dataset has unique characteristics and is divided into training, validation, and testing sets, except for the unannotated datasets, which are used for self-supervised pre-training.
Calcium OCT has one class: calcium. The size of each image is $500\times 500$, while the depth varies between $375$ and $539$. Examples of Calcium OCT images are shown in Figure~\ref{fig:calcium-oct}. The dataset is split into 6 training volumes and 2 testing volumes, and the training volumes are further divided into 3 folds of training and validation sets. In each fold, a model is trained using the training set, and the best model is selected using the validation set before being evaluated on the testing set not seen during training. Finally, performance data across 3 folds are averaged to report the final performance. This is done to ensure a robust evaluation of the methods.

LaW OCT has 2 classes: lumen and wall. The size of each image is $500\times 500$, while the depth varies between $270$ and $540$. Examples of LaW OCT images are shown in Figure~\ref{fig:lumen-and-wall-oct}. The dataset is split into 16 training volumes and 4 testing volumes. 

The unannotated datasets are of size $500\times 500$ and varying depths. They are not split into training and testing sets because they have not been annotated (and hence there is no ground truth to test against) and can only be used for self-supervised pre-training. However, as will be explained in later sections, each self-supervised learning algorithm may divide the unannotated datasets into training and validation sets to track convergence. Examples of unannotated OCT images are shown in Figure~\ref{fig:unannotated-oct}.

\begin{figure}[hbt]
    % \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.65\linewidth]{figures/fig_datasets_coregistered_prepostivl.png}
        \caption{Two examples (top and bottom) of co-registered images between Pre-IVL (left) and Post-IVL (right)}
        \label{fig:co-registered-oct}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/fig_datasets_unlabeled_oct_sample.pdf}
        \caption{Examples of unannotated OCT images Pre-IVL (left), Post-IVL (middle), and Post-Stent (right), taken from the middle slice across three axes arranged from top to bottom}
        \label{fig:unannotated-oct}
    \end{subfigure}
    \caption{Examples of unannotated Calcium OCT images.}%For each image, the top row shows the original image at the middle slice across three axes, while the bottom row displays the corresponding segmentation masks.}
\end{figure}

\newpage
\section{Algorithms}
In this section, we describe various learning methods for calcium segmentation, categorizing them into supervised and self-supervised approaches. The key difference lies in whether the approach requires human-annotated data for training. As the names suggest, "supervised" learning requires annotated data, whereas "self-supervised" learning does not. The distinction is not always clear, and more explanation will be provided as necessary.

Supervised learning can be achieved in two manners. The first and more straightforward method is to train a model for calcium segmentation from scratch using only the Calcium OCT dataset. In other words, this approach uses calcium-annotated data to teach the model to identify calcium plaques. The second method involves pre-training a model on a seemingly unrelated annotated dataset (also called "supervised pre-training") before fine-tuning it for calcium segmentation (also called "transfer learning"). For example, we could use the LaW OCT dataset to teach a model about the general anatomy of the artery (pre-training) before re-purposing it to identify calcium plaques in the Calcium OCT dataset (transfer learning). 

Self-supervised learning involves pre-training a model on an unannotated dataset before fine-tuning it for calcium segmentation. There are several ways by which we can pre-train a model using unannotated data, which are described in detail below. For example, we can artificially censor a small part of an unannotated OCT image and train a model to fill in the blank. As the model gets better at filling in the blank, it also learns more about the anatomy of the artery and may eventually be able to identify calcium plaques accurately. While this may sound unintuitive, it underscores the ingenuity of self-supervised learning.  

%The key difference between supervised and self-supervised in this study lies in whether the pre-training process requires human annotation. It is important to note that CLIP is typically categorized as a weakly supervised learning method since it relies on human annotations to co-register pairs of different modalities. However, as we advance towards automatic co-registration, we have opted to classify it under self-supervised learning for simplicity.

\subsection{Supervised Learning}
\subsubsection{nnUNet}\label{sec:design:nnunet}
nnUNet is a self-configuring framework for medical image segmentation~\cite{Isensee2020}. First, a data fingerprint must be provided to nnUNet, which specifies the number of image modalities, the normalization process for each modality, and the number of segmentation classes. Additional details such as intensity range, image spacing, shape, and resolution are derived from the metadata. Using this fingerprint, nnUNet automatically determines the network topology, sampling strategy, data augmentation approach, batch size, and patch size for the training process. Notably, nnUNet also offers a supervised pre-training pipeline, allowing the model to be trained on other datasets before being fine-tuned on the target dataset.

We designed two approaches to apply the algorithm to our datasets. The first approach involves directly training the model on the Calcium OCT dataset. The second approach involves pre-training the model on the LaW OCT dataset before fine-tuning it for calcium segmentation. 

nnUNet offers both 2D and 3D segmentation architectures, as well as a cascade 3D architecture combining full-resolution and low-resolution networks. Our preliminary experiments indicated that the 2D and 3D architectures outperformed the cascade architecture. Consequently, we decided to use only the 2D and 3D architectures for the remainder of our experiments. The cascade architecture will not be discussed further.

\subsubsection{SegFormer}
SegFormer is a transformer-based model that integrates multi-resolution feature fusion through simple multi-layer perceptrons (MLP~\cite{Rumelhart1986})~\cite{Xie2021SegFormer}. Pre-trained on the ADE20k dataset~\cite{Zhou2018}, SegFormer mirrors U-Net's encoder by downsampling input images via overlap patch merging, which unifies adjacent patches into a single unit. This process allows the model to capture the features at various resolutions. Furthermore, SegFormer utilizes these multi-resolution features with an All-MLP decoder, which aligns lower-resolution features with the channels of their higher-resolution counterparts. The decoder then upsamples the features up to the original resolution and concatenates them with the higher-resolution features, ultimately predicting the segmentation masks using only MLPs and upsampling layers. 

SegFormer architectures comes in various sizes, from MiT-B1 to MiT-B5, with the largest model, MiT-B5, containing 84.7M parameters. We fine-tuned the largest pre-trained model on Calcium OCT for 80,000 iterations, which was sufficient for the model to converge.

\subsubsection{RADIO}
RADIO is a distillation framework that enables the use of multiple vision foundational models as teachers \cite{Ranzinger2024RADIO}. The framework was assessed on 2D segmentation tasks, incorporating batch normalization and linear projection in the decoder head. In alignment with this evaluation protocol, we employed their pre-trained Visual Transformer (ViT) model. Specifically, we utilized the RADIO ViT-H/16 model and fine-tuned it on the Calcium OCT dataset. The training was conducted for 80,000 iterations, which was determined to be adequate for model convergence.

\subsubsection{SuPreM}~\label{sec:design:suprem}
SuPreM is a supervised pre-training method specifically designed for 3D medical image segmentation \cite{Li2024}. It involves training models on a large scale dataset of CT volumes before fine-tuning. Among the models pre-trained with SuPreM, we chose the SwinUNETR model~\cite{Tang2022} due to its reported superior performance and fine-tuned it on the Calcium OCT dataset. It is important to note that the publicly available models have been pre-trained on 2,100 CT volumes, rather than 9,262, as the authors have not yet released the fully pre-trained versions.

SwinUNETR is a transformer-based model designed for segmenting brain tumors in 3D MRI images. Its architecture is built upon the Swin Transformer encoder, extracting hierarchical features from the input image.~\cite{Liu2021Swin}. The model also includes a CNN-based decoder, UNETR, featuring convolutional layers with upsampling and skip connections to merge features across different levels~\cite{Hatamizadeh2022}. The authors of SwinUNETR proposed a self-supervised learning algorithm for the model~\cite{Tang2022}. 

To evaluate the algorithms, we conducted three experiments. The first experiment fine-tuned the pre-trained model on the Calcium OCT dataset. The second experiment involved fine-tuning the self-supervised SwinUNETR model on the same dataset. The third experiment trained the SwinUNETR model from scratch, serving as a baseline to assess the effectiveness of SuPreM pre-training and the self-supervised learning method proposed for SwinUNETR within the context of the Calcium OCT dataset.

\subsection{Self-Supervised Learning}
\subsubsection{V-JEPA}
V-JEPA is a self-supervised learning method that learns representations by predicting masked parts of a video. It uses a context encoder to make these predictions and compares the predicted latent vectors with those generated by a target encoder, which views the video without masking~\cite{Bardes2024Vjepa}. The author provides ViT models pre-trained on the VideoMix2M dataset, which contains 2 million videos, using V-JEPA pre-training. We conducted two experiments to evaluate the effectiveness of V-JEPA on OCT images. In the first experiment, we fine-tuned the V-JEPA model on Calcium OCT. In the second experiment, we applied V-JEPA pre-training to the Unannotated OCT (Dataset ~\ref{enum:unannotated-dataset}) and fine-tuned the resulting model on Calcium OCT. Since the original paper does not address segmentation tasks, we will discuss the implementation of the decoder head in Section~\ref{sec:implementation:vjepa}.

\subsubsection{Genesis}
Genesis is a self-supervised learning method that learns representations by restoring the original images from their corrupted versions~\cite{Zhou2021}. In their paper, Genesis is applied to a U-Net architecture, which contains 15M parameters. We adapted their method to the same 3D nnUNet architecture in Section~\ref{sec:design:nnunet}, which has 30M parameters. Thereafter, we applied Genesis pre-training to unannotated OCT images and fine-tuned the resulting model on Calcium OCT.

\subsubsection{CLIP}
Contrastive Language-Image Pre-training (CLIP) is a contrastive learning method that matches encoded vectors of images with their text captions~\cite{Radford2021CLIP}. In our work, we adapted CLIP to OCT images by using co-registered Pre-IVL and Post-IVL images. We used the same nnUNet architecture as described in Section~\ref{sec:design:nnunet}. CLIP pre-training was applied to the 3D nnUNet, which was then fine-tuned on the Calcium OCT dataset. To implement this, we modified the code from a publicly available project~\cite{Shariatnia2021}. While the original paper proposed this pre-training framework for aligning language and images, we tailored the framework for aligning co-registered OCT images instead. Details of this adaptation will be discussed in Section~\ref{sec:implementation:clip}. 

It is important to note that CLIP is typically categorized as a weakly supervised learning method since it relies on human co-registration of image pairs. However, as the field is advancing towards automatic co-registration, we have opted to classify it under self-supervised learning for simplicity. 

%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

% The implementation covers some of the implementation details of your project.
% This is not intended to be a low-level description of every line of code that
% you wrote but covers the implementation aspects of the projects.

% This section is usually 3-5 pages.
\section{Supervised Learning}
\subsubsection{nnUNet}
We applied the official nnUNet~\cite{Isensee2020} code on our datasets, adding an early stopping criterion to the optimizer. In particular, we observed that the 3D segmentation performance of nnUNet was significantly worse than its 2D counterpart. To address this, we experimented with different patch sizes for 3D segmentation, diverging from the default settings. Reducing the depth of the original patch size from $112$ to $32$ improved performance significantly. This could be because smaller depth provided the model with more examples to learn from. In the second experiment, we pre-trained the model on LaW OCT before fine-tuning it on Calcium OCT using the supervised pre-training script provided by nnUNet.

\subsubsection{SegFormer}\label{sec:implementation:segformer}
We adopted the pre-trained SegFormer model along with a training script from MMSegmentation~\cite{mmseg2020}. Since SegFormer is designed exclusively for 2D segmentation, we pre-processed our Calcium OCT dataset into 2D images before training the model on the reformatted data. To ensure a fair comparison, we also maintained the same three folds and testing set as those used in nnUNet.

\subsubsection{RADIO}
We utilized the pre-trained RADIO's ViT model and its training script available in MMSegmentation~\cite{mmseg2020}. Since RADIO, like SegFormer, only supports 2D segmentation, we used the same pre-processed datasets as described in Section~\ref{sec:implementation:segformer}.

\subsubsection{SuPreM}
SuPreM provides official pre-trained models and code~\cite{Li2024}. Accordingly, we reformatted our Calcium OCT dataset to align with the format of SuPreM's datasets, while ensuring that the three folds and testing set remained consistent with those used in other experiments within this study. We three experiments, as outlined in Section~\ref{sec:design:suprem}, employing the official script from SuPreM.

\section{Self-Supervised Learning}
\subsubsection{V-JEPA}\label{sec:implementation:vjepa}
We developed three segmentation decoder heads for the Vision Transformer (ViT) pre-trained with V-JEPA, because it had previously been evaluated solely on classification tasks using an encoder-only transformer referred to as attentive decoder. To extend its utility to segmentation tasks, we designed an attentive decoder head for segmentation, drawing inspiration from the MAE decoder~\cite{He2022}, which directly predicts pixel values. Specifically, our decoder head employs an encoder-only transformer of 1 depth and 12 attention heads, followed by a linear projection layer that transforms embedding vectors into a vector of size \(\{\text{patch size} \times \text{patch size} \times \text{number of classes}\}\). 

In addition, we experimented with alternative decoding architectures, such as a simple batch normalization layer followed by a linear projection, similar to the adaptations made in RADIO~\cite{Ranzinger2024RADIO} and DINOv2~\cite{Oquab2024dinov} for segmentation tasks. Both variations are illustrated in Figure~\ref{fig:vjepa-attentive-and-batchnorm-decoder-head}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/fig_implementation_vjepa_attentive_and_batchnorm_decoder.pdf}
    \caption{Attentive and linear batch normalization decoder head for ViT (V-JEPA).}
    \label{fig:vjepa-attentive-and-batchnorm-decoder-head}
\end{figure}%

Further, inspired by SegFormer~\cite{Xie2021SegFormer}, we developed multi-feature decoder with a multi-layer perceptron (MLP) on each feature vector at different depths, followed by a linear projection to combine those features into a segmentation mask. This approach allows the decoder to leverage multiple features at various levels. However, unlike SegFormer, our method does not downsample each feature, thereby not explicitly forcing them to lower resolutions. Ultimately, this method allowed us to utilize ViT pre-trained with V-JEPA, ViT (V-JEPA), in our segmentation task. The multi-feature decoder head is shown in Figure~\ref{fig:vjepa-multi-feature-decoder-head}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig_implementation_vjepa_multifeat.pdf}
    \caption{A multi-feature decoder head. Feature at each transformer block is projected to the same dimension and concatenated before being passed to a convolutional layer that produces the segmentation mask.}
    \label{fig:vjepa-multi-feature-decoder-head}
\end{figure}

\subsubsection{Genesis}
We altered the official code for Genesis provided by the authors~\cite{Zhou2021} by adding a pre-processing step to the method. Genesis implementation begins with an initial pre-processing step on unannotated OCT images, where small cubes of size $64\times 64\times 64\times 32$ are sampled from the volumes. These cubes are then dynamically corrupted during the Genesis pre-training process. The authors recommend sampling $96$ cubes per volume, noting that increasing this number does not necessarily improve the performance and adds computation cost. Following this recommendation, we implemented an additional sampling strategy to enhance the quality of the cubes.

Our strategy involved randomly sampling $96$ cubes from each volume, ensuring that the selected cubes contained enough information, given that OCT images often contain substantial background, unlike CT images used in the original paper. We achieved this by applying a thresholding technique, selecting only those cubes with average intensities above a certain threshold. As illustrated in Figure~\ref{fig:genesis-cubes-with-threshold}, the resulting cubes contain meaningful structures such as the artery's lumen and wall. In contrast, without our thresholding strategy, the sampled cubes are dominated by background noise, as shown in Figure~\ref{fig:genesis-cubes-without-threshold}. We trained the model on these cubes until convergence.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube.png}
        \caption{Cubes from unannotated OCT images sampled with the thresholding strategy.}
        \label{fig:genesis-cubes-with-threshold}

    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/fig_implementation_genesis_extracted_cube_no_threshold.png}
        \caption{Examples of cubes sampled from unannotated OCT images. Each row represents a sampled cube with slices unrolled along the depth dimension.}
        \label{fig:genesis-cubes-without-threshold}
    \end{subfigure}
    \caption{Samples of cubes from unannotated OCT images. Each row is a sampled cube with slices unrolled along the depth dimension.}
\end{figure}

For fine-tuning, we loaded the pre-trained weights of the model's encoder and decoder into the 3D nnUNet, leaving the segmentation head to be randomly initialized. We then fine-tuned the pre-trained model on the Calcium OCT dataset, utilizing the optimal patch size of $160\times 128\times 32$ (height by width by depth) identified in Section~\ref{sec:result:nnunet}. The nnUNet's training script was used with the same configurations as the supervised pre-training, providing the pre-trained model as the initial weights.

\subsubsection{CLIP}\label{sec:implementation:clip}
We adopted the CLIP model, originally designed for language-image pairs, to work with paired Pre-IVL and Post-IVL volumes by replacing the text encoder with an image encoder. However, this straightforward modification did not result in successful pre-training convergence, necessitating further adjustments using a shared encoder.

Our experiments revealed that using separate encoders and projectors for each modality was less effective than employing a shared encoder and projector. To validate this, we conducted a sanity check by evaluating different encoder and projector configurations. Specifically, we passed the same images through these configurations to see if they could learn to represent the same image as the same vector. We tested three setups: (1) separate encoders and projectors, (2) separate encoders with shared projectors, and (3) shared encoders and projectors. The results showed that the configurations with shared components—either shared projectors or shared encoders and projectors—could match the same image.

Building on these findings, we applied the two successful configurations to the main task of matching co-registered images. Only the configuration with shared encoders and projectors achieved convergence, which is logical as this setup forces the image encoder and projector to learn a representation applicable to both modalities. This process is illustrated in Figure~\ref{fig:clip-oct}. Further details of the experiment are provided in Section~\ref{sec:results:discussion:clip}.

\begin{figure}[hb]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/fig_implementation_clip_oct.pdf}
    \caption{A modified CLIP model for co-registered multi-modal OCT images. Note that the projector is omitted for simplicity. The image encoder and projector are shared across all modalities.}
    \label{fig:clip-oct}
\end{figure}

%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Discussion}
%%%%%%%%%%%%%%%%%%%%

% In the evaluation you convince the reader that your design works as intended.
% Describe the evaluation setup, the designed experiments, and how the
% experiments showcase the individual points you want to prove.

% This section is usually 5-10 pages.
We evaluated the effectiveness of supervised and self-supervised learning algorithms for calcium segmentation on coronary OCT images. During training, models were fine-tuned against the validation subset of the Calcium OCT dataset to select the best model, which was repeated across 3 folds. These best models were then evaluated against the testing set to determine whether they can accurately identify calcium plaques compared to medical annotations, also called ground truth. Performance was measured using the Dice score, a common metric for segmentation tasks, which is defined as follows:

\begin{equation}
    \text{Dice} = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}
\end{equation}

where TP, FP, and FN are true positive, false positive, and false negative, respectively. True positives refer to algorithm-predicted calcium that is present in the ground truth; false positives refer to algorithm-predicted calcium that is absent in the ground truth; and false negatives refer to a lack of algorithm-predicted calcium that is present in the ground truth. The Dice score ranges from 0 to 1, where 1 indicates perfect segmentation. All dice scores shown in this section are the final dice scores that have been averaged across several folds of training sets and across several patients in the testing set, effectively representing the mean performance over multiple replicate studies. %The Dice score was calculated for each patient in the testing set, and the average Dice score was reported as the performance of the model for each fold. The final performance is the average of the Dice scores across all folds.

\section{nnUNet}\label{sec:result:nnunet}
\subsubsection{Effect of Image Size}
The default configuration of 3D nnUNet (Dice score = 0.697 for 3D $512\times 512\times 32$) has significantly worse performance than 2D nnUNet (Dice score = 0.753 for 2D $512\times 512$), suggesting that 2D may be generally advantageous over 3D for this task. For 3D nnUNet, reducing the depth of the patch size from $112$ to $32$ improves performance significantly, increasing the Dice score from 0.661 to 0.697. The smaller depth potentially allows sectioning of the 3D images into more slices, generating more examples for the model to learn from. By contrast, expanding the width and height of the patch size from $160\times 128$ to $512\times 512$ does not improve performance, decreasing the Dice score from 0.732 to 0.697. This may be due to the localized nature of calcium plaques in OCT images, where a planar expansion along the width and height dimensions is unlikely to provide the model with additional information while reducing examples available for the training process. Alternatively, the expanded images are of higher resolution and may require a deeper network than the one used for the smaller images. The performance of nnUNet on the Calcium OCT dataset is summarized in Figure~\ref{fig:nnunet-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/result_nnunet_results.png}
    \caption{Dice score of variations of nnUNet. Results are grouped according to the patch size used in training. Results in hashes indicate supervised pre-training on the lumen and wall dataset (LaW OCT). 
    }
    \label{fig:nnunet-results}
\end{figure}

\subsubsection{Effect of Pre-training}
Pre-training nnUNet with the lumen and wall dataset (LaW OCT) before fine-tuning it on Calcium OCT confers little benefit to 2D nnUNet (Dice scores = 0.752 versus 0.753) but significantly improves 3D nnUNet performance, especially when the patch size in the depth dimension is small (Dice scores = 0.742 versus 0.732 for $160\times 128\times 32$, and Dice scores = 0.740 versus 0.697 for $512\times 512\times 32$). We speculate that, because calcium plaques are typically located around the periphery of artery walls, pre-training with LaW OCT allows the model to learn the features of the walls, which may be helpful for calcium segmentation. The LaW-pre-trained 3D nnUNet models (Dice scores = 0.742 and 0.740) yield statistically comparable performance to 2D nnUNet models (Dice scores = 0.752 and 0.753) Figure~\ref{fig:nnunet-results}. 

\subsubsection{Continuity}
As 2D nnUNet has a better Dice score than 3D nnUNet, it is worth asking whether 3D segmentation provides any advantages at all, or are we better off developing only 2D segmentation models going forward?

We find that 3D models may still be of use, particularly in cases where continuity is necessary. Continuity refers to the consistency of segmentation results across adjacent image slices. Imagine looking at three consecutive, closely spaced slices of the same artery. If a calcium plaque is identified in the two outer slices, then it should also be present in the middle slice by continuity. In our research group, we place great emphasis on the continuity of segmentation because one of the use cases is to reconstruct a 3D digital twin of the artery, which can be used for various medical simulations. Consequently, we analyzed the correctness and consistency of the 2D and 3D nnUNet. To add more variety to the continuity analysis, we examined the validation images of each fold rather than the testing images. First, we defined continuity to be the inverse of changes between adjacent segmentation slices. The mathematical definition is as follows.

\begin{equation}
\text{Continuity} = \left(\frac{1}{M\times (N - 1)}\sum_{i = 1}^{M}\sum_{j = 1}^{N-1} \left| I_{j}^{i} - I_{j+1}^{i}\right|\right)^{-1}
\end{equation}

where $M$ is the number of validation images and $N$ is the number of slices per volume. $I_{j}^{i}$ denotes a segmentation mask of the $j$th slice of the $i$th volume. Using this definition, we tracked the Dice scores of the validation images while progressively applying morphological operations -- first opening and then closing -- to enhance segmentation continuity. We applied different kernel shapes and sizes to smoothen the mask and increase thereby increasing the continuity. The kernels included 3D spheres and ellipsoids. In ellipsoids, the semi-axis in the depth dimension is greater than the others, as discontinuity is likely to occur. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/result_nnunet_dice_and_continuity.png}
    \caption{A plot showing the relationship between Dice score and continuity as smoothing is increasingly applied. Original denotes the initial segmentation. Each point is annotated with the kernel shape and size ($H\times W\times D$) used in smoothing.}
    \label{fig:nnunet-continuity-vs-correctness}
\end{figure}

Continuity was examined in 3 models: 2D $512\times 512$, 3D $160\times 128\times 32$, and 3D $512\times 512\times 32$, with results illustrated in Figure~\ref{fig:nnunet-continuity-vs-correctness}. The original segmentation of all models began in the top left extreme, indicating low continuity but high accuracy (Dice score). As the models undergo smoothing with kernels of various shapes and sizes, segmentation becomes more continuous at the expense of the Dice score. The extent of this trade-off varies across the 3 models, but 3D nnUNet appears to show a slower trade-off than 2D nnUNet as evidenced by a more gradual descending slope. Therefore, 3D segmentation may be advantageous over 2D segmentation when continuity is required.

\section{SuPreM}
We evaluated three configurations of the SwinUNETR model as described in Section~\ref{sec:design:suprem}. SwinUNETR denotes training the model from scratch; SwinUNET (SSL) denotes self-supervised SwinUNETR fine-tuned on the same dataset; and SwinUNETR (SuPreM) denotes a SuPreM-pre-trained SwinUNETR fine-tuned on Calcium OCT. The results are summarized in Figure~\ref{fig:suprem-results}. 

Our findings indicate that SuPreM pre-trained SwinUNETR yields the best results with an average Dice score of $0.631\pm0.030$, statistically higher than those of the other two configurations. However, even the top-performing SuPreM configuration still vastly underperforms 2D nnUNet (Dice scores = 0.631 versus 0.753; see Figure~\ref{fig:transformer-results}). More discussion is provided in the next section. %SuPreM pre-training provides superior features for calcium segmentation compared to self-supervised learning and training from scratch. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/result_SwinUNETR_results.png}
    \caption{Dice scores of SwinUNETR models. Parentheses indicate pre-training algorithms. SuPreM pre-training on SwinUNETR yields the best results and is significantly better than both self-supervised pre-training (SwinUNET (SSL)) and training from scratch (SwinUNETR).}
    \label{fig:suprem-results}
\end{figure}



\section{Transformer-based Models}
Besides SwinUNETR (SuPreM), we also investigate the transformer-based models SegFormer (ADE60K) and ViT (RADIO) which gave average Dice scores of 0.687 and 0.543, respectively (Figure~\ref{fig:transformer-results}). We find that the best 3D nnUNet model (Dice score = 0.742) significantly outperforms all these transformer-based models (Dice scores ranging from 0.543 to 0.687) (Figure~\ref{fig:transformer-results}). This might be due to nnUNet being designed specifically for small-scale datasets, which is more suitable for the dataset that we have.
% This may be attributed to fundamental differences between CNN and transformer architectures. CNNs are designed to learn local features, while transformers are designed to learn global features.
% Calcium plaques in OCT images are localized, making CNNs more suitable for this task. This is also supported by the fact that, in CNNs, increasing the patch size in the width and height dimensions of the 3D nnUNet does not improve performance, while reducing the patch size in the depth dimension significantly improves performance (see Section~\ref{sec:result:nnunet}). The effectiveness of the SegFormer, ViT (RADIO), and SwinUNTER (SuPreM) compared to the nnUNet is summarized in Figure~\ref{fig:transformer-results}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/result_nnUNet_vs_Transformer_results.png}
    \caption{Dice scores of transformer-based models compared to those of the nnUNet. Compared to the best 3D nnUNet model pre-trained on the LaW OCT dataset, Dice scores of transformer-based models are significantly lower.}
    \label{fig:transformer-results}
\end{figure}

\section{V-JEPA}
Many variations of V-JEPA are conducted, and they are named based on the following nomenclature: architecture + decoder head (dataset used for pre-training). For example, "ViT+Attentive (V-JEPA VideoMix2M)" denotes the ViT architecture with an attentive decoder head pre-trained on the VideoMix2M dataset. The asterisk in "Attentive*" denotes an attentive decoder head with intensive data augmentation and positive data sampling. All V-JEPA results are summarized in Figure~\ref{fig:vjepa-decoder-results}. The control experiment, \textbf{ViT + Attentive*}, is a ViT model followed by an attentive* decoder head without V-JEPA pre-training that gives a Dice score of 0.316 as a baseline for comparison. 

\subsubsection{Effect of Pre-training}
\textbf{ViT + Attentive* (V-JEPA VideoMix2M)} significantly improves the performance of the ViT model compared to the no-pretraining control (\textbf{ViT + Attentive*}) (Dice scores = 0.672 versus 0.316; Figure~\ref{fig:vjepa-decoder-results}). This observation suggests that the representation learned by V-JEPA on VideoMix2M is useful for calcium segmentation and highlights the utility of V-JEPA pre-training. 

Even though V-JEPA pre-trained on VideoMix2M yielded a significant improvement over the baseline, V-JEPA pre-trained on our unannotated OCT images \textbf{ViT + Attentive* (V-JEPA Unannotated OCT)} did not improve the performance of the ViT model over the baseline (Dice scores = 0.320 versus 0.316; Figure~\ref{fig:vjepa-decoder-results}). The difference in these outcomes may be related to the dataset size. Unannotated OCT consists of only 500 volumes, whereas VideoMix2M contains 2 million videos. This demonstrates the importance of dataset scale for self-supervised learning on transformer-based models.

\subsubsection{Effect of Decoder Head}
There is no significant difference between the various decoder heads (Attentive, Attentive*, Multifeat, and BNLinear). All of these decoders give similar Dice scores ranging from 0.653 to 0.672 when used on ViT and V-JEPA pre-trained with VideoMix2M.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/result_VJEPA_results.png}
    \caption{Dice score of V-JEPA algorithms with various decoder heads and pre-trained datasets. BNLinear denotes batch normalization followed by linear projection. Attentive signifies the attentive decoder head. Attentive* indicates an attentive decoder head with more intensive data augmentation and positive data sampling. MultiFeat denotes the multi-features decoder head. The dataset used to pre-train V-JEPA is indicated in parentheses.}
    \label{fig:vjepa-decoder-results}
\end{figure}


\section{CLIP}
CLIP pre-training on co-registered OCT images significantly improves the performance of the 3D nnUNet model. In addition, CLIP is statistically comparable to the 3D nnUNet model pre-trained on the LaW OCT dataset. This shows that CLIP is as effective as supervised pre-training on the LaW OCT dataset, while the cost for co-registration is lower than the cost for annotating the lumen and wall. The performance of CLIP pre-training on co-registered multi-modal OCT images is summarized in Figure~\ref{fig:clip-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_CLIP_results.png}
    \caption{Dice score of CLIP on Calcium OCT dataset compared to the scores of nnUNet. The algorithm and dataset used for pre-training are indicated in the parenthesis. The LaW OCT is the supervised pre-training on the Lumen and Wall OCT dataset. CLIP is the pre-training on co-registered Pre-IVL and Post-IVL images.
    }
    \label{fig:clip-results}
\end{figure}

The success of CLIP pre-training may result from the fact that aligning Pre-IVL and Post-IVL requires the model to learn the features of the lumen and wall, which has been proven to be useful for calcium segmentation in 3D nnUNet. To prove this, we visualize the PCA of the features learned by CLIP compared to the features output from the initial weight of the model. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_default_feature_map_batch0_feature1.png}
        \caption{PCA of the output of the features from the initial weight of the model.}
        \label{fig:pca-initial}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/discussion_clip_feature_map_batch0_feature1.png}
        \caption{PCA of the features learned by CLIP.}
        \label{fig:pca-clip}
        
    \end{subfigure}
    \caption{PCA of the output of the features from the initial weight of the model and the features learned by CLIP. CLIP pre-training provides more distinguishable features on the walls than those from the initial weight of the model. 
    }
\end{figure}

The dataset used in CLIP is co-registered Pre-IVL and Post-IVL. Therefore, the author suggests similar experiments be further studied on the co-registered Pre-IVL and Post-Stent dataset.

\section{Genesis}
Pre-training Genesis on unannotated OCT images results in a Dice score of $0.744\pm0.000$ on the Calcium OCT dataset. Furthermore, Genesis significantly improves the performance of the 3D nnUNet model and performs comparably to the best 2D nnUNet and the pre-trained 3D nnUNet on the LaW OCT dataset, while not requiring additional data annotations. The results and statistical tests are summarized in Figure~\ref{fig:genesis-results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/result_nnUNet_and_Genesis_results.png}
    \caption{a Dice score of Genesis on the Calcium OCT dataset compared to those of nnUNets. The dataset used for pre-training is indicated in the parenthesis.
    }
    \label{fig:genesis-results}    
\end{figure}

Since the lumen and wall have been identified as key features for improving calcium segmentation, restoring these structures using the Genesis method enables the model to learn these features. Visualizing the PCA of the features produced by the nnUNet pre-trained with Genesis reveals a similar pattern to that of CLIP, where the features around the arterial wall are discernible (see Figure~\ref{fig:pca-genesis}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/discussion_genesis_feature_map_batch0_feature1.png}
    \caption{PCA of the output of the features from the model pre-trained with Genesis}
    \label{fig:pca-genesis}
\end{figure}


\section{Final Results}
The results of our study are categorized into three distinct groups: SSL (self-supervised learning models trained on unannotated OCT images), Transformer (supervised learning models based on transformer architectures), and nnUNet (supervised learning models using the nnUNet framework). Within these categories, V-JEPA trained on unannotated OCT is classified under SSL, while V-JEPA trained on VideoMix2M is categorized under Transformer. All results are statistically compared to the 2D nnUNet model, which demonstrates the highest average Dice score among the models evaluated. A summary of the best-performing models in each category is presented in Figure~\ref{fig:results}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=1\textwidth]{figures/result_best_in_class_results.png}
    \caption{Summary of the results. The best model in each category is illustrated. SSL denotes self-supervised learning models, Transformer denotes transformer-based models, and nnUNet denotes 2D and 3D CNN-based nnUNet models.}
    \label{fig:results}
\end{figure}

In the SSL category, V-JEPA pre-trained on unannotated OCT images performs significantly worse than 2D nnUNet. This disparity may be attributed to the scale of the dataset, which plays a crucial role in V-JEPA, as discussed previously. Conversely, the Genesis and CLIP model pre-trained on unannotated OCT images achieves performance comparable to the 2D nnUNet. Additionally, they are as effective as supervised pre-training on LaW OCT while not requiring additional annotations. These results indicate the importance of pre-training, and demonstrate that self-supervised pre-training using Genesis or CLIP is a viable cost effective method.

Our experiments show that the Dice scores of transformer-based models are significantly lower than those of the 2D nnUNet. We hypothesize that this difference arises because nnUNet is specifically designed for small datasets, whereas transformer-based models typically assume access to moderately large datasets. As a result, nnUNet appears to be more suitable for our dataset.
However, increasing the size of the dataset and model parameters may not provide further benefits for nnUNet beyond a certain point. In contrast, transformers are likely to benefit from scaling~\cite{Zhai2021}. This study does not aim to prove that nnUNet is universally superior to transformers. Instead, it demonstrates that nnUNet is more appropriate for calcium segmentation on OCT images given the current scale of our dataset and model parameters.

In our study, the 2D nnUNet stands out as the top-performing model, achieving an average Dice score of $0.753 \pm 0.006$. Interestingly, this model does not benefit from additional pre-training, likely because the dataset, when converted into 2D images, provides sufficient training samples.
Although the 2D nnUNet achieves the highest Dice score, continuity is better achieved and retained during the post-processing by 3D nnUNet. 
The best results with the 3D nnUNet are obtained when the model is pre-trained on the LaW OCT dataset. The patch size in the 3D nnUNet plays a critical role in its performance. Specifically, reducing the patch size in the depth dimension significantly improves the model’s accuracy, while increasing the context in the width and height dimensions does not yield similar benefits. This behavior reflects the nature of annotating calcium plaques in OCT images. This behavior reflects the specific characteristics of calcium plaque annotation in OCT images, where plaques are identified by intensity changes around artery walls. As such, expanding the height and width dimensions beyond the walls does not provide additional useful information to the model. Similarly, calcium plaques frequently occur between adjacent slices, making additional context in the depth dimension less critical. Additionally, using a large patch size reduces the number of samples available for training the model, which is a critical consideration given that the Calcium OCT dataset is relatively small.

\section{Discussion}
\subsection{Scale Analysis}
The impact of dataset size on model performance was investigated by systematically subsampling the Calcium OCT dataset and evaluating the resulting models. We randomly subsampled 33\%, 50\%, 66\%, and 83\% of the Calcium OCT dataset while excluding the test set. Each sampled dataset was split into 75\% and 25\% for training and validation respectively. We then trained the model with the training set, while tracking the best model with the validation set. The final Dice score and cross entropy loss are calculated using the test set which remains the same across all scales and experiments.

Given the small size of the Calcium OCT dataset, the model's performance is highly sensitive to the selection of training data. This sensitivity leads to significant volatility and a high standard deviation in the scores, especially as the dataset scale increases. The limited dataset size makes this outcome unsurprising, as small datasets typically contribute to greater variability in model performance.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=1\linewidth]{figures/discussion_scale_analysis.png}
        \caption{Dice score scale analysis.}
        \label{fig:scale-analysis}
    \end{subfigure}%
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=1\linewidth]{figures/discussion_cross_entropy_scale_analysis.png}
        \caption{Cross entropy loss scale analysis.}
        \label{fig:cross-entropy-scale-analysis}
    \end{subfigure}
    \caption{Scale analysis of 3D nnUNets trained on the Calcium OCT dataset, subsampled at different scales. The values are shown in a moving average with a window size of 3.}
\end{figure}

Supervised pre-training on the LaW OCT dataset is more reliable than self-supervised learning on a smaller dataset, resulting in a lower standard deviation and a higher averaged Dice score, as demonstrated in Figure~\ref{fig:scale-analysis}. Additionally, both self-supervised pre-training methods are more beneficial than training the model from scratch. These findings underscore the importance of leveraging pre-training, particularly on smaller datasets, to exchange model performance and stability.

Cross-entropy loss provides a quantitative measure of a model's confidence and accuracy in its predictions. Specifically, when the model is highly confident in a correct prediction, the loss is low, reflecting accurate performance. Conversely, if the model exhibits high confidence in an incorrect prediction, the loss is significantly higher, indicating poor performance. Model confidence and correctness are important as they help users trust the automated segmentation process, ensuring that the results are reliable and can be confidently used in decision-making. The mathematical definition of binary cross-entropy loss is defined below.

\begin{equation}
-\frac{1}{N}\sum_{i=1}^{N}\left[y_{i}\cdot\log\left(p(y_{i})\right)+ \left( 1-y_{i} \right)\cdot \log\left(1 - p(y_{i})\right) \right]
\end{equation}

As shown in Figure~\ref{fig:cross-entropy-scale-analysis}, supervised pre-training on the LaW OCT dataset, CLIP, and Genesis can enhance the correctness and confidence of the 3D nnUNet. Although CLIP and Genesis may be less effective compared to supervised pre-training, they requires fewer resources and can serve as viable alternatives for improving prediction confidence and accuracy in a resource-constrained environments.
One possible explanation for this is that CLIP and Genesis implicitly learn the features related to the lumen and wall, whereas supervised pre-training explicitly learns these features. Consequently, self-supervised methods like CLIP and Genesis require a larger dataset to effectively translate implicit features into explicit ones during fine-tuning, as these methods benefit from larger datasets to maximize their performance. Overall, the choice between supervised pre-training and self-supervised methods depends on the specific resource constraints and performance requirements of the application.

\subsection{Evaluating Features using PCA}
Visually appealing features do not always translate to better performance, as the significance of feature distinction often outweighs their aesthetic quality. We compare features from V-JEPA (VideoMix2M) with those from the initial weights of the ViT model. As shown in Figure~\ref{fig:visually-good-features}, both sets of features preserve the structure of the artery walls. However, the V-JEPA (VideoMix2M) features enhance the contrast between the artery walls and the background by using a uniform color for the background, thereby improving their distinguishability. This increased contrast may explain why the V-JEPA (VideoMix2M) model demonstrates better performance. 

In contrast, the features derived from the model's initial weights, while preserving the original structure of the artery walls, exhibit less contrast. The artery walls blend into a gradient of colors with the background and other structures, making them less distinguishable. This suggests that features that appear visually appealing to humans may not necessarily be the most effective for the model to learn from.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_input.jpg}
        \caption{Input image}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_initial.jpg}
        \caption{PCA feature from ViT with initial weight}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_visual_vjepa_pretrained.jpg}
        \caption{PCA feature from ViT with V-JEPA pre-trained weight}
    \end{subfigure}
    \caption{PCA of the output features from the initial weight of the model and the features learned by V-JEPA. The volume is unrolled along the depth dimension.}
    \label{fig:visually-good-features}
\end{figure}

 Similarly, visualizing features from ViT (RADIO) supports this hypothesis. As shown in Figure~\ref{fig:radio-features}, these features are interpretable by humans and can effectively delineate OCT image semantics. The border between the background and the areas where the waves propagate is clearly visible, forming a distinct circle. However, the features representing the artery walls lack distinctiveness, which may account for the lower Dice score performance of the ViT (RADIO) model.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/discussion_radio_feature.png}
    \caption{PCA of output features from ViT (RADIO). The middle column shows PCA with the first principal component retained, while the right column shows PCA with the first principal component removed.}
    \label{fig:radio-features}
\end{figure}

\subsection{V-JEPA Pre-training}
Unlike Genesis, where loss and semantic checks of the model's learning on the pre-text task can be easily determined, V-JEPA does not provide such clarity. We experimented with different configurations of V-JEPA pre-training on our unannotated OCT images and faced difficulties in converging the loss. As shown in Figure~\ref{fig:v-jepa-training}, the loss curve of ViT-L (V-JEPA Unannotated OCT) is converging, but it is sensitive to the configurations used.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_1.png}
        \caption{ViT-L (V-JEPA Unannotated OCT)}
    \end{subfigure}\hfill%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_2.png}
        \caption{ViT-S (V-JEPA Unannotated OCT)}
    \end{subfigure}\hfill%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_vjepa_training_3.png}
        \caption{ViT-L (V-JEPA Unannotated OCT) with more weight decay}
    \end{subfigure}
    \caption{Loss curves of V-JEPA pre-training on unannotated OCT images with various configurations.}
    \label{fig:v-jepa-training}
\end{figure}

One way to assess V-JEPA's convergence is by plotting and comparing the hidden features from the target encoder and the context encoder. The target encoder processes video patches without masking, while the context encoder does so with masking. As illustrated in Figure~\ref{fig:v-jepa-prediction}, successful pre-training in the V-JEPA framework enables accurate prediction of the hidden features for the masked-out parts of the video. Unlike Genesis or other pixel-level prediction tasks, it's challenging to determine whether the prediction is correct because the output is an embedding vector rather than a pixel-based prediction. This trade-off makes V-JEPA more computationally efficient than pixel-level tasks but also makes it harder to verify if the model is learning effectively.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/discussion_vjepa_prediction.png}
    \caption{Hidden features from the target encoder (denoted as \(h\) and colored in blue) and the context encoder of ViT (V-JEPA on unannotated OCT data) (denoted as \(z\) and colored in orange) over training epochs. The bottom row highlights the differences in hidden features between the target encoder and context encoder in the final epoch.}
    \label{fig:v-jepa-prediction}
\end{figure}

\subsection{CLIP Pre-training}\label{sec:results:discussion:clip}
As mentioned in Section~\ref{sec:implementation:clip}, CLIP requires a shared encoder and projector for successful pre-training convergence. In this section, we present the sanity checks performed and their results, offering insights for future efforts to adapt CLIP to multi-modal imaging.

We began by performing a sanity check of CLIP by training the model to match the identical images, as illustrated in Figure~\ref{fig:clip-sanity-check-same-image}. This is a much simpler task, and configurations that fail to match identical images would be unlikely to succeed in matching co-registered multi-modal OCT images. By identifying and eliminating unsuccessful configurations early, we avoided prolonged training. Next, we trained the model to match Pre-IVL and Post-IVL images, as shown in Figure~\ref{fig:clip-sanity-check-different-image}. This is the primary task of CLIP. Our findings indicate that only a shared encoder and projector can effectively converge the losses in this context.

\begin{figure}[hbt]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image.png}
        \caption{Same images}
        \label{fig:clip-sanity-check-same-image}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_pre-ivl_post-ivl_image.png}
        \caption{Pre-IVL and Post-IVL}
        \label{fig:clip-sanity-check-different-image}
    \end{subfigure}\\
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_diff_enc_diff_proj_logits.png}
        \caption{Co-similarity matrix of different encoders and projectors on the same images.}
        \label{fig:clip-cosimilarity-diff-enc-proj}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_diff_enc_shared_proj_logits.png}
        \caption{Co-similarity matrix of different encoders and shared projectors on the same images.}
        \label{fig:clip-cosimilarity-same-enc-proj}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/discussion_clip_same_image_shared_enc_shared_proj_logits.png}
        \caption{Co-similarity matrix of shared encoders and projectors on different modalities.}
        \label{fig:clip-cosimilarity-shared-enc-proj}
    \end{subfigure}
    \caption{Sanity check of CLIP. The model is trained to match the same images.}
    \label{fig:clip-sanity-check}
\end{figure}

To assess whether the model is learning effectively, we can plot a co-similarity matrix of the features output by the model. As shown in Figure~\ref{fig:clip-cosimilarity-diff-enc-proj}, when using different encoders and projectors, the loss did not converge, and the co-similarity matrix did not resemble an identity matrix. In contrast, with a shared projector, the loss converged using the same images, resulting in a co-similarity matrix closely resembling an identity matrix, as shown in Figure~\ref{fig:clip-cosimilarity-same-enc-proj}. However, the model did not converge when trained on different image modalities unless both the encoders and projectors were shared. Under these conditions, the co-similarity matrix appeared close to an identity matrix, as shown in Figure~\ref{fig:clip-cosimilarity-shared-enc-proj}.

Unlike language and image, different OCT image modalities are not as distinct from one another. Therefore, using different encoders and projectors to align hidden features may introduce noise during the initial training phase, preventing the model from converging.
%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

% The related work section covers closely related work. Here you can highlight
% the related work, how it solved the problem, and why it solved a different
% problem. Do not play down the importance of related work, all of these
% systems have been published and evaluated! Say what is different and how
% you overcome some of the weaknesses of related work by discussing the 
% trade-offs. Stay positive!

% This section is usually 3-5 pages.

% Self-supervised learning
\section{Self-supervised learning}
Self-supervised learning (SSL) has emerged as a promising approach for natural language processing (NLP). SSL is typically divided into two stages: pre-text tasks and downstream tasks. Pre-text tasks involve learning representations from the data itself without the need for human annotations. Once these representations are learned, downstream tasks use them to solve the primary objectives, which usually require human annotations. This approach enables models to learn robust representations while reducing the costs of human annotations~\cite{Jaiswal2020}. Although self-supervised pre-text tasks are widely used in the computer vision (CV) domain, identifying suitable pre-text tasks for visual SSL remains an active area of research. Examples of pre-text tasks in CV include autoencoder~\cite{Hinton2006}, denoising autoencoder~\cite{Vincent2008}, and colorization~\cite{Larsson2017}.

% Self-supervised learning in CVs
In recent years, both discriminative and generative tasks have been proposed as pre-text tasks in visual SSL. SimCLR, a discriminative method, learns representations by maximizing the similarity between differently augmented views of the same image while minimizing the similarity between views of different images~\cite{Chen2020Simple}. This approach, known as contrastive learning (CL), has shown to be effective for classification tasks, even outperforming supervised learning on ImageNet~\cite{Russakovsky2015} using the same ResNet50 architecture~\cite{He2016}. SimCLRv2 further improves SimCLR by incorporating a larger encoder, a projection head, and momentum contrast (MoCo)~\cite{He2020}, which uses a queue and a moving average encoder to stabilize training~\cite{Chen2020}. However, contrastive learning methods require a large batch size and a substantial number of negative samples to be effective~\cite{Chen2020Simple}. These requirements make it challenging to scale datasets and models.

On the contrary, generative learning generally predicts the data distribution to learn robust representations. Masked autoencoding (MAE) learns representations by generating pixel values of the masked-out parts of images, similar to the masked language modeling (MLM)~\cite{Devlin2019} in NLP~\cite{He2022}. MAE was initially explored using the vision transformer (ViT) model when the architecture was first proposed~\cite{Dosovitskiy2020vit}. However, its performance in the original study did not surpass that of supervised learning. To make this work, He et al. proposed masking out 75\% of an image rather than the previously experimented 50\%, and introduced asymmetric encoder-decoder architecture in MAE~\cite{He2022}. The encoder processes only the visible pixels, while the lightweight decoder operates on the encoded tokens. Compared to CL, MAE is less dependent on batch size, data augmentation, and the number of negative samples, rendering it more scalable. MAE outperforms supervised learning and supervised pre-training on ImageNet using the same ViT architecture and has been successfully evaluated on semantic segmentation such as ADE20k~\cite{Zhou2018}. Nonetheless, because MAE operates at a pixel level, it is computationally more expensive than CL.

While MAE focuses on reconstructing pixel values of masked patches, the bidirectional encoder for image transformer (BEiT) takes a different approach by first learning an image patch tokenizer and then predicting the tokenized values of the masked tokens, rather than the pixel values~\cite{Bao2022beit}. BEiT outperforms supervised learning and supervised pre-training on ImageNet~\cite{Russakovsky2015} and achieves state-of-the-art performance in semantic segmentation on ADE20k. However, the requirement for a tokenizer in BEiT adds complexity to the pre-text task compared to MAE. 

DINOv2 combines both restorative and discriminative pre-text tasks by learning representations by predicting the tokenized values of masked tokens and matching the class tokens of teacher and student networks, which encode different crops of the same image~\cite{Oquab2024dinov}. This approach has been shown to outperform other visual SSL methods of its time. Similar to BEiT~\cite{Bao2022beit}, DINOv2 is more efficient than pixel-level SSLs by performing representation learning at the token level. Building on this concept, I-JEPA takes it a step further, by directly predicting the encoded vectors of masked tokens instead of the tokenized values, removing the tokenizer from the approach. When evaluated on image classification, I-JEPA demonstrated comparable performance to other SSLs while being simpler and more efficient~\cite{Assran2023}. These advancements underscore the ongoing evolution and optimization of self-supervised learning techniques in visual representation learning.

Recent SSL methods have been extended to handle dynamic visual data, incorporating temporal dimensions to enhance visual SSL. VideoMAE, for instance, introduces tube masking to MAE~\cite{He2022} and employs an exceptionally high masking ratio of 90\%~\cite{Tong2022VideoMAE}. Without relying on external data, VideoMAE achieves state-of-the-art results on Kinetics-400~\cite{Kay2017Kinetics} and Something-Something V2~\cite{Goyal2017Something-SomethingV2} datasets. However, generating pixel values for video data imposes significant computational limitations, as video clips with numerous frames are challenging to process efficiently. V-JEPA, an extension of I-JEPA~\cite{Assran2023}, predicts the encoded vectors of masked patches in videos~\cite{Bardes2024Vjepa}. Given that I-JEPA is more efficient than MAE, V-JEPA offers a more efficient approach to learning representations in video data, outperforming VideoMAE in the process. These concepts can be applied to 3D medical images by substituting the temporal dimension with the depth axis.

% Self-supervised learning in medical imaging
\section{Self-supervised learning in medical imaging}
Self-supervised learning (SSL) offers significant benefits in medical imaging, where obtaining human annotations is both costly and time-consuming. Despite its potential, SSL methods have not been fully explored in the medical field, primarily due to the unique challenges it presents. One key challenge is the 3D nature of many medical images, unlike the 2D images typically used in SSL for natural images. Another challenge lies in the variety of medical imaging modalities, such as X-ray, CT, MRI, and ultrasound. In our study, we focus on optical coherence tomography (OCT) images of arteries, which are 3D and lack public datasets suitable for SSL. This presents a particular challenge, as no prior research has applied SSL to OCT images of arteries.

% Basic rotation, jigsaw, Rubik
Various classical visual SSL techniques have been explored to enhance segmentation tasks in medical imaging. For example, solving Rubik's cube has been used as a 3D pre-text task where the model learns to classify the orientation and order of a shuffled volumetric cube, leading to improved 3D brain tumor segmentation~\cite{Zhuang2019}. Similarly, the jigsaw-solving technique involves learning feature representations by predicting the correct order of shuffled image patches, leading to improved performance in tumor segmentation~\cite{Taleb2020}. Deep clustering groups feature vectors of the image in an unsupervised manner and use these clusters as pseudo-labels for classification~\cite{Caron2018}, leading to an improved abdominal organs classification~\cite{Dadoun2023}.

Applying one or multiple classical pre-text tasks has become a common theme in several SSL research within medical imaging~\cite{Zhou2021, Zhang2021, Dufumier2021}. For instance, Taleb et al. explored five distinct 3D self-supervised learning tasks, including predicting latent vectors of adjacent patches, predicting the location of a given patch, solving a jigsaw puzzle, predicting rotation angles, and contrastive learning ~\cite{Taleb2020}. Among these tasks, predicting latent vectors of adjacent patches resulted in the best performance in downstream tasks. Additionally, Song et al. combined rotation prediction, instance discrimination, and variation auto-encoder (VAE~\cite{Kingma2013}) as pre-text tasks~\cite{Song2022}. Using a VAE with an encoder-decoder equips the segmentation capability for COVID-19 infection segmentation on lung CT images. This approach outperformed supervised learning models with similar architectures, such as U-Net~\cite{Ronneberger2015} and U-Net++~\cite{Zhou2020}.

% Contrastive learning
Medical SSL research has increasingly focused on applying contrastive learning techniques to medical data.  TS-SSL is applied on 2D spectral domain optical coherence tomography (SD-OCT) images of the retina to improve retinal anomaly classification~\cite{Zhang2021}. TS-SSL simultaneously learns from classification, discriminative, and generative tasks. For the discriminative task, it employs contrastive loss to align different views of the same image. Concurrently, it predicts the rotation angle of rotated views and the order of the shuffled patches as generative tasks. Along with these pre-text tasks, TS-SSL classifies retinal anomalies of the SD-OCT images. However, this method only outperforms supervised learning when 10\% of the labeled data have been used for training. 

He et al.~\cite{He2022Intra} extended the application of contrastive learning to segmentation tasks by introducing intra- and inter-slice contrastive losses for point-supervised OCT fluid segmentation of the retina. Given that retinal OCT is three-dimensional, the authors utilized the correlation between adjacent slices within the same volume during the training of a U-Net~\cite{Ronneberger2015} segmentation model, using inter-slice contrastive loss. Segmented masks were subsequently used to identify small patches for intra-slice contrastive loss, which aimed to compare these patches with the background fluid patches. Simultaneously, the segmentation mask is also optimized by comparing the prediction with the ground truth using cross-entropy loss. This method outperformed other point-based segmentation techniques, although it did not achieve the same performance as fully supervised learning. Additionally, temporal consistency, akin to spatial consistency, can be leveraged to improve the segmentation of brain MRI images, especially when images are acquired from the same patient at different time points~\cite{Ren2022}.

Contrastive learning also facilitates the integration of different data modalities. Dufumier et al.~\cite{Dufumier2021} used patient age as a measure of similarity between different images, which improved the classification of bipolar disorder, Alzheimer's, and schizophrenia on 3D brain MRI images. Furthermore, the incorporation of tabular medical records into medical imaging can be done through Contrastive Language-Image Pre-training (CLIP)~\cite{Radford2021CLIP}, which aligns text with corresponding images. This approach has been shown to enhance classification tasks on brain MRI images~\cite{Hager2023}.

% Restorative tasks
Restorative tasks have emerged as effective pre-text tasks in medical imaging, particularly in settings where data is scarce. One such task is patch shuffling, which has been applied to 2D medical images from MRI, CT, and ultrasound. This task involves distorting an image by randomly cutting and pasting patches within it, and then challenging the model to restore the original image. This method has been shown to enhance downstream segmentation tasks in data-limited environments~\cite{Chen2019}.Similarly, in-painting, where a model predicts and fills in a missing part of an image~\cite{Pathak2016}, has demonstrated improvements in tissue segmentation for MRI and CT scans under data-scarce conditions~\cite{Dominic2023}.

Building on these ideas, the Genesis method combines several image distortion techniques to create 3D restorative pre-text tasks for CT images~\cite{Zhou2021}. Comprehensive experiments have shown that Genesis outperforms specialized 3D state-of-the-art segmentation models and other pre-text tasks, including denoising~\cite{Vincent2010}, in-painting~\cite{Pathak2016}, jigsaw~\cite{Noroozi2016}, deep clustering~\cite{Caron2018}, Rubik's cube solving~\cite{Zhuang2019}, and patch shuffling~\cite{Chen2019}. By integrating multiple restorative tasks, Genesis has set a new benchmark for 3D self-supervised learning (SSL) in medical imaging.  Following a similar approach, SwinUNTER~\cite{Tang2022} proposes training a transformer-based model using a combination of in-painting, contrastive learning, and rotation prediction to improve brain tumor segmentation in MRI images.

The success of Genesis has inspired follow-up research. TransVW extends Genesis by incorporating deep clustering~\cite{Caron2018} and adding a classification task to the restorative task framework of Genesis~\cite{Zhou2021}. In TransVW, visual words are automatically extracted and grouped into clusters using deep latent features. These visual words are then classified through deep clustering and restored using the Genesis method, resulting in improved segmentation and classification performance on some datasets compared to Genesis~\cite{Haghighi2021}. DiRA further enhances TransVW by introducing adversarial learning to distinguish between restored images and the original images~\cite{Haghighi2024}. Additionally, contrastive learning is employed as an additional pre-text task. By combining discriminative, restorative, and adversarial training, DiRA outperforms TransVW and models trained from scratch. However, despite these advancements, the evaluations of TransVW and DiRA are not as comprehensive as those for Genesis. TransVW only outperforms Genesis on certain datasets, while DiRA's performance is primarily evaluated against TransVW.

A gap exists between the pre-text tasks explored in medical imaging and those introduced in recent visual SSL research. While visual SSL research has shifted towards more efficient and simpler pre-text tasks such as DINOv2~\cite{Oquab2024dinov}, MAE~\cite{He2022}, and I-JEPA~\cite{Assran2023}, medical imaging SSL research has predominantly focused on blending multiple pre-text tasks. Genesis~\cite{Zhou2021} attempts to bridge this gap by establishing a modern restorative task as a new baseline for medical SSL. However, more studies are needed to evaluate the suitability of these modern visual SSL techniques in medical imaging. 

Recent studies have begun to explore this potential. Baharoon et al. applied DINOv2~\cite{Oquab2024dinov} directly to classification and segmentation tasks on X-ray, CT, and MRI images, demonstrating that self-supervised foundational models can effectively address medical imaging challenges~\cite{Baharoon2023general}. Similarly, Zhou et al. used MAE~\cite{He2022} pre-training on X-ray, CT, and MRI images and reported improvements in abdominal segmentation compared to UNETR baseline~\cite{Hatamizadeh2022}~\cite{Zhou2022}. These findings highlight the promising potential of modern visual SSL techniques in advancing medical imaging. Nevertheless, rigorous evaluation in medical imaging research must be done as many reported results may not be reproducible~\cite{Isensee2024}.

% Alternatives
\section{Self-supervised learning alternatives}
Self-supervised learning holds significant potential in medical imaging by enabling the extraction of useful representations from medical images without relying on human annotations. However, the development of effective pre-text tasks for SSL is still in its early stages, requiring extensive evaluation. Given the limited availability of large annotated medical datasets, it is crucial to explore alternative methods that can efficiently train models using small datasets.

One such alternative is supervised pre-training. The SuPreM approach, for example, advocates for pre-training models with supervised learning on large, annotated medical datasets. SuPreM compiles a dataset of 9,262 CT volumes annotated with abdominal organ segmentations. The models were initially pre-trained to segment a specific subset of these organs, and then fine-tuned to segment a different, non-overlapping set. The experiment demonstrated that supervised pre-training required less data to learn meaningful representations than self-supervised learning. Additionally, the representations learned through SuPreM exhibited strong transferability to different tasks and datasets.

Instead of pre-training, the nnUNet framework offers a self-configuring approach to medical image segmentation. This framework automatically builds a U-Net model for 2D and 3D medical images tailored to a given dataset. It has outperformed several state-of-the-art models, including popular transformer-based and Mamba-based architectures, on various medical image segmentation tasks. Notably, nnUNet is particularly effective on small datasets due to its robust data sampling and augmentation~\cite{Isensee2020}. Subsequent study on nnUNet underscores that it consistently demonstrates superior performance on the evaluated tasks without the need for additional data~\cite{Isensee2024}.

Another prevalent approach involves fine-tuning models that were pre-trained on different datasets. For instance, SegFormer, a state-of-the-art model for semantic segmentation, was initially trained on the ADE20k dataset. This model leverages a transformer-based architecture that hierarchically processes input features through multiple transformer blocks, capturing both local and global context effectively. SegFormer uses simple multilayer perceptron (MLP) decoder heads that aggregate features from different depths, combining information across scales to produce accurate segmentation masks~\cite{Xie2021SegFormer}. It has achieved state-of-the-art on semantic segmentation at the time, and has shown promising results when applied to other segmentation tasks~\cite{Ghosh2024, Khaled2023}.

Taking a different approach, RADIO proposes to learn representations from multiple teacher models. RADIO is a distillation framework with a student model tasked to predict the same feature as its teacher models, including DINOv2~\cite{Oquab2024dinov}, SAM~\cite{Kirillov2023SAM} and CLIP~\cite{Radford2021CLIP}. Furthermore, RADIO is resolution-agnostic, allowing it to interpret images at different resolutions. RADIO model has demonstrated the ability to generate high-resolution feature maps that can be utilized for several downstream tasks~\cite{Ranzinger2024RADIO}.

While self-supervised learning holds great potential in medical imaging, it is essential to compare SSL methods with existing alternatives to ensure a comprehensive understanding to their relative strengths and weaknesses.

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion, you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\end{document}