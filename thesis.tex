%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file
% Goal: provide formatting for theses and project reports
% Author: Mathias Payer <mathias.payer@epfl.ch>
%
% To avoid any implication, this template is released into the
% public domain / CC0, whatever is most convenient for the author
% using this template.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject
\usepackage[MScThesis,lablogo]{EPFLreport}
\usepackage{xspace}

\title{A wonderful thesis\\about the merits of scientific writing}
\author{Naravich Chutisilp}
\supervisor{The Doctoral Student}
\adviser{Prof. Dr. sc. ETH Mathias Payer}
%\coadviser{Second Adviser}
\expert{The External Reviewer}

% \newcommand{\sysname}{FooSystem\xspace}

\dedication{
    \begin{raggedleft}
        It’s no use going back to yesterday, because I was a different person then.\\
        --- Lewis Carroll, Alice’s Adventures in Wonderland\\
    \end{raggedleft}
    \vspace{4cm}
    \begin{center}
        Dedicated to my lovely family.
    \end{center}
}
\acknowledgments{
% This is where you thank those who supported you on this journey. Good examples
% are your significant other, family, advisers, and other parties that inspired
% you during this project. Generally this section is about 1/2 page to a page.

% Consider acknowledging the use and location of this thesis package.

% Define your acknowledgments in \texttt{\textbackslash{}acknowledgments\{...\}}
% and show them with \texttt{\textbackslash{}makeacks}.

    I would like to show my deep gratitude to my mom and dad for their unwavering supports and firm trust to let me choose my own path, my two lovely sisters, Namking and Namkow, for their continuous love and understanding. 
    I am also grateful to all the friends I have made during my time at EPFL, Set, James, Kwang, Ice, Sundae, Nai, Cindy, Jenestin, Thomas, Paulina, Fah, Ting-Wei, Pin-Yen, Kai, Hong-Bin, Leo, Edvin, Anthon, Aamir, Jirka, and many more people I have not mentioned, who have made my life abroad memorable and amicable.
    Furthermore, I could not forget the friends have made during my time at MIT, Mee, Pooh, and Cue who have made my final Master's semester fun and unforgettable.
    Additionally, it would not be possible if I did not have support from my friends in Thailand, V, Meak, Korn, Marie and uncountable more that I could not fit into this section.

    I would like to thank Prof. Elazer Edelman and Karim Kaldry for giving me this unparalleled opportunity to join the IMES lab at MIT and let me complete my Master's thesis here, as well as providing me advises and support on my research.
}


\begin{document}
\maketitle
\makededication
\makeacks

\begin{abstract}
The sysname tool enables lateral decomposition of a multi-dimensional
flux compensator along the timing and space axes.

The abstract serves as an executive summary of your project.
Your abstract should cover at least the following topics, 1-2 sentences for
each: what area you are in, the problem you focus on, why existing work is
insufficient, what the high-level intuition of your work is, maybe a neat
design or implementation decision, and key results of your evaluation.
\end{abstract}

\begin{frenchabstract}
For a doctoral thesis, you have to provide a French translation of the
English abstract. For other projects this is optional.
\end{frenchabstract}

\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

The introduction is a longer writeup that gently eases the reader into your
% thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
In the second paragraph you can introduce the main challenge that you see.
The third paragraph lists why related work is insufficient.
The fourth and fifth paragraphs discuss your approach and why it is needed.
The sixth paragraph will introduce your thesis statement. Think how you can
distill the essence of your thesis into a single sentence.
The seventh paragraph will highlight some of your results
The eights paragraph discusses your core contribution.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your
% work. This is not necessarily related work but technologies and dependencies
% that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.

% Self-supervised learning
Self-supervised learning (SSL) has been proven to be a promising approach for natural language processing tasks (NLP). SSL can be split into two parts which are pre-text tasks and downstream tasks. Pre-text tasks are tasks that are designed to learn representations from the data itself without human annotations. Downstream tasks are finetuning tasks that use the learned representations from pre-text tasks to solve the tasks that require human annotations. It allows models to learn representations while avoiding costs of human annotations~\cite{Jaiswal2020}. Unsupervised pre-text tasks is not uncommon in the computer vision (CV) domain. For example, autoencoder~\cite{Hinton2006}, denoising autoencoder~\cite{Vincent2008}, and colorization~\cite{Larsson2017} are some of the pre-text tasks that have been used in CV. However, suitable pre-text tasks for SSL in computer vision are still under research.

% Self-supervised learning in CVs
In the recent years, contrastive learning (CL) and masked autoencoding (MAE) have been proposed as pre-text tasks for CV field. SimCLR~\cite{Chen2020Simple} is a contrastive learning method that learns representations by maximizing the similarity between differently augmented views of the same image and minimizing the similarity between views of different images. Representations learned from SimCLR have shown to be effective for classification tasks, outperforming supervised learning on ImageNet~\cite{Russakovsky2015} using the same architecture ResNet50~\cite{He2016}. SimCLRv2~\cite{Chen2020} is an improvement of SimCLR using a larger encoder and projection head and momentum contrast (MoCo)~\cite{He2020} that uses a queue and a moving average encoder to stabilize the training. Nevertheless, contrastive learning methods require a large batch size and a large number of negative samples to work effectively~\cite{Chen2020Simple}. This makes it difficult to scale to large datasets and models. 

In contrast to contrastive learning, masked autoencoding (MAE) learns representations by predicting the masked out parts of images, honing similar spirit to masked language modeling (MLM)~\cite{Devlin2019} in NLP. It has been preliminarily explored on vision transformer (ViT) model at the same time the architecture has been proposed~\cite{Dosovitskiy2020vit}. However, the performace of masked autoencoding in the original paper is not as good as supervised learning. To make this works, He et al.~\cite{He2022} propose masking out 75\% of an image rather than 50\% experimented previously. Furthermore, an asymmetric encoder-decoder architecture of MAE is introduced. An encoder operates only on the visible pixels and a light weight decoder operates on the encoded tokens as well as the mask tokens. Compared to CL, batch size, data augmentation and number of negative samples are not as critical for MAE, rendering it more scalable. MAE outperforms supervised learning and supervised pre-training on ImageNet using the same architecture ViT. Additionally, MAE has been evaluated on segmentic segmentation on ADE20k~\cite{Zhou2018}, highlighting that it can be used for dense prediction downstream tasks as well. Nonetheless, working on the pixel level, MAE is computationally expensive compared to CL.

Along MAE, bidirectional encoder for image transformer (BEiT) first learns image patch tokenizer and predicts the tokenized values of the mask tokens instead of the pixel values in MAE~\cite{Bao2022beit}. BEiT has been shown to outperform supervised learning and supervised pre-training on ImageNet. It has also been evaluated on segmentic segmentation on ADE20k achieving state-of-the-art performace at the time of the paper submission. As BEiT requires tokenizer to be learned, it adds more complexity to the pre-text task compared to MAE. DINOv2\cite{Oquab2024dinov} proposes to learn representations by predicting the tokenized values of the mask tokens as well as matching the class tokens of teacher and student networks on different crops of the same image. This approach has been shown to outperform other image SSLs at the time. Performing representation learning at the token level allows DINOv2 to be more efficient than pixel-level SSLs. In addition, I-JEPA~\cite{Assran2023} adapts this idea one step further. Attempting to find the most efficient approach to pre-train the, I-JEPA proposes to predict directly the encoded vectors of mask tokens instead of the tokenized values. Evaluated on image classification, I-JEPA has shown to be comparable to other SSLs while being simpler and more efficient. 

% Self-supervised learning in medical imaging

% 2D and 3D self-supervised learning (extension to 3D)

% 3D self-supervised learning in medical imaging
% Genesis, SwinUNTER

% supervised segmentation in medical imaging
% nnUNet, Segformer?

%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

Introduce and discuss the design decisions that you made during this project.
Highlight why individual decisions are important and/or necessary. Discuss
how the design fits together.

This section is usually 5-10 pages.


%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

The implementation covers some of the implementation details of your project.
This is not intended to be a low level description of every line of code that
you wrote but covers the implementation aspects of the projects.

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%

In the evaluation you convince the reader that your design works as intended.
Describe the evaluation setup, the designed experiments, and how the
experiments showcase the individual points you want to prove.

This section is usually 5-10 pages.


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

The related work section covers closely related work. Here you can highlight
the related work, how it solved the problem, and why it solved a different
problem. Do not play down the importance of related work, all of these
systems have been published and evaluated! Say what is different and how
you overcome some of the weaknesses of related work by discussing the 
trade-offs. Stay positive!

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%
% You need the following items:
% \begin{itemize}
% \item A box
% \item Crayons
% \item A self-aware 5-year old
% \end{itemize}

\end{document}