@string{SP="IEEE Security and Privacy Magazine"}
@string{CODASPY="ACM Conference on Data and Application Security and Privacy"}
@string{ATC="Usenix Annual Technical Conference"}
@string{CSUR="ACM Computing Surveys"}
@string{RTSS="Real-Time Systems Symposium"}
@string{arXiv="arXiv Technical Report"}
@string{LangSec="Language-theoretic Security IEEE Security and Privacy Workshop"}
@string{CCS="ACM Conference on Computer and Communication Security"}
@string{IOTSP="Workshop on the Internet of Things Security and Privacy"}
@string{NDSS="Network and Distributed System Security Symposium"}
@string{ISPASS="International Symposium on Performance Analysis of Systems and Software"}
@string{CCC="Chaos Communication Congress"}
@string{NSPW="New Security Paradigms Workshop"}
@string{BHEU="BlackHat Europe"}
@string{TR="Technical Report"}
@string{PLDI="ACM International Conference on Programming Language Design and Implementation"}
@string{EuroSP="IEEE European Symposium on Security and Privacy"}
@string{SS3P="Open Textbook"}
@string{DIMVA="Conference on Detection of Intrusions and Malware and Vulnerability Assessment"}
@string{ISMM="ACM SIGPLAN International Symposium on Memory Management"}
@string{ESSoS="Int'l. Symp. on Eng. Secure Software and Systems"}
@string{SEC="Usenix Security Symposium"}
@string{IMC="ACM Internet Measurement Conference"}
@string{TSE="IEEE Transactions on Software Engineering"}
@string{CC="International Conference on Compiler Construction"}
@string{STM="International Workshop on Security and Trust Management"}
@string{ArmsRace="The Continuing Arms Race"}
@string{TRB="Transportation Research Board"}
@string{FEAST="Forming an Ecosystem Around Software Transformation"}
@string{DSN="IEEE/IFIP International Conference on Dependable Systems and Networks"}
@string{SyScan360="Symposium on Security for Asia Network + 360"}
@string{BalCCon="Balkan Computer Congress"}
@string{DSAL="AOSD workshop on Domain-Specific Aspect Languages"}
@string{ESORICS="European Symposium on Research in Computer Security"}
@string{WOOT="Usenix Workshop on Offensive Technologies"}
@string{TIFS="IEEE Transactions on Information Forensics and Security"}
@string{HotSWUp="Usenix Workshop on Hot Topics in Software Upgrades"}
@string{AsiaCCS="ACM Symp. on InformAtion, Computer and Communications Security"}
@string{AMAS-BT="Workshop on Architectural and Microarchitectural Support for Binary Translation"}
@string{PPREW="Program Protection and Reverse Engineering Workshop"}
@string{SYSTOR="ACM International Systems and Storage Conference"}
@string{VEE="ACM International Conference on Virtual Execution Environments"}
@string{OSDI="Usenix Symposium on Operating Systems Design and Implementation"}
@string{Oakland="IEEE International Symposium on Security and Privacy"}
@string{PST="IEEE Conference on Privacy, Security, and Trust"}

@inproceedings{Assran2023,
  title     = {Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture},
  url       = {http://dx.doi.org/10.1109/CVPR52729.2023.01499},
  doi       = {10.1109/cvpr52729.2023.01499},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Assran,  Mahmoud and Duval,  Quentin and Misra,  Ishan and Bojanowski,  Piotr and Vincent,  Pascal and Rabbat,  Michael and LeCun,  Yann and Ballas,  Nicolas},
  year      = {2023},
  month     = jun
}

@inproceedings{Bao2022beit,
  title     = {{BE}iT: {BERT} Pre-Training of Image Transformers},
  author    = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{Chen2020,
  author    = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  title     = {Big self-supervised models are strong semi-supervised learners},
  year      = {2020},
  isbn      = {9781713829546},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (≤13 labeled images per class) using ResNet-50, a 10 x improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  articleno = {1865},
  numpages  = {13},
  location  = {Vancouver, BC, Canada},
  series    = {NIPS '20}
}
@inproceedings{Chen2020Simple,
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  title     = {A simple framework for contrastive learning of visual representations},
  year      = {2020},
  publisher = {JMLR.org},
  abstract  = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  articleno = {149},
  numpages  = {11},
  series    = {ICML'20}
}
@inproceedings{Devlin2019,
  url       = {http://dx.doi.org/10.18653/v1/N19-1423},
  doi       = {10.18653/v1/n19-1423},
  booktitle = {Proceedings of the 2019 Conference of the North},
  publisher = {Association for Computational Linguistics},
  author    = {Devlin,  Jacob and Chang,  Ming-Wei and Lee,  Kenton and Toutanova,  Kristina},
  year      = {2019}
}
@article{Dosovitskiy2020vit,
  title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal = {ICLR},
  year    = {2021}
}
@inproceedings{He2016,
  title     = {Deep Residual Learning for Image Recognition},
  url       = {http://dx.doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/cvpr.2016.90},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Zhang,  Xiangyu and Ren,  Shaoqing and Sun,  Jian},
  year      = {2016},
  month     = jun
}
@inproceedings{He2020,
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00975},
  doi       = {10.1109/cvpr42600.2020.00975},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Fan,  Haoqi and Wu,  Yuxin and Xie,  Saining and Girshick,  Ross},
  year      = {2020},
  month     = jun
}

@inproceedings{He2022,
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.01553},
  doi       = {10.1109/cvpr52688.2022.01553},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Chen,  Xinlei and Xie,  Saining and Li,  Yanghao and Dollar,  Piotr and Girshick,  Ross},
  year      = {2022},
  month     = jun
}

@article{Hinton2006,
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  volume    = {313},
  issn      = {1095-9203},
  url       = {http://dx.doi.org/10.1126/science.1127647},
  doi       = {10.1126/science.1127647},
  number    = {5786},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Hinton,  G. E. and Salakhutdinov,  R. R.},
  year      = {2006},
  month     = jul,
  pages     = {504–507}
}

@article{Jaiswal2020,
  title     = {A Survey on Contrastive Self-Supervised Learning},
  volume    = {9},
  issn      = {2227-7080},
  url       = {http://dx.doi.org/10.3390/technologies9010002},
  doi       = {10.3390/technologies9010002},
  number    = {1},
  journal   = {Technologies},
  publisher = {MDPI AG},
  author    = {Jaiswal,  Ashish and Babu,  Ashwin Ramesh and Zadeh,  Mohammad Zaki and Banerjee,  Debapriya and Makedon,  Fillia},
  year      = {2020},
  month     = dec,
  pages     = {2}
}
@inproceedings{Larsson2017,
  title     = {Colorization as a Proxy Task for Visual Understanding},
  url       = {http://dx.doi.org/10.1109/CVPR.2017.96},
  doi       = {10.1109/cvpr.2017.96},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Larsson,  Gustav and Maire,  Michael and Shakhnarovich,  Gregory},
  year      = {2017},
  month     = jul
}

@article{Oquab2024dinov,
  title   = {{DINO}v2: Learning Robust Visual Features without Supervision},
  author  = {Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2024},
  url     = {https://openreview.net/forum?id=a68SUt6zFt},
  note    = {}
}

@article{Russakovsky2015,
  title     = {ImageNet Large Scale Visual Recognition Challenge},
  volume    = {115},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-015-0816-y},
  doi       = {10.1007/s11263-015-0816-y},
  number    = {3},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Russakovsky,  Olga and Deng,  Jia and Su,  Hao and Krause,  Jonathan and Satheesh,  Sanjeev and Ma,  Sean and Huang,  Zhiheng and Karpathy,  Andrej and Khosla,  Aditya and Bernstein,  Michael and Berg,  Alexander C. and Fei-Fei,  Li},
  year      = {2015},
  month     = apr,
  pages     = {211–252}
}

@inproceedings{Vincent2008,
  series     = {ICML ’08},
  title      = {Extracting and composing robust features with denoising autoencoders},
  url        = {http://dx.doi.org/10.1145/1390156.1390294},
  doi        = {10.1145/1390156.1390294},
  booktitle  = {Proceedings of the 25th international conference on Machine learning - ICML ’08},
  publisher  = {ACM Press},
  author     = {Vincent,  Pascal and Larochelle,  Hugo and Bengio,  Yoshua and Manzagol,  Pierre-Antoine},
  year       = {2008},
  collection = {ICML ’08}
}

@article{Zhou2018,
  title     = {Semantic Understanding of Scenes Through the ADE20K Dataset},
  volume    = {127},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-018-1140-0},
  doi       = {10.1007/s11263-018-1140-0},
  number    = {3},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Zhou,  Bolei and Zhao,  Hang and Puig,  Xavier and Xiao,  Tete and Fidler,  Sanja and Barriuso,  Adela and Torralba,  Antonio},
  year      = {2018},
  month     = dec,
  pages     = {302–321}
}