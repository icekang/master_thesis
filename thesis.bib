@string{SP="IEEE Security and Privacy Magazine"}
@string{CODASPY="ACM Conference on Data and Application Security and Privacy"}
@string{ATC="Usenix Annual Technical Conference"}
@string{CSUR="ACM Computing Surveys"}
@string{RTSS="Real-Time Systems Symposium"}
@string{arXiv="arXiv Technical Report"}
@string{LangSec="Language-theoretic Security IEEE Security and Privacy Workshop"}
@string{CCS="ACM Conference on Computer and Communication Security"}
@string{IOTSP="Workshop on the Internet of Things Security and Privacy"}
@string{NDSS="Network and Distributed System Security Symposium"}
@string{ISPASS="International Symposium on Performance Analysis of Systems and Software"}
@string{CCC="Chaos Communication Congress"}
@string{NSPW="New Security Paradigms Workshop"}
@string{BHEU="BlackHat Europe"}
@string{TR="Technical Report"}
@string{PLDI="ACM International Conference on Programming Language Design and Implementation"}
@string{EuroSP="IEEE European Symposium on Security and Privacy"}
@string{SS3P="Open Textbook"}
@string{DIMVA="Conference on Detection of Intrusions and Malware and Vulnerability Assessment"}
@string{ISMM="ACM SIGPLAN International Symposium on Memory Management"}
@string{ESSoS="Int'l. Symp. on Eng. Secure Software and Systems"}
@string{SEC="Usenix Security Symposium"}
@string{IMC="ACM Internet Measurement Conference"}
@string{TSE="IEEE Transactions on Software Engineering"}
@string{CC="International Conference on Compiler Construction"}
@string{STM="International Workshop on Security and Trust Management"}
@string{ArmsRace="The Continuing Arms Race"}
@string{TRB="Transportation Research Board"}
@string{FEAST="Forming an Ecosystem Around Software Transformation"}
@string{DSN="IEEE/IFIP International Conference on Dependable Systems and Networks"}
@string{SyScan360="Symposium on Security for Asia Network + 360"}
@string{BalCCon="Balkan Computer Congress"}
@string{DSAL="AOSD workshop on Domain-Specific Aspect Languages"}
@string{ESORICS="European Symposium on Research in Computer Security"}
@string{WOOT="Usenix Workshop on Offensive Technologies"}
@string{TIFS="IEEE Transactions on Information Forensics and Security"}
@string{HotSWUp="Usenix Workshop on Hot Topics in Software Upgrades"}
@string{AsiaCCS="ACM Symp. on InformAtion, Computer and Communications Security"}
@string{AMAS-BT="Workshop on Architectural and Microarchitectural Support for Binary Translation"}
@string{PPREW="Program Protection and Reverse Engineering Workshop"}
@string{SYSTOR="ACM International Systems and Storage Conference"}
@string{VEE="ACM International Conference on Virtual Execution Environments"}
@string{OSDI="Usenix Symposium on Operating Systems Design and Implementation"}
@string{Oakland="IEEE International Symposium on Security and Privacy"}
@string{PST="IEEE Conference on Privacy, Security, and Trust"}

@book{Ahmad2023,
  title     = {Percutaneous Coronary Intervention},
  author    = {Ahmad, Mansoor and Mehta, Parth and Reddivari, Anil Kumar Reddy
               and Mungee, Sudhir},
  abstract  = {Coronary artery disease (CAD) is one of the leading causes of
               death. Percutaneous coronary intervention (PCI) is a
               non-surgical, invasive procedure with the goal of relieving the
               narrowing or occlusion of the coronary artery and improve blood
               supply to the ischemic tissue. This is usually achieved by
               different methods, the most common being ballooning the narrow
               segment or deploying a stent to keep the artery open.},
  publisher = {StatPearls Publishing},
  month     = jun,
  year      = 2023
}

@article{Ali2019,
  author  = {Ziad A. Ali  and Holger Nef  and Javier Escaned  and Nikos Werner  and Adrian P. Banning  and Jonathan M. Hill  and Bernard De Bruyne  and Matteo Montorfano  and Thierry Lefevre  and Gregg W. Stone  and Aaron Crowley  and Mitsuaki Matsumura  and Akiko Maehara  and Alexandra J. Lansky  and Jean Fajadet  and Carlo Di Mario },
  title   = {Safety and Effectiveness of Coronary Intravascular Lithotripsy for Treatment of Severely Calcified Coronary Stenoses},
  journal = {Circulation: Cardiovascular Interventions},
  volume  = {12},
  number  = {10},
  pages   = {e008434},
  year    = {2019},
  doi     = {10.1161/CIRCINTERVENTIONS.119.008434},
  url     = {https://www.ahajournals.org/doi/abs/10.1161/CIRCINTERVENTIONS.119.008434},
  eprint  = {https://www.ahajournals.org/doi/pdf/10.1161/CIRCINTERVENTIONS.119.008434}
}
@inproceedings{Assran2023,
  title     = {Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture},
  url       = {http://dx.doi.org/10.1109/CVPR52729.2023.01499},
  doi       = {10.1109/cvpr52729.2023.01499},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Assran,  Mahmoud and Duval,  Quentin and Misra,  Ishan and Bojanowski,  Piotr and Vincent,  Pascal and Rabbat,  Michael and LeCun,  Yann and Ballas,  Nicolas},
  year      = {2023},
  month     = jun
}
@misc{Balestriero2023,
  doi       = {10.48550/ARXIV.2304.12210},
  url       = {https://arxiv.org/abs/2304.12210},
  author    = {Balestriero,  Randall and Ibrahim,  Mark and Sobal,  Vlad and Morcos,  Ari and Shekhar,  Shashank and Goldstein,  Tom and Bordes,  Florian and Bardes,  Adrien and Mialon,  Gregoire and Tian,  Yuandong and Schwarzschild,  Avi and Wilson,  Andrew Gordon and Geiping,  Jonas and Garrido,  Quentin and Fernandez,  Pierre and Bar,  Amir and Pirsiavash,  Hamed and LeCun,  Yann and Goldblum,  Micah},
  keywords  = {Machine Learning (cs.LG),  Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {A Cookbook of Self-Supervised Learning},
  publisher = {arXiv},
  year      = {2023},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@inproceedings{Bao2022beit,
  title     = {{BE}iT: {BERT} Pre-Training of Image Transformers},
  author    = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=p-BhZSz59o4}
}
@misc{Bardes2024Vjepa,
  title  = {V-{JEPA}: Latent Video Prediction for Visual Representation Learning},
  author = {Adrien Bardes and Quentin Garrido and Jean Ponce and Xinlei Chen and Michael Rabbat and Yann LeCun and Mido Assran and Nicolas Ballas},
  year   = {2024},
  url    = {https://openreview.net/forum?id=WFYbBOEOtv}
}
@inproceedings{Beluch2018,
  title     = {The Power of Ensembles for Active Learning in Image Classification},
  url       = {http://dx.doi.org/10.1109/CVPR.2018.00976},
  doi       = {10.1109/cvpr.2018.00976},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Beluch,  William H. and Genewein,  Tim and Nurnberger,  Andreas and Kohler,  Jan M.},
  year      = {2018},
  month     = jun
}

@book{Butt2023,
  title     = {Intravascular Lithotripsy},
  author    = {Butt, Nausharwan and Khalid, Nauman and Shlofmitz, Evan},
  abstract  = {Coronary artery calcification (CAC) is an independent predictor
               for major cardiovascular events.[1][2][3][4] Additionally,
               coronary calcium deposition can hinder successful percutaneous
               coronary intervention (PCI) as a result of inadequate stent
               expansion, difficulty transiting the catheter through a
               calcified lesion, coated drug separation from a stent,
               proclivity for in-stent restenosis and stent thrombosis, and a
               change to the underlying pharmacokinetics. Consequently, PCI of
               calcified lesions correlates with worse outcomes.[5]},
  publisher = {StatPearls Publishing},
  month     = aug,
  year      = 2023
}

@inproceedings{Caron2018,
  author    = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  title     = {Deep Clustering for Unsupervised Learning of Visual Features},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  month     = {September},
  year      = {2018}
}

@article{Carpenter2022,
  author   = {Carpenter, Harry J. and Ghayesh, Mergen H. and Zander, Anthony C. and Li, Jiawen and Di Giovanni, Giuseppe and Psaltis, Peter J.},
  title    = {Automated Coronary Optical Coherence Tomography Feature Extraction with Application to Three-Dimensional Reconstruction},
  journal  = {Tomography},
  volume   = {8},
  year     = {2022},
  number   = {3},
  pages    = {1307--1349},
  url      = {https://www.mdpi.com/2379-139X/8/3/108},
  pubmedid = {35645394},
  issn     = {2379-139X},
  abstract = {Coronary optical coherence tomography (OCT) is an intravascular, near-infrared light-based imaging modality capable of reaching axial resolutions of 10–20 µm. This resolution allows for accurate determination of high-risk plaque features, such as thin cap fibroatheroma; however, visualization of morphological features alone still provides unreliable positive predictive capability for plaque progression or future major adverse cardiovascular events (MACE). Biomechanical simulation could assist in this prediction, but this requires extracting morphological features from intravascular imaging to construct accurate three-dimensional (3D) simulations of patients’ arteries. Extracting these features is a laborious process, often carried out manually by trained experts. To address this challenge, numerous techniques have emerged to automate these processes while simultaneously overcoming difficulties associated with OCT imaging, such as its limited penetration depth. This systematic review summarizes advances in automated segmentation techniques from the past five years (2016–2021) with a focus on their application to the 3D reconstruction of vessels and their subsequent simulation. We discuss four categories based on the feature being processed, namely: coronary lumen; artery layers; plaque characteristics and subtypes; and stents. Areas for future innovation are also discussed as well as their potential for future translation.},
  doi      = {10.3390/tomography8030108}
}
@article{Chen2019,
  title     = {Self-supervised learning for medical image analysis using image context restoration},
  volume    = {58},
  issn      = {1361-8415},
  url       = {http://dx.doi.org/10.1016/j.media.2019.101539},
  doi       = {10.1016/j.media.2019.101539},
  journal   = {Medical Image Analysis},
  publisher = {Elsevier BV},
  author    = {Chen,  Liang and Bentley,  Paul and Mori,  Kensaku and Misawa,  Kazunari and Fujiwara,  Michitaka and Rueckert,  Daniel},
  year      = {2019},
  month     = dec,
  pages     = {101539}
}

@inproceedings{Chen2020,
  author    = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  title     = {Big self-supervised models are strong semi-supervised learners},
  year      = {2020},
  isbn      = {9781713829546},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (≤13 labeled images per class) using ResNet-50, a 10 x improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  articleno = {1865},
  numpages  = {13},
  location  = {Vancouver, BC, Canada},
  series    = {NIPS '20}
}

@inproceedings{Chen2020Simple,
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  title     = {A simple framework for contrastive learning of visual representations},
  year      = {2020},
  publisher = {JMLR.org},
  abstract  = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  articleno = {149},
  numpages  = {11},
  series    = {ICML'20}
}

@article{Costopoulos2016,
  title     = {Intravascular ultrasound and optical coherence tomography imaging of coronary atherosclerosis},
  author    = {Costopoulos, Charis and Brown, Adam J and Teng, Zhongzhao and Hoole, Stephen P and West, Nick EJ and Samady, Habib and Bennett, Martin R},
  journal   = {The International Journal of Cardiovascular Imaging},
  volume    = {32},
  pages     = {189--200},
  year      = {2016},
  publisher = {Springer}
}

@inproceedings{Devlin2019,
  url       = {http://dx.doi.org/10.18653/v1/N19-1423},
  doi       = {10.18653/v1/n19-1423},
  booktitle = {Proceedings of the 2019 Conference of the North},
  publisher = {Association for Computational Linguistics},
  author    = {Devlin,  Jacob and Chang,  Ming-Wei and Lee,  Kenton and Toutanova,  Kristina},
  year      = {2019}
}

@article{Dosovitskiy2020vit,
  title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal = {ICLR},
  year    = {2021}
}

@inbook{Dufumier2021,
  title     = {Contrastive Learning with Continuous Proxy Meta-data for 3D MRI Classification},
  isbn      = {9783030871963},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-030-87196-3_6},
  doi       = {10.1007/978-3-030-87196-3_6},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  author    = {Dufumier,  Benoit and Gori,  Pietro and Victor,  Julie and Grigis,  Antoine and Wessa,  Michele and Brambilla,  Paolo and Favre,  Pauline and Polosan,  Mircea and McDonald,  Colm and Piguet,  Camille Marie and Phillips,  Mary and Eyler,  Lisa and Duchesnay,  Edouard},
  year      = {2021},
  pages     = {58–68}
}

@article{Fu2012,
  title     = {A survey on instance selection for active learning},
  volume    = {35},
  issn      = {0219-3116},
  url       = {http://dx.doi.org/10.1007/s10115-012-0507-8},
  doi       = {10.1007/s10115-012-0507-8},
  number    = {2},
  journal   = {Knowledge and Information Systems},
  publisher = {Springer Science and Business Media LLC},
  author    = {Fu,  Yifan and Zhu,  Xingquan and Li,  Bin},
  year      = {2012},
  month     = jun,
  pages     = {249–283}
}

@article{Fujimoto2003,
  title     = {Optical coherence tomography for ultrahigh resolution in vivo imaging},
  author    = {Fujimoto, James G},
  journal   = {Nature biotechnology},
  volume    = {21},
  number    = {11},
  pages     = {1361--1367},
  year      = {2003},
  publisher = {Nature Publishing Group}
}

@inproceedings{Gal2017,
  author    = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  title     = {Deep Bayesian active learning with image data},
  year      = {2017},
  publisher = {JMLR.org},
  abstract  = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  pages     = {1183–1192},
  numpages  = {10},
  location  = {Sydney, NSW, Australia},
  series    = {ICML'17}
}

@inproceedings{Goyal2017Something-SomethingV2,
  title     = {The "Something Something" Video Database for Learning and Evaluating Visual Common Sense},
  author    = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fründ and Peter Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
  year      = {2017},
  doi       = {10.1109/ICCV.2017.622},
  url       = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2017.622},
  researchr = {https://researchr.org/publication/GoyalKMMWKHFYMH17},
  cites     = {0},
  citedby   = {0},
  pages     = {5843-5851},
  booktitle = {IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017},
  publisher = {IEEE},
  isbn      = {978-1-5386-1032-9}
}

@inproceedings{Hager2023,
  author    = {Hager, Paul and Menten, Martin J. and Rueckert, Daniel},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi       = {10.1109/cvpr52729.2023.02291},
  month     = {June},
  publisher = {IEEE},
  title     = {Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data},
  url       = {http://dx.doi.org/10.1109/CVPR52729.2023.02291},
  year      = {2023}
}

@article{Haghighi2021,
  title     = {Transferable Visual Words: Exploiting the Semantics of Anatomical Patterns for Self-Supervised Learning},
  volume    = {40},
  issn      = {1558-254X},
  url       = {http://dx.doi.org/10.1109/TMI.2021.3060634},
  doi       = {10.1109/tmi.2021.3060634},
  number    = {10},
  journal   = {IEEE Transactions on Medical Imaging},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Haghighi,  Fatemeh and Taher,  Mohammad Reza Hosseinzadeh and Zhou,  Zongwei and Gotway,  Michael B. and Liang,  Jianming},
  year      = {2021},
  month     = oct,
  pages     = {2857–2868}
}

@article{Haghighi2024,
  title     = {Self-supervised learning for medical image analysis: Discriminative,  restorative,  or adversarial?},
  volume    = {94},
  issn      = {1361-8415},
  url       = {http://dx.doi.org/10.1016/j.media.2024.103086},
  doi       = {10.1016/j.media.2024.103086},
  journal   = {Medical Image Analysis},
  publisher = {Elsevier BV},
  author    = {Haghighi,  Fatemeh and Hosseinzadeh Taher,  Mohammad Reza and Gotway,  Michael B. and Liang,  Jianming},
  year      = {2024},
  month     = may,
  pages     = {103086}
}

@inproceedings{He2016,
  title     = {Deep Residual Learning for Image Recognition},
  url       = {http://dx.doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/cvpr.2016.90},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Zhang,  Xiangyu and Ren,  Shaoqing and Sun,  Jian},
  year      = {2016},
  month     = jun
}

@inproceedings{He2020,
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00975},
  doi       = {10.1109/cvpr42600.2020.00975},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Fan,  Haoqi and Wu,  Yuxin and Xie,  Saining and Girshick,  Ross},
  year      = {2020},
  month     = jun
}

@inproceedings{He2022,
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.01553},
  doi       = {10.1109/cvpr52688.2022.01553},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {He,  Kaiming and Chen,  Xinlei and Xie,  Saining and Li,  Yanghao and Dollar,  Piotr and Girshick,  Ross},
  year      = {2022},
  month     = jun
}

@article{He2022Intra,
  title     = {Intra- and Inter-Slice Contrastive Learning for Point Supervised OCT Fluid Segmentation},
  volume    = {31},
  issn      = {1941-0042},
  url       = {http://dx.doi.org/10.1109/TIP.2022.3148814},
  doi       = {10.1109/tip.2022.3148814},
  journal   = {IEEE Transactions on Image Processing},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {He,  Xingxin and Fang,  Leyuan and Tan,  Mingkui and Chen,  Xiangdong},
  year      = {2022},
  pages     = {1870–1881}
}

@article{Hennessey2023,
  title     = {Contemporary percutaneous management of coronary calcification: current status and future directions},
  volume    = {10},
  issn      = {2053-3624},
  url       = {http://dx.doi.org/10.1136/openhrt-2022-002182},
  doi       = {10.1136/openhrt-2022-002182},
  number    = {1},
  journal   = {Open Heart},
  publisher = {BMJ},
  author    = {Hennessey,  Breda and Pareek,  Nilesh and Macaya,  Fernando and Yeoh,  Julian and Shlofmitz,  Evan and Gonzalo,  Nieves and Hill,  Jonathan and Escaned,  Javier},
  year      = {2023},
  month     = feb,
  pages     = {e002182}
}

@article{Hinton2006,
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  volume    = {313},
  issn      = {1095-9203},
  url       = {http://dx.doi.org/10.1126/science.1127647},
  doi       = {10.1126/science.1127647},
  number    = {5786},
  journal   = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  author    = {Hinton,  G. E. and Salakhutdinov,  R. R.},
  year      = {2006},
  month     = jul,
  pages     = {504–507}
}

@article{Isensee2020,
  title     = {nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
  volume    = {18},
  issn      = {1548-7105},
  url       = {http://dx.doi.org/10.1038/s41592-020-01008-z},
  doi       = {10.1038/s41592-020-01008-z},
  number    = {2},
  journal   = {Nature Methods},
  publisher = {Springer Science and Business Media LLC},
  author    = {Isensee,  Fabian and Jaeger,  Paul F. and Kohl,  Simon A. A. and Petersen,  Jens and Maier-Hein,  Klaus H.},
  year      = {2020},
  month     = dec,
  pages     = {203–211}
}

@misc{Isensee2024,
  doi       = {10.48550/ARXIV.2404.09556},
  url       = {https://arxiv.org/abs/2404.09556},
  author    = {Isensee,  Fabian and Wald,  Tassilo and Ulrich,  Constantin and Baumgartner,  Michael and Roy,  Saikat and Maier-Hein,  Klaus and Jaeger,  Paul F.},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image Segmentation},
  publisher = {arXiv},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Jaiswal2020,
  title     = {A Survey on Contrastive Self-Supervised Learning},
  volume    = {9},
  issn      = {2227-7080},
  url       = {http://dx.doi.org/10.3390/technologies9010002},
  doi       = {10.3390/technologies9010002},
  number    = {1},
  journal   = {Technologies},
  publisher = {MDPI AG},
  author    = {Jaiswal,  Ashish and Babu,  Ashwin Ramesh and Zadeh,  Mohammad Zaki and Banerjee,  Debapriya and Makedon,  Fillia},
  year      = {2020},
  month     = dec,
  pages     = {2}
}

@misc{Kay2017Kinetics,
  doi       = {10.48550/ARXIV.1705.06950},
  url       = {https://arxiv.org/abs/1705.06950},
  author    = {Kay,  Will and Carreira,  Joao and Simonyan,  Karen and Zhang,  Brian and Hillier,  Chloe and Vijayanarasimhan,  Sudheendra and Viola,  Fabio and Green,  Tim and Back,  Trevor and Natsev,  Paul and Suleyman,  Mustafa and Zisserman,  Andrew},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {The Kinetics Human Action Video Dataset},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{Kingma2013,
  doi       = {10.48550/ARXIV.1312.6114},
  url       = {https://arxiv.org/abs/1312.6114},
  author    = {Kingma,  Diederik P and Welling,  Max},
  keywords  = {Machine Learning (stat.ML),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {Auto-Encoding Variational Bayes},
  publisher = {arXiv},
  year      = {2013},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Kirillov2023SAM,
  title   = {Segment Anything},
  author  = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal = {arXiv:2304.02643},
  year    = {2023}
}

@inproceedings{Larsson2017,
  title     = {Colorization as a Proxy Task for Visual Understanding},
  url       = {http://dx.doi.org/10.1109/CVPR.2017.96},
  doi       = {10.1109/cvpr.2017.96},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Larsson,  Gustav and Maire,  Michael and Shakhnarovich,  Gregory},
  year      = {2017},
  month     = jul
}

@inproceedings{Li2024,
  title     = {How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?},
  author    = {Wenxuan Li and Alan Yuille and Zongwei Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=AhizIPytk4}
}

@inbook{Noroozi2016,
  title     = {Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  isbn      = {9783319464664},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-319-46466-4_5},
  doi       = {10.1007/978-3-319-46466-4_5},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  author    = {Noroozi,  Mehdi and Favaro,  Paolo},
  year      = {2016},
  pages     = {69–84}
}

@article{Oktay2020,
  title     = {Evaluation of deep learning to augment image-guided radiotherapy
               for head and neck and prostate cancers},
  author    = {Oktay, Ozan and Nanavati, Jay and Schwaighofer, Anton and
               Carter, David and Bristow, Melissa and Tanno, Ryutaro and Jena,
               Rajesh and Barnett, Gill and Noble, David and Rimmer, Yvonne and
               Glocker, Ben and O'Hara, Kenton and Bishop, Christopher and
               Alvarez-Valle, Javier and Nori, Aditya},
  abstract  = {Importance: Personalized radiotherapy planning depends on
               high-quality delineation of target tumors and surrounding organs
               at risk (OARs). This process puts additional time burdens on
               oncologists and introduces variability among both experts and
               institutions. Objective: To explore clinically acceptable
               autocontouring solutions that can be integrated into existing
               workflows and used in different domains of radiotherapy. Design,
               Setting, and Participants: This quality improvement study used a
               multicenter imaging data set comprising 519 pelvic and 242 head
               and neck computed tomography (CT) scans from 8 distinct clinical
               sites and patients diagnosed either with prostate or head and
               neck cancer. The scans were acquired as part of treatment dose
               planning from patients who received intensity-modulated
               radiation therapy between October 2013 and February 2020.
               Fifteen different OARs were manually annotated by expert readers
               and radiation oncologists. The models were trained on a subset
               of the data set to automatically delineate OARs and evaluated on
               both internal and external data sets. Data analysis was
               conducted October 2019 to September 2020. Main Outcomes and
               Measures: The autocontouring solution was evaluated on external
               data sets, and its accuracy was quantified with volumetric
               agreement and surface distance measures. Models were benchmarked
               against expert annotations in an interobserver variability (IOV)
               study. Clinical utility was evaluated by measuring time spent on
               manual corrections and annotations from scratch. Results: A
               total of 519 participants' (519 [100\%] men; 390 [75\%] aged
               62-75 years) pelvic CT images and 242 participants' (184 [76\%]
               men; 194 [80\%] aged 50-73 years) head and neck CT images were
               included. The models achieved levels of clinical accuracy within
               the bounds of expert IOV for 13 of 15 structures (eg, left
               femur, $\kappa$ = 0.982; brainstem, $\kappa$ = 0.806) and
               performed consistently well across both external and internal
               data sets (eg, mean [SD] Dice score for left femur, internal vs
               external data sets: 98.52\% [0.50] vs 98.04\% [1.02]; P = .04).
               The correction time of autogenerated contours on 10 head and
               neck and 10 prostate scans was measured as a mean of 4.98 (95\%
               CI, 4.44-5.52) min/scan and 3.40 (95\% CI, 1.60-5.20) min/scan,
               respectively, to ensure clinically accepted accuracy. Manual
               segmentation of the head and neck took a mean 86.75 (95\% CI,
               75.21-92.29) min/scan for an expert reader and 73.25 (95\% CI,
               68.68-77.82) min/scan for a radiation oncologist. The
               autogenerated contours represented a 93\% reduction in time.
               Conclusions and Relevance: In this study, the models achieved
               levels of clinical accuracy within expert IOV while reducing
               manual contouring time and performing consistently well across
               previously unseen heterogeneous data sets. With the availability
               of open-source libraries and reliable performance, this creates
               significant opportunities for the transformation of radiation
               treatment planning.},
  journal   = {JAMA Netw. Open},
  publisher = {American Medical Association (AMA)},
  volume    = 3,
  number    = 11,
  pages     = {e2027426},
  month     = nov,
  year      = 2020,
  language  = {en}
}
@article{Oquab2024dinov,
  title   = {{DINO}v2: Learning Robust Visual Features without Supervision},
  author  = {Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  journal = {Transactions on Machine Learning Research},
  issn    = {2835-8856},
  year    = {2024},
  url     = {https://openreview.net/forum?id=a68SUt6zFt},
  note    = {}
}
@inproceedings{Pathak2016,
  title     = {Context Encoders: Feature Learning by Inpainting},
  url       = {http://dx.doi.org/10.1109/CVPR.2016.278},
  doi       = {10.1109/cvpr.2016.278},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Pathak,  Deepak and Krahenbuhl,  Philipp and Donahue,  Jeff and Darrell,  Trevor and Efros,  Alexei A.},
  year      = {2016},
  month     = jun
}
@misc{Radford2021CLIP,
  doi       = {10.48550/ARXIV.2103.00020},
  url       = {https://arxiv.org/abs/2103.00020},
  author    = {Radford,  Alec and Kim,  Jong Wook and Hallacy,  Chris and Ramesh,  Aditya and Goh,  Gabriel and Agarwal,  Sandhini and Sastry,  Girish and Askell,  Amanda and Mishkin,  Pamela and Clark,  Jack and Krueger,  Gretchen and Sutskever,  Ilya},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@inproceedings{Rahimi2021,
  title  = {Addressing the Exorbitant Cost of Labeling Medical Images with Active Learning W},
  author = {Saba Rahimi and Ozan Oktay and Javier Alvarez-Valle and Sujeeth Bharadwaj},
  year   = {2021},
  url    = {https://api.semanticscholar.org/CorpusID:237641383}
}
@inproceedings{Ranzinger2024RADIO,
  author    = {Ranzinger, Mike and Heinrich, Greg and Kautz, Jan and Molchanov, Pavlo},
  title     = {AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into One},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {12490-12500}
}

@misc{Ren2022,
  title         = {Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis},
  author        = {Mengwei Ren and Neel Dey and Martin A. Styner and Kelly Botteron and Guido Gerig},
  url           = {https://arxiv.org/abs/2206.04281},
  year          = {2022},
  eprint        = {2206.04281},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@inproceedings{Ronneberger2015,
  author    = {Ronneberger, Olaf
               and Fischer, Philipp
               and Brox, Thomas},
  editor    = {Navab, Nassir
               and Hornegger, Joachim
               and Wells, William M.
               and Frangi, Alejandro F.},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
  year      = {2015},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {234--241},
  abstract  = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn      = {978-3-319-24574-4}
}

@article{Russakovsky2015,
  title     = {ImageNet Large Scale Visual Recognition Challenge},
  volume    = {115},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-015-0816-y},
  doi       = {10.1007/s11263-015-0816-y},
  number    = {3},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Russakovsky,  Olga and Deng,  Jia and Su,  Hao and Krause,  Jonathan and Satheesh,  Sanjeev and Ma,  Sean and Huang,  Zhiheng and Karpathy,  Andrej and Khosla,  Aditya and Bernstein,  Michael and Berg,  Alexander C. and Fei-Fei,  Li},
  year      = {2015},
  month     = apr,
  pages     = {211–252}
}

@article{Segars2013,
  title     = {Population of anatomically variable {4D} {XCAT} adult phantoms
               for imaging research and optimization},
  author    = {Segars, W P and Bond, Jason and Frush, Jack and Hon, Sylvia and
               Eckersley, Chris and Williams, Cameron H and Feng, Jianqiao and
               Tward, Daniel J and Ratnanather, J T and Miller, M I and Frush,
               D and Samei, E},
  abstract  = {PURPOSE: The authors previously developed the 4D extended
               cardiac-torso (XCAT) phantom for multimodality imaging research.
               The XCAT consisted of highly detailed whole-body models for the
               standard male and female adult, including the cardiac and
               respiratory motions. In this work, the authors extend the XCAT
               beyond these reference anatomies by developing a series of
               anatomically variable 4D XCAT adult phantoms for imaging
               research, the first library of 4D computational phantoms.
               METHODS: The initial anatomy of each phantom was based on
               chest-abdomen-pelvis computed tomography data from normal
               patients obtained from the Duke University database. The major
               organs and structures for each phantom were segmented from the
               corresponding data and defined using nonuniform rational
               B-spline surfaces. To complete the body, the authors manually
               added on the head, arms, and legs using the original XCAT adult
               male and female anatomies. The structures were scaled to best
               match the age and anatomy of the patient. A multichannel large
               deformation diffeomorphic metric mapping algorithm was then used
               to calculate the transform from the template XCAT phantom (male
               or female) to the target patient model. The transform was
               applied to the template XCAT to fill in any unsegmented
               structures within the target phantom and to implement the 4D
               cardiac and respiratory models in the new anatomy. Each new
               phantom was refined by checking for anatomical accuracy via
               inspection of the models. RESULTS: Using these methods, the
               authors created a series of computerized phantoms with thousands
               of anatomical structures and modeling cardiac and respiratory
               motions. The database consists of 58 (35 male and 23 female)
               anatomically variable phantoms in total. Like the original XCAT,
               these phantoms can be combined with existing simulation packages
               to simulate realistic imaging data. Each new phantom contains
               parameterized models for the anatomy and the cardiac and
               respiratory motions and can, therefore, serve as a jumping point
               from which to create an unlimited number of 3D and 4D variations
               for imaging research. CONCLUSIONS: A population of phantoms that
               includes a range of anatomical variations representative of the
               public at large is needed to more closely mimic a clinical study
               or trial. The series of anatomically variable phantoms developed
               in this work provide a valuable resource for investigating 3D
               and 4D imaging devices and the effects of anatomy and motion in
               imaging. Combined with Monte Carlo simulation programs, the
               phantoms also provide a valuable tool to investigate
               patient-specific dose and image quality, and optimization for
               adults undergoing imaging procedures.},
  journal   = {Med. Phys.},
  publisher = {Wiley},
  volume    = 40,
  number    = 4,
  pages     = {043701},
  month     = apr,
  year      = 2013,
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  language  = {en}
}

@book{Shahjehan2023,
  title     = {Coronary artery disease},
  author    = {Shahjehan, Rai Dilawar and Bhutta, Beenish S},
  abstract  = {Coronary artery disease is a condition in which there is an
               inadequate supply of blood and oxygen to the myocardium. It
               results from occlusion of the coronary arteries and results in a
               demand-supply mismatch of oxygen. It typically involves the
               formation of plaques in the lumen of coronary arteries that
               impede blood flow. It is the major cause of death in the US and
               worldwide. At the beginning of the 20th century, it was an
               uncommon cause of death. Deaths due to CAD peaked in the
               mid-1960s and then decreased however, it still is the leading
               cause of death worldwide.[1]},
  publisher = {StatPearls Publishing},
  month     = aug,
  year      = 2023
}

@article{Song2022,
  title     = {COVID-19 Infection Segmentation and Severity Assessment Using a Self-Supervised Learning Approach},
  volume    = {12},
  issn      = {2075-4418},
  url       = {http://dx.doi.org/10.3390/diagnostics12081805},
  doi       = {10.3390/diagnostics12081805},
  number    = {8},
  journal   = {Diagnostics},
  publisher = {MDPI AG},
  author    = {Song,  Yao and Liu,  Jun and Liu,  Xinghua and Tang,  Jinshan},
  year      = {2022},
  month     = jul,
  pages     = {1805}
}

@inproceedings{Taleb2020,
  author    = {Taleb, Aiham and Loetzsch, Winfried and Danz, Noel  and Severin, Julius and Gaertner, Thomas and Bergner, Benjamin and Lippert, Christoph},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {18158--18172},
  publisher = {Curran Associates, Inc.},
  title     = {3D Self-Supervised Methods for Medical Imaging},
  url       = {https://proceedings.neurips.cc/paper/2020/file/d2dc6368837861b42020ee72b0896182-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{Tang2022,
  title     = {Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.02007},
  doi       = {10.1109/cvpr52688.2022.02007},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Tang,  Yucheng and Yang,  Dong and Li,  Wenqi and Roth,  Holger R. and Landman,  Bennett and Xu,  Daguang and Nath,  Vishwesh and Hatamizadeh,  Ali},
  year      = {2022},
  month     = jun
}

@inproceedings{Tong2022VideoMAE,
  title     = {Video{MAE}: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author    = {Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year      = {2022},
  url       = {https://openreview.net/forum?id=AhccnBXSne}
}


@inproceedings{Vincent2008,
  series     = {ICML ’08},
  title      = {Extracting and composing robust features with denoising autoencoders},
  url        = {http://dx.doi.org/10.1145/1390156.1390294},
  doi        = {10.1145/1390156.1390294},
  booktitle  = {Proceedings of the 25th international conference on Machine learning - ICML ’08},
  publisher  = {ACM Press},
  author     = {Vincent,  Pascal and Larochelle,  Hugo and Bengio,  Yoshua and Manzagol,  Pierre-Antoine},
  year       = {2008},
  collection = {ICML ’08}
}

@article{Vincent2010,
  author     = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  title      = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
  year       = {2010},
  issue_date = {3/1/2010},
  publisher  = {JMLR.org},
  volume     = {11},
  issn       = {1532-4435},
  abstract   = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  journal    = {J. Mach. Learn. Res.},
  month      = {dec},
  pages      = {3371–3408},
  numpages   = {38}
}

@article{Virani2021Heart,
  title     = {Heart disease and stroke statistics-2021 update: a report from the American Heart Association.},
  author    = {Virani, Salim S and Alonso, Alvaro and Aparicio, Hugo J and Benjamin, Emelia J and Bittencourt, Marcio S and Callaway, Clifton W and Carson, April P and Chamberlain, Alanna M and Cheng, Susan and Delling, Francesca N and others},
  year      = {2021},
  publisher = {Ovid Technologies (Wolters Kluwer Health)}
}

@article{Wu2015,
  title     = {High Blood Pressure and All-Cause and Cardiovascular Disease Mortalities in Community-Dwelling Older Adults},
  volume    = {94},
  issn      = {0025-7974},
  url       = {http://dx.doi.org/10.1097/md.0000000000002160},
  doi       = {10.1097/md.0000000000002160},
  number    = {47},
  journal   = {Medicine},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  author    = {Wu,  Chen-Yi and Hu,  Hsiao-Yun and Chou,  Yiing-Jenq and Huang,  Nicole and Chou,  Yi-Chang and Li,  Chung-Pin},
  year      = {2015},
  month     = nov,
  pages     = {e2160}
}

@misc{Xie2021SegFormer,
  doi       = {10.48550/ARXIV.2105.15203},
  url       = {https://arxiv.org/abs/2105.15203},
  author    = {Xie,  Enze and Wang,  Wenhai and Yu,  Zhiding and Anandkumar,  Anima and Alvarez,  Jose M. and Luo,  Ping},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Zhang2021,
  title     = {Twin self-supervision based semi-supervised learning (TS-SSL): Retinal anomaly classification in SD-OCT images},
  volume    = {462},
  issn      = {0925-2312},
  url       = {http://dx.doi.org/10.1016/j.neucom.2021.08.051},
  doi       = {10.1016/j.neucom.2021.08.051},
  journal   = {Neurocomputing},
  publisher = {Elsevier BV},
  author    = {Zhang,  Yuhan and Li,  Mingchao and Ji,  Zexuan and Fan,  Wen and Yuan,  Songtao and Liu,  Qinghuai and Chen,  Qiang},
  year      = {2021},
  month     = oct,
  pages     = {491–505}
}

@article{Zhou2018,
  title     = {Semantic Understanding of Scenes Through the ADE20K Dataset},
  volume    = {127},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-018-1140-0},
  doi       = {10.1007/s11263-018-1140-0},
  number    = {3},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Zhou,  Bolei and Zhao,  Hang and Puig,  Xavier and Xiao,  Tete and Fidler,  Sanja and Barriuso,  Adela and Torralba,  Antonio},
  year      = {2018},
  month     = dec,
  pages     = {302–321}
}

@article{Zhou2020,
  title     = {UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation},
  volume    = {39},
  issn      = {1558-254X},
  url       = {http://dx.doi.org/10.1109/TMI.2019.2959609},
  doi       = {10.1109/tmi.2019.2959609},
  number    = {6},
  journal   = {IEEE Transactions on Medical Imaging},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Zhou,  Zongwei and Siddiquee,  Md Mahfuzur Rahman and Tajbakhsh,  Nima and Liang,  Jianming},
  year      = {2020},
  month     = jun,
  pages     = {1856–1867}
}

@article{Zhou2021,
  title     = {Models Genesis},
  volume    = {67},
  issn      = {1361-8415},
  url       = {http://dx.doi.org/10.1016/j.media.2020.101840},
  doi       = {10.1016/j.media.2020.101840},
  journal   = {Medical Image Analysis},
  publisher = {Elsevier BV},
  author    = {Zhou,  Zongwei and Sodha,  Vatsal and Pang,  Jiaxuan and Gotway,  Michael B. and Liang,  Jianming},
  year      = {2021},
  month     = jan,
  pages     = {101840}
}

@inbook{Zhuang2019,
  title     = {Self-supervised Feature Learning for 3D Medical Images by Playing a Rubik’s Cube},
  isbn      = {9783030322519},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-030-32251-9_46},
  doi       = {10.1007/978-3-030-32251-9_46},
  booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019},
  publisher = {Springer International Publishing},
  author    = {Zhuang,  Xinrui and Li,  Yuexiang and Hu,  Yifan and Ma,  Kai and Yang,  Yujiu and Zheng,  Yefeng},
  year      = {2019},
  pages     = {420–428}
}